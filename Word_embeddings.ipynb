{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/catalan_general_crawling contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/catalan_general_crawling\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Reduïu els costos dels processos administratius al vostre organisme públic\\nEviteu els desplaçaments i pèrdua de temps als ciutadans en les seves gestions\\nOferiu una administració més transparent a ciutadans i empreses\\nEns grans i petits experimenten aquesta transformació amb èxit, gràcies al suport de l\\'AOC\\nDepartament de Sistemes d\\'Informació i Processos\\n\" Via Oberta ens ha permès fer efectiu el dret dels ciutadans a no aportar documents, eliminant paper i simplificant procediments\"\\n\" e.FACT proporciona informació indispensable per a la realització de les auditories del registre comptable de factures de les Administracions Públiques Catalanes\"\\nCoordinador del departament d\\'Informàtica\\n\"El servei VIA OBERTA és el que ha aportat majors avantatges per als ciutadans\"\\n\"Amb l\\' e-NOTUM hem escurçat els procediments en 12 dies, quasi un 40% menys!\"\\nCoordinadora d\\'organització de persones i e-administració\\n\" Via Oberta ofereix millores per als ciutadans al no haver d\\'aportar cap document\"\\nResponsable d\\'Informàtica i Administració Electrònica\\n\" e-TRAM ens ha permès implantar un servei de tramitació electrònica per als ciutadans de forma ràpida, senzilla i amb un cost reduït\"\\n\"Els municipis amb pocs habitants trobem en els serveis de l\\'AOC la gratuïtat i la comoditat necessàries per dur a terme el nostre dia a dia\"\\n\"Les T-CAT han permès incorporar de forma segura la signatura electrònica dins dels nostres procediments afavorint la transformació digital de la nostra activitat\"\\nCap de Departament de Sistemes i Tecnologies de la Informació\\n\"Amb el desplegament de l\\' idCAT hem apropat l\\'Ajuntament a la ciutadania\"\\n\"Mitjançant els serveis de Govern Obert de l\\'AOC hem pogut fer fàcil el que sembla difícil\"\\n\"Al tauler electrònic pots penjar fins i tot el projecte sencer i al final et permet fer també la diligència\"\\nÀrea de Promoció Econòmica, Administració i Hisenda\\n\"El Sobre Digital i la PSCP han aconseguit una comunió senzilla entre empreses i administració per universalitzar la compra pública electrònica\"\\n\"L\\' e-SET és la implantació d\\'un nou sistema de treball que facilita la feina del dia a dia\"\\nCap del servei de contractació i compres\\n\"El Sobre Digital, una experiència imprescindible per a la bona administració amb estalvi de recursos i millora de la seguretat jurídica i la transparència\"\\nÀrea d\\'Organització i Administració Electrònica\\n\"El desplegament de la valisa electrònica ha estat clau en el procés de transformació digital dels nostres procediments interns\"\\n\"L\\' Hèstia permet el treball en temps real i des de qualsevol lloc, així com sistematitzar la pràctica professional, recollir la informació ordenadament i amb el mateix llenguatge\"\\nConsulta els materials del Congrés de Govern Digital 2019\\nGoverns transparents, fluids, dinàmics, líquids... un bon lema pel principal objectiu de la governança del segle XXI: democratitzar-ho tot.\\nConfluències, rius, cooperació.\\nCatalunya, Mediterrània, mar de drets.\\nA favor: totes les Administracions movent-se per posar-se al dia i millorar, tot aprofitant la revolució digital.\\nEn contra: quants cops estem reinventant la roda i quantes quantes oportunitats perdudes de fer-ho una única vegada i de forma coordinada i col·laborativa?\\n\"La transparència és una oportunitat.\\nHem de perdre tota por a explicar què fem\": la conclusió de la taula d\\'alcaldies de la Jornada de Govern Obert pic.twitter.com/ERbgLSIXZM\\nEl director general de Participació Ciutadana ens convida a transformar les administracions públiques a partir de la participació ciutadana\\nEns cal que allò que preocupa i ocupa els governants formi part d\\'allò en què participa la ciutadania pic.twitter.com/NwQr4EZSCS: \"A moltes institucions encara els sona xinés això de les dades obertes i la transparència.\\nDe que serveix que hi hagi un portal, si llavors no hi ha dades?\\nLlavors l\\'accés a la informació pels periodistes és molt parcial\".\\nOferim eines que, conjuntament amb la metodologia i el suport necessari, fan possible l\\'assoliment d\\'un govern digital\\nPosem al vostre abast tot el coneixement: formació, guies, normatives, etc.\\nTenim eines per gestionar àgilment part del procés administratiu del vostre ens\\nEl nostre equip farà tot el possible per resoldre les vostres incidències\\nSabem que es tracta d\\'una decisió molt important per al vostre ens i és per això que us ho volem posar fàcil.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.'}\n",
      "\n",
      "{'text': 'En compliment de la Directiva 2009/136/CE, desenvolupada en el nostre ordenament per l\\'article 22.2 de la Llei de Serveis de Societat de la Informació (LSSI) i seguint les instruccions de l\\'Agència Espanyola de Protecció de Dades, procedim a informar-li detalladament de l\\'ús que es realitza a la nostra pàgina web.\\nAquesta informació no revela la seva identitat, però sí que permet la seva identificació com a un usuari concret i pot guardar informació relativa a la freqüència amb la que visita la pàgina web, les seves preferències de navegació o aquella informació que més l\\'interessa.\\nEl que ens permet, cada vegada que accedeix a www.aoc.cat, millorar la qualitat i la usabilitat de la nostra pàgina web.\\nNo obstant, si les desactiva, pot ser que la seva navegació per www.aoc.cat no sigui òptima i algunes de les seves utilitats no funcionin correctament.\\nCookies analítiques: galetes de Google Analytics\\nAquesta pàgina web utilitza Google Analytics, un servei analític del web prestat per Google.\\nInc, una companyia de Delaware l\\'oficina principal de la qual es troba a 1600 Amphitheatre Parkway, Mountain View (Califòrnia), CA 94043, Estats Units (\" Google\").\\nLa informació que genera la cookie sobre l\\'ús del lloc web (incloent l\\'adreça IP) serà directament transmesa i arxivada per Google en els seus servidors d\\'Estats Units.\\nGoogle utilitzarà aquesta informació per compte nostre amb el propòsit de seguir la pista del seu ús del lloc web.\\nGoogle podrà transmetre aquesta informació a tercers quan així ho requereixi la legislació, o quan aquests tercers processin la informació per compte de Google.\\nEn aquests casos, Google no associarà la seva adreça IP amb cap altra dada de què disposi.\\nEn utilitzar aquesta pàgina web consent el tractament de la seva informació per Google en la forma i per als fins anteriorment indicats.\\nL\\'exercici de qualsevol dret s\\'haurà de realitzar mitjançant comunicació directa amb Google.\\nPer optar per no ser rastrejats per Google Analytics a través de tots els llocs web podeu consultar http://tools.google.com/dlpage/gaoptout\\nAixí mateix, també registra quan va ser la primera i l\\'última vegada que l\\'usuari va visitar el web www.aoc.cat.\\nGaletes en altres llocs web del Consorci AOC\\nLes seves finalitats són descrites a la pàgina de privacitat de Twitter.\\nConfiguració de l\\'usuari per evitar Cookies\\nÉs cas de dubte pot dirigir-se al webmaster del domini creador de la cookie.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.'}\n",
      "\n",
      "{'text': 'L\\'ús de la informació continguda en aquest lloc web implica l\\'acceptació i el consentiment en els termes i les condicions que es detallen en aquest avís legal.\\nTitularitat i règim de responsabilitat de la pàgina web\\nEl responsable d\\'aquesta pàgina web és el Consorci Administració Oberta de Catalunya, (Consorci AOC), amb NIF Q0801175A, i ubicat al Carrer de Tànger, núm. 98, (planta baixa) 08018 (tel. 93 272 25 00 i fax.\\nTota persona que accedeixi a aquest lloc web assumeix el paper d\\'usuari, comprometent-se a l\\'observança i compliment rigorós de les disposicions aquí disposades, així com a qualsevol altre disposició legal que li sigui d\\'aplicació.\\nEl Consorci AOC té el dret a modificar la informació que apareix en aquesta pàgina web, sense que existeixi la obligació de preavís o posada en coneixement dels usuaris de les noves obligacions -a excepció dels compromisos assumits en virtut de convenis específics – entenent-se com a suficient la seva publicació en el lloc web.\\nLes informacions i els continguts relacionats amb l\\'actuació i les funcions del Consorci AOC, que s\\'inclouen en aquest web, estan subjectes a les previsions següents:\\nResponsabilitat amb relació als continguts\\nEl Consorci AOC treballa perquè les informacions, els continguts i els serveis oferts o difosos en aquest web acompleixin de manera suficient la necessària integritat, veracitat, actualització, accessibilitat i usabilitat.\\nA aquest efecte, cal tenir en compte la data d\\'actualització de cadascun dels continguts que en cada cas s\\'indiqui.\\nLa pàgina web ofereix informació, consells, guies i d\\'altres continguts preparats pel Consorci AOC amb finalitats de difusió, informació, conscienciació i en determinats casos la prestació de serveis específics d\\'administració electrònica.\\nS\\'informa a l\\'usuari que tots aquests continguts, malgrat estar preparats amb el màxim nivell de qualitat possibles, no poden suposar en cap moment assessorament específic en matèria tecnològica i/o jurídica o ser considerats com a actuacions dirigides a la resolució de problemàtiques específiques.\\nEn qualsevol cas, el Consorci AOC es reserva el dret a modificar-los, suprimir-los, desenvolupar-los o actualitzar-los unilateralment sense notificació prèvia i sense assumir cap responsabilitat.\\nEl Consorci AOC ofereix la traducció automàtica en altres llengües diferents del català dels continguts del seu web per tal de facilitar als ciutadans la comprensió del text en el seu propi idioma.\\nMalgrat tot, els articles traduïts automàticament poden contenir errors materials dels quals el Consorci AOC no se\\'n fa responsable.\\nEn l\\'actualitat, el Consorci AOC només garanteix la veracitat dels continguts en llengua catalana.\\nReferències i enllaços a webs d\\'altres organitzacions\\nEl web del Consorci AOC conté referències o enllaços a webs de tercers (\"links\"), la major part d\\'elles són a pàgines d\\'Internet d\\'altres administracions públiques, que s\\'han considerat d\\'interès pels usuaris.\\nEn el cas que s\\'abandoni el web, el Consorci AOC no assumeix cap responsabilitat derivada de la connexió o dels contingut dels enllaços de tercers.\\nTot i això, es revisen periòdicament els enllaços a altres pàgines per tal d\\'evitar la inclusió d\\'enllaços que no compleixen la normativa de protecció de dades, així com la resta de normativa vigent.\\nEn aquest sentit, el Consorci manifesta que si es detecta qualsevol contingut que pugui contravenir la legislació nacional o internacional, o l\\'ordre públic, es procedirà a la retirada immediata de l\\'enllaç, posant-ho en coneixement de les autoritat competents.\\nEl Consorci AOC no es fa responsable de la informació i continguts emmagatzemats, a títol enunciatiu però no limitatiu, en els fòrums, blocs, comentaris en xarxes socials, o qualsevol altre mitjà del Consorci AOC que permeti a tercers publicar continguts de forma independent.\\nNo obstant, en compliment de l\\'article 11 i 16 de la LSSICE, es posa a disposició de tots els usuaris, autoritats i forces de seguretat, per col·laborar de forma activa en la retirada o, en el seu cas, bloqueig de tots aquells continguts que poguessin afectar o contravenir la legislació nacional, o internacional, drets de tercers o la moral i l\\'ordre públic.\\nEn el cas que qualsevol usuari consideri que existeix en el lloc web algun contingut que pugui ser susceptible de l\\'anterior classificació, es sol·licita que ho comuniqui de forma immediata a l\\'administrador del la pàgina web per mitjà del correu electrònic [EMAIL]\\nReproducció de continguts propis\\nEl Consorci AOC facilita la consulta lliure i gratuïta de la informació continguda en el web i autoritza la reproducció total o parcial dels seus continguts, sempre i quan els continguts esmentats es conservin íntegres, es citi la font i la data en la que s\\'ha realitzat la còpia, no es manipulin, ni alterin els continguts i no s\\'utilitzi directament amb finalitats comercials (Llei 37/2007, de 16 de novembre, sobre la reutilització de la informació del sector públic).\\nEl Consorci AOC autoritza la descarrega gratuïta dels manuals, impresos, programes i publicacions informatives que s\\'inclouen en el seu web, a efectes de la seva reproducció i distribució, llevat que s\\'indiqui el contrari de forma expressa.\\nNo obstant això, en determinats supòsits el Consorci AOC pot indicar de manera explícita que és necessari sol·licitar una autorització expressa.\\nAixí mateix, la reutilització es pot limitar per la tutela d\\'altres béns jurídics prioritaris, com ara la protecció de les dades personals, la intimitat o els drets de protecció intel·lectual de tercers.\\nEl domini d\\'aquest web és titularitat del Consorci AOC, així com els drets de propietat intel·lectual, el seu disseny i els codis que conté, llevat que s\\'indiqui una titularitat diferent.\\nNo s\\'autoritza en cap cas, l\\'ús de marques o signes distintius, logotips i en general símbols distintius de qualsevol naturalesa propietat del Consorci AOC, en publicacions i webs que no siguin d\\'ens participats o patrocinats per aquest Consorci, sense el coneixement i l\\'autorització corresponent del Consorci AOC.\\nEl Consorci AOC no assumirà cap responsabilitat derivada de l\\'ús per part de tercers del contingut d\\'aquesta pàgina Web i podrà exercitar totes les accions civils o penals que li corresponguin en cas d\\'infracció d\\'aquests drets per part de l\\'usuari.\\nAquesta informació es troba continguda a la Política de Cookies del Consorci AOC.\\nDret aplicable i jurisdicció competent\\nLa llei aplicable en cas de disputa o conflicte d\\'interpretació dels termes que conforme aquest Avís legal, així com qualsevol aspecte relacionat amb els serveis d\\'aquest web, serà la legislació espanyola.\\nEls possibles conflictes relatius a aquest web es regiran exclusivament pel dret espanyol, essent els jutjats de Barcelona els únics competents.\\nTota persona usuària del web, independentment de la jurisdicció territorial des de la qual es produeixi el seu accés, accepta el compliment i respecte d\\'aquesta clàusula amb renúncia expressa a qualsevol altre fur que li pogués correspondre.\\nSi alguna part o clàusula d\\'aquestes Condicions fos declarada nul·la o deixada sense efecte per una resolució judicial, les restants estipulacions conservaran la seva validesa.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.'}\n",
      "\n",
      "{'text': \"Els Reconeixements Administració Oberta als ajuntaments i consells comarcals que atorga anualment l'AOC han esdevingut el reconeixement públic de tots aquells ens que destaquen en la transformació digital de la seva relació amb la ciutadania i en la seva gestió interna.\\nEls guardons s'atorguen en base a uns indicadors objectius, l'anàlisi dels webs dels ens i l'ús de determinats serveis del Consorci AOC.\\nEl seu objectiu és valorar i reconèixer la implantació i l'ús dels serveis d'administració electrònica i, tal i com s'ha indicat anteriorment, la seva conseqüent transformació digital en seva relació amb la ciutadania i en la seva gestió interna.\\nEls Reconeixements Administració Oberta tenen set categories d'acord amb el nombre d'habitants dels ens i la seva naturalesa:\\nDescripció del mètode d'avaluació dels Reconeixements Administració Oberta\\nLlistat de guardonats a l'edició del 2018: ajuntaments i consells comarcals capdavanters en administració digital\\nSegells Reconeixements Administració Oberta 2018\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n",
      "{'text': \"En el marc del desenvolupament de l'Administració electrònica, des de l'AOC copsem l'estat de l'administració pel que fa als instruments municipals d'Administració electrònica i, més específicament, al reconeixement efectiu dels drets dels ciutadans arrel de la normativa que regula l'ús dels mitjans electrònics, la transparència, l'accés a la informació pública i el bon govern.\\nAmb aquest objectiu, periòdicament duem a terme una revisió de l'estat de l'e-Administració als 947 ajuntaments de Catalunya 1 i publiquem les dades en un informe.\\nAixí mateix, els resultats de l'anàlisi es plasmen en un mapa de Catalunya interactiu.\\nAl mapa municipal català podeu accedir a la informació relacionada amb els 947 municipis catalans:\\nLa informació que mostren els mapes és la més recent.\\nSi voleu conèixer les dades dels estudis anteriors podeu accedir als informes sobre administració.\\nTambé podeu consultar les dades dels 42 consells comarcals al mapa d'e-administració per comarques.\\nEls informes sobre e-Administració són el resultat d'un estudi dut a terme pel personal del Gabinet Tècnic de l'AOC en un període de temps concret i en base a:\\nA continuació detallem els paràmetres analitzats a l'estudi, agrupats en sis blocs\\nSi l'ajuntament ha aprovat normativa en matèria d'administració electrònica\\nSi l'ajuntament ha aprovat alguna norma específica per regular l'ús dels mitjans electrònics en la seva administració (ordenança reguladora d'administració electrònica, registre electrònic i seu electrònica).\\nEl resultat d'aquesta anàlisi es contrasta amb el personal tècnic dels consells comarcals, d'acord amb el conveni de col·laboració que hi ha entre els consells i el Consorci AOC.\\n1 En els tres primers informes de 2010, es van recollir dades de 946 ajuntaments, ja que La Canonja encara no era legalment municipi independent.\\nTambé es va analitzar en el seu moment el municipi de Medinyà fins a la seva forçosa desaparició el febrer de 2018.\\nEvolució de les dades dels informes sobre l'e-Administració (desembre 2017) (977 kB)\\nNOTA: les dades recollides en aquests informes es publiquen ara en format de dades obertes a indicadors públics d'activitat\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n",
      "{'text': \"Cursos de lliure accès per a que els comenceu quan vulgueu i els feu al vostre ritme.\\nCursos amb data d'inici planificada i seguiment per part del tutor\\nUna pinzellada de les principals característiques de cada servei.\\nEs pot completar en uns minuts i us permetrà començar amb bon peu.\\nCreació del preu públic del servei de formació (aprovada a la Comissió Executiva del Consorci AOC de 17 de desembre de 2014):\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n",
      "{'text': 'En aquest document trobareu la darrera versió de les condicions generals de prestació dels Serveis AOC amb la seva data d\\'aprovació per part de la Comissió Executiva del Consorci AOC.\\nTambé trobareu el document immediatament anterior a aquesta versió:\\nCondicions de prestació específiques\\nHi ha serveis que, donades les seves particularitats, disposen addicionalment d\\'unes condicions específiques de prestació.\\nEn cas de dubte o discrepàncies entre l\\'acord específic i el general, les condicions específiques són les que prevalen sobre les genèriques.\\nEn aquests documents trobareu la darrera versió de les condicions específiques amb la seva data d\\'aprovació per part de la Comissió Executiva del Consorci AOC.\\nEn cas d\\'haver-se modificat, també trobareu la versió del document immediatament anterior a l\\'actual:\\nSignatura electrònica i seguretat\\nSi sou usuaris d\\'un dels serveis del Consorci AOC i voleu comunicar-nos la vostra sol·licitud de baixa, feu-ho a través del tràmit de \"Sol·licitud de baixa de servei\" del servei \"Baixa de servei\" que trobareu a l\\'apartat \"Tràmits\", prestador \"Consorci AOC\"\\nAdreceu-vos al Centre d\\'Atenció a l\\'Usuari (CAU) del Consorci AOC.\\nPreus públics del servei de certificació (SCD):\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.'}\n",
      "\n",
      "{'text': \"A continuació podeu accedir als convenis de col·laboració vigents subscrits pel Consorci Administració Oberta de Catalunya:\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n",
      "{'text': \"La informació sobre els processos de selecció de l'AOC es troba a la seu electrònica.\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n",
      "{'text': \"El Consorci Administració Oberta de Catalunya (Consorci AOC) té la seva gènesi en el Pacte per a la promoció i el desenvolupament de la Societat de la Informació a les administracions públiques catalanes (42,56 kB), signat al Parlament de Catalunya el 23 de juliol de 2001, entre els presidents de tots els grups parlamentaris, el Govern de la Generalitat de Catalunya i els governs locals representats per Localret.\\nLa missió de l'AOC és impulsar la transformació digital de les administracions catalanes, per promoure governs àgils, lògics i col·laboratius.\\nI la nostra visió es aconseguir que les persones gaudeixen de serveis públics de qualitat i visquin en una societat oberta.\\nSegons la Llei 29/2010, de 3 d'agost, de l'ús dels mitjans electrònics al sector públic de Catalunya, els objectius estratègics del Consorci AOC són, fonamentalment, col·laborar amb l'Administració de la Generalitat, els ens locals i, si s'escau, altres organismes públics, per:\\na) Promoure la interoperabilitat dels sistemes d'informació catalans amb la resta d'administracions.\\nb) Crear i prestar serveis comuns d'administració electrònica.\\nc) Reutilitzar les aplicacions i els serveis d'administració electrònica que es desenvolupin.\\nd) Garantir la identitat i acreditar la voluntat en les actuacions dels ciutadans i el personal del sector públic, així\\xad com la confidencialitat i el no-rebuig en les comunicacions electròniques.\\nMitjançant el Consorci AOC, també es desenvolupen i executen mesures de cooperació i foment de l'Administració de la Generalitat amb els ens locals en matèria d'ús dels mitjans electrònics.\\nEl Consorci AOC treballa amb una clara vocació de servei a les administracions públiques i la voluntat d'anticipar-se de les necessitats futures que ja es preveuen en l'actual marc normatiu o que demanda la ciutadania.\\nConcretament treballem en les següents línies d'actuació estratègiques:\\n1.- Serveis de col·laboració administrativa, amb l'objectiu de potenciar l'intercanvi d'informació per mitjans telemàtics entre les administracions públiques per millorar la seva eficiència i eficàcia, és a dir, de promoure la interoperabilitat dels sistemes d'informació de les administracions públiques catalanes.\\nI això es fa desenvolupant i prestant:\\n2.- Serveis comuns d'Administració electrònica, per tal de proporcionar suport als projectes d'ús intensiu de les tecnologies de la informació i les comunicacions que impulsin les institucions catalanes, potenciant la reusabilitat i la reutilització de les solucions d'Administració electrònica.\\nAmb aquests serveis el Consorci contribueix a:\\nEls serveis comuns d'Administració electrònica en els que treballa el Consorci es classifiquen en:\\n3.- Serveis d'identitat i signatura electrònica, adreçats als empleats públics i carrecs electes, i també a la ciutadania de Catalunya.\\nEl Consorci exerceix també funcions de divulgació, formació i assessorament, això com quan correspon, suport financer a les entitats locals en el desenvolupament dels seus projectes i iniciatives d'Administració electrònica.\\nEstatuts Consorci Administració Oberta de Catalunya\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset['train']\n",
    "\n",
    "# i want to see the first 10 examples\n",
    "for i in range(10):\n",
    "    print(dataset[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenim diferents mides de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57333\n",
      "369003\n",
      "738007\n"
     ]
    }
   ],
   "source": [
    "# i want to get the first 100 MB of the dataset\n",
    "size = 0\n",
    "for i in range(len(dataset)):\n",
    "    size += len(dataset[i]['text'])\n",
    "    if size > 100000000:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "dataset_100MB = dataset.select(list(range(i)))\n",
    "\n",
    "\n",
    "# i want to get the first 500 MB of the dataset\n",
    "size = 0\n",
    "for j in range(len(dataset)):\n",
    "    size += len(dataset[i]['text'])\n",
    "    if size > 500000000:\n",
    "        print(j)\n",
    "        break\n",
    "\n",
    "dataset_500MB = dataset.select(list(range(j)))\n",
    "\n",
    "\n",
    "# i want to get the first 1 GB of the dataset\n",
    "size = 0\n",
    "for g in range(len(dataset)):\n",
    "    size += len(dataset[i]['text'])\n",
    "    if size > 1000000000:\n",
    "        print(g)\n",
    "        break\n",
    "\n",
    "dataset_1GB = dataset.select(list(range(g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['reduïu', 'els', 'costos', 'dels', 'processos', 'administratius', 'al', 'vostre', 'organisme', 'públic', 'eviteu', 'els', 'desplaçaments', 'i', 'pèrdua', 'de', 'temps', 'als', 'ciutadans', 'en', 'les', 'seves', 'gestions', 'oferiu', 'una', 'administració', 'més', 'transparent', 'a', 'ciutadans', 'i', 'empreses', 'ens', 'grans', 'i', 'petits', 'experimenten', 'aquesta', 'transformació', 'amb', 'èxit', 'gràcies', 'al', 'suport', 'de', 'l', 'aoc', 'departament', 'de', 'sistemes', 'd', 'informació', 'i', 'processos', 'via', 'oberta', 'ens', 'ha', 'permès', 'fer', 'efectiu', 'el', 'dret', 'dels', 'ciutadans', 'a', 'no', 'aportar', 'documents', 'eliminant', 'paper', 'i', 'simplificant', 'procediments', 'e', 'fact', 'proporciona', 'informació', 'indispensable', 'per', 'a', 'la', 'realització', 'de', 'les', 'auditories', 'del', 'registre', 'comptable', 'de', 'factures', 'de', 'les', 'administracions', 'públiques', 'catalanes', 'coordinador', 'del', 'departament', 'd', 'informàtica', 'el', 'servei', 'via', 'oberta', 'és', 'el', 'que', 'ha', 'aportat', 'majors', 'avantatges', 'per', 'als', 'ciutadans', 'amb', 'l', 'e', 'notum', 'hem', 'escurçat', 'els', 'procediments', 'en', '12', 'dies', 'quasi', 'un', '40', 'menys', 'coordinadora', 'd', 'organització', 'de', 'persones', 'i', 'e', 'administració', 'via', 'oberta', 'ofereix', 'millores', 'per', 'als', 'ciutadans', 'al', 'no', 'haver', 'd', 'aportar', 'cap', 'document', 'responsable', 'd', 'informàtica', 'i', 'administració', 'electrònica', 'e', 'tram', 'ens', 'ha', 'permès', 'implantar', 'un', 'servei', 'de', 'tramitació', 'electrònica', 'per', 'als', 'ciutadans', 'de', 'forma', 'ràpida', 'senzilla', 'i', 'amb', 'un', 'cost', 'reduït', 'els', 'municipis', 'amb', 'pocs', 'habitants', 'trobem', 'en', 'els', 'serveis', 'de', 'l', 'aoc', 'la', 'gratuïtat', 'i', 'la', 'comoditat', 'necessàries', 'per', 'dur', 'a', 'terme', 'el', 'nostre', 'dia', 'a', 'dia', 'les', 't', 'cat', 'han', 'permès', 'incorporar', 'de', 'forma', 'segura', 'la', 'signatura', 'electrònica', 'dins', 'dels', 'nostres', 'procediments', 'afavorint', 'la', 'transformació', 'digital', 'de', 'la', 'nostra', 'activitat', 'cap', 'de', 'departament', 'de', 'sistemes', 'i', 'tecnologies', 'de', 'la', 'informació', 'amb', 'el', 'desplegament', 'de', 'l', 'idcat', 'hem', 'apropat', 'l', 'ajuntament', 'a', 'la', 'ciutadania', 'mitjançant', 'els', 'serveis', 'de', 'govern', 'obert', 'de', 'l', 'aoc', 'hem', 'pogut', 'fer', 'fàcil', 'el', 'que', 'sembla', 'difícil', 'al', 'tauler', 'electrònic', 'pots', 'penjar', 'fins', 'i', 'tot', 'el', 'projecte', 'sencer', 'i', 'al', 'final', 'et', 'permet', 'fer', 'també', 'la', 'diligència', 'àrea', 'de', 'promoció', 'econòmica', 'administració', 'i', 'hisenda', 'el', 'sobre', 'digital', 'i', 'la', 'pscp', 'han', 'aconseguit', 'una', 'comunió', 'senzilla', 'entre', 'empreses', 'i', 'administració', 'per', 'universalitzar', 'la', 'compra', 'pública', 'electrònica', 'l', 'e', 'set', 'és', 'la', 'implantació', 'd', 'un', 'nou', 'sistema', 'de', 'treball', 'que', 'facilita', 'la', 'feina', 'del', 'dia', 'a', 'dia', 'cap', 'del', 'servei', 'de', 'contractació', 'i', 'compres', 'el', 'sobre', 'digital', 'una', 'experiència', 'imprescindible', 'per', 'a', 'la', 'bona', 'administració', 'amb', 'estalvi', 'de', 'recursos', 'i', 'millora', 'de', 'la', 'seguretat', 'jurídica', 'i', 'la', 'transparència', 'àrea', 'd', 'organització', 'i', 'administració', 'electrònica', 'el', 'desplegament', 'de', 'la', 'valisa', 'electrònica', 'ha', 'estat', 'clau', 'en', 'el', 'procés', 'de', 'transformació', 'digital', 'dels', 'nostres', 'procediments', 'interns', 'l', 'hèstia', 'permet', 'el', 'treball', 'en', 'temps', 'real', 'i', 'des', 'de', 'qualsevol', 'lloc', 'així', 'com', 'sistematitzar', 'la', 'pràctica', 'professional', 'recollir', 'la', 'informació', 'ordenadament', 'i', 'amb', 'el', 'mateix', 'llenguatge', 'consulta', 'els', 'materials', 'del', 'congrés', 'de', 'govern', 'digital', '2019', 'governs', 'transparents', 'fluids', 'dinàmics', 'líquids', 'un', 'bon', 'lema', 'pel', 'principal', 'objectiu', 'de', 'la', 'governança', 'del', 'segle', 'xxi', 'democratitzar', 'ho', 'tot', 'confluències', 'rius', 'cooperació', 'catalunya', 'mediterrània', 'mar', 'de', 'drets', 'a', 'favor', 'totes', 'les', 'administracions', 'movent', 'se', 'per', 'posar', 'se', 'al', 'dia', 'i', 'millorar', 'tot', 'aprofitant', 'la', 'revolució', 'digital', 'en', 'contra', 'quants', 'cops', 'estem', 'reinventant', 'la', 'roda', 'i', 'quantes', 'quantes', 'oportunitats', 'perdudes', 'de', 'fer', 'ho', 'una', 'única', 'vegada', 'i', 'de', 'forma', 'coordinada', 'i', 'col', 'laborativa', 'la', 'transparència', 'és', 'una', 'oportunitat', 'hem', 'de', 'perdre', 'tota', 'por', 'a', 'explicar', 'què', 'fem', 'la', 'conclusió', 'de', 'la', 'taula', 'd', 'alcaldies', 'de', 'la', 'jornada', 'de', 'govern', 'obert', 'pic', 'twitter', 'com', 'erbglsixzm', 'el', 'director', 'general', 'de', 'participació', 'ciutadana', 'ens', 'convida', 'a', 'transformar', 'les', 'administracions', 'públiques', 'a', 'partir', 'de', 'la', 'participació', 'ciutadana', 'ens', 'cal', 'que', 'allò', 'que', 'preocupa', 'i', 'ocupa', 'els', 'governants', 'formi', 'part', 'd', 'allò', 'en', 'què', 'participa', 'la', 'ciutadania', 'pic', 'twitter', 'com', 'nwqr4ezscs', 'a', 'moltes', 'institucions', 'encara', 'els', 'sona', 'xinés', 'això', 'de', 'les', 'dades', 'obertes', 'i', 'la', 'transparència', 'de', 'que', 'serveix', 'que', 'hi', 'hagi', 'un', 'portal', 'si', 'llavors', 'no', 'hi', 'ha', 'dades', 'llavors', 'l', 'accés', 'a', 'la', 'informació', 'pels', 'periodistes', 'és', 'molt', 'parcial', 'oferim', 'eines', 'que', 'conjuntament', 'amb', 'la', 'metodologia', 'i', 'el', 'suport', 'necessari', 'fan', 'possible', 'l', 'assoliment', 'd', 'un', 'govern', 'digital', 'posem', 'al', 'vostre', 'abast', 'tot', 'el', 'coneixement', 'formació', 'guies', 'normatives', 'etc', 'tenim', 'eines', 'per', 'gestionar', 'àgilment', 'part', 'del', 'procés', 'administratiu', 'del', 'vostre', 'ens', 'el', 'nostre', 'equip', 'farà', 'tot', 'el', 'possible', 'per', 'resoldre', 'les', 'vostres', 'incidències', 'sabem', 'que', 'es', 'tracta', 'd', 'una', 'decisió', 'molt', 'important', 'per', 'al', 'vostre', 'ens', 'i', 'és', 'per', 'això', 'que', 'us', 'ho', 'volem', 'posar', 'fàcil', 'la', 'selecció', 'de', 'l', 'actualitat', 'd', 'administració', 'oberta', 'a', 'la', 'vostra', 'safata'], ['en', 'compliment', 'de', 'la', 'directiva', '2009', '136', 'ce', 'desenvolupada', 'en', 'el', 'nostre', 'ordenament', 'per', 'l', 'article', '22', '2', 'de', 'la', 'llei', 'de', 'serveis', 'de', 'societat', 'de', 'la', 'informació', 'lssi', 'i', 'seguint', 'les', 'instruccions', 'de', 'l', 'agència', 'espanyola', 'de', 'protecció', 'de', 'dades', 'procedim', 'a', 'informar', 'li', 'detalladament', 'de', 'l', 'ús', 'que', 'es', 'realitza', 'a', 'la', 'nostra', 'pàgina', 'web', 'aquesta', 'informació', 'no', 'revela', 'la', 'seva', 'identitat', 'però', 'sí', 'que', 'permet', 'la', 'seva', 'identificació', 'com', 'a', 'un', 'usuari', 'concret', 'i', 'pot', 'guardar', 'informació', 'relativa', 'a', 'la', 'freqüència', 'amb', 'la', 'que', 'visita', 'la', 'pàgina', 'web', 'les', 'seves', 'preferències', 'de', 'navegació', 'o', 'aquella', 'informació', 'que', 'més', 'l', 'interessa', 'el', 'que', 'ens', 'permet', 'cada', 'vegada', 'que', 'accedeix', 'a', 'www', 'aoc', 'cat', 'millorar', 'la', 'qualitat', 'i', 'la', 'usabilitat', 'de', 'la', 'nostra', 'pàgina', 'web', 'no', 'obstant', 'si', 'les', 'desactiva', 'pot', 'ser', 'que', 'la', 'seva', 'navegació', 'per', 'www', 'aoc', 'cat', 'no', 'sigui', 'òptima', 'i', 'algunes', 'de', 'les', 'seves', 'utilitats', 'no', 'funcionin', 'correctament', 'cookies', 'analítiques', 'galetes', 'de', 'google', 'analytics', 'aquesta', 'pàgina', 'web', 'utilitza', 'google', 'analytics', 'un', 'servei', 'analític', 'del', 'web', 'prestat', 'per', 'google', 'inc', 'una', 'companyia', 'de', 'delaware', 'l', 'oficina', 'principal', 'de', 'la', 'qual', 'es', 'troba', 'a', '1600', 'amphitheatre', 'parkway', 'mountain', 'view', 'califòrnia', 'ca', '94043', 'estats', 'units', 'google', 'la', 'informació', 'que', 'genera', 'la', 'cookie', 'sobre', 'l', 'ús', 'del', 'lloc', 'web', 'incloent', 'l', 'adreça', 'ip', 'serà', 'directament', 'transmesa', 'i', 'arxivada', 'per', 'google', 'en', 'els', 'seus', 'servidors', 'd', 'estats', 'units', 'google', 'utilitzarà', 'aquesta', 'informació', 'per', 'compte', 'nostre', 'amb', 'el', 'propòsit', 'de', 'seguir', 'la', 'pista', 'del', 'seu', 'ús', 'del', 'lloc', 'web', 'google', 'podrà', 'transmetre', 'aquesta', 'informació', 'a', 'tercers', 'quan', 'així', 'ho', 'requereixi', 'la', 'legislació', 'o', 'quan', 'aquests', 'tercers', 'processin', 'la', 'informació', 'per', 'compte', 'de', 'google', 'en', 'aquests', 'casos', 'google', 'no', 'associarà', 'la', 'seva', 'adreça', 'ip', 'amb', 'cap', 'altra', 'dada', 'de', 'què', 'disposi', 'en', 'utilitzar', 'aquesta', 'pàgina', 'web', 'consent', 'el', 'tractament', 'de', 'la', 'seva', 'informació', 'per', 'google', 'en', 'la', 'forma', 'i', 'per', 'als', 'fins', 'anteriorment', 'indicats', 'l', 'exercici', 'de', 'qualsevol', 'dret', 's', 'haurà', 'de', 'realitzar', 'mitjançant', 'comunicació', 'directa', 'amb', 'google', 'per', 'optar', 'per', 'no', 'ser', 'rastrejats', 'per', 'google', 'analytics', 'a', 'través', 'de', 'tots', 'els', 'llocs', 'web', 'podeu', 'consultar', 'http', 'tools', 'google', 'com', 'dlpage', 'gaoptout', 'així', 'mateix', 'també', 'registra', 'quan', 'va', 'ser', 'la', 'primera', 'i', 'l', 'última', 'vegada', 'que', 'l', 'usuari', 'va', 'visitar', 'el', 'web', 'www', 'aoc', 'cat', 'galetes', 'en', 'altres', 'llocs', 'web', 'del', 'consorci', 'aoc', 'les', 'seves', 'finalitats', 'són', 'descrites', 'a', 'la', 'pàgina', 'de', 'privacitat', 'de', 'twitter', 'configuració', 'de', 'l', 'usuari', 'per', 'evitar', 'cookies', 'és', 'cas', 'de', 'dubte', 'pot', 'dirigir', 'se', 'al', 'webmaster', 'del', 'domini', 'creador', 'de', 'la', 'cookie', 'la', 'selecció', 'de', 'l', 'actualitat', 'd', 'administració', 'oberta', 'a', 'la', 'vostra', 'safata']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "# Funció per netejar i tokenitzar el text\n",
    "def preprocess_text(text):\n",
    "    # Neteja el text: treu caràcters no desitjats, converteix a minúscules, etc.\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Substitueix caràcters no alfanumèrics per espais\n",
    "    text = text.lower()  # Converteix a minúscules\n",
    "    words = text.split()  # Tokenitza\n",
    "    return words\n",
    "\n",
    "# Carrega el fitxer de text i processa'l\n",
    "corpus_100MB = []\n",
    "for line in dataset_100MB:\n",
    "    words = preprocess_text(line['text'])\n",
    "    if words:  # Assegura't que la línia no està buida\n",
    "        corpus_100MB.append(words)\n",
    "\n",
    "# Comprova algunes línies per assegurar-te que tot està bé\n",
    "print(corpus_100MB[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model de similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/sts-ca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/sts-ca\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "text_semantic = load_dataset(\"projecte-aina/sts-ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model_100MB = word2vec.Word2Vec(corpus_100MB, vector_size=100, window=5, min_count=10, workers=4, epochs=25)\n",
    "model_100MB.save(\"model_100MB\")\n",
    "# Obtenir un word-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100MB = Word2Vec.load(\"model_100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('frase', 0.7243554592132568), ('lletra', 0.6709128618240356), ('grafia', 0.6292251348495483), ('locució', 0.5762051343917847), ('afirmació', 0.5579834580421448), ('pronúncia', 0.556050717830658), ('recomanació', 0.5452941656112671), ('versió', 0.5388414263725281), ('piulada', 0.5328034162521362), ('conjunció', 0.5324693322181702)]\n"
     ]
    }
   ],
   "source": [
    "# Exemple: trobar paraules similars\n",
    "similar_words = model_100MB.wv.most_similar('paraula')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_semantic_train = text_semantic['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_semantic_test = text_semantic['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Atorga per primer cop les mencions Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència Universitària',\n",
       " 'sentence2': 'Creen la menció M. Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència universitària',\n",
       " 'label': 3.5}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_semantic_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprcessament del text_semantic\n",
    "corpus_semantic = []\n",
    "semantic_score = []\n",
    "for line in text_semantic_train:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic.append((frase1, frase2))\n",
    "    semantic_score.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_semantic_test = []\n",
    "semantic_score_test = []\n",
    "for line in text_semantic_test:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic_test.append((frase1, frase2))\n",
    "    semantic_score_test.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari = {}\n",
    "\n",
    "for frases in corpus_semantic:\n",
    "    for frase in frases:\n",
    "        for paraula in frase:\n",
    "            if paraula in vocabulari:\n",
    "                vocabulari[paraula] += 1\n",
    "            else:\n",
    "                vocabulari[paraula] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari_reduit = {palabra for palabra, frecuencia in vocabulari.items() if frecuencia >= 10}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │     \u001b[38;5;34m16,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m20,736\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ embedding[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_and_compile_model(input_length: int = 100, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16) -> tf.keras.Model:\n",
    "    \n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "    \n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True)\n",
    "    \n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "    \n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Exemple d'ús del model\n",
    "baseline_model = build_and_compile_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │     \u001b[38;5;34m16,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m20,736\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ embedding_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_and_compile_model(input_length: int = 764, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16) -> tf.keras.Model:\n",
    "    \n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "    \n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True)\n",
    "    \n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "    \n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Exemple d'ús del model\n",
    "baseline_model = build_and_compile_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ concatenate_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ concatenate_5 (\u001b[38;5;33mConcatenate\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(hidden_size: int = 64) -> tf.keras.Model:\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Concatenate(axis=-1, ),\n",
    "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "\n",
    "# Exemple d'ús del model\n",
    "baseline_model = build_and_compile_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función para calcular la representación one-hot de un texto\n",
    "def compute_one_hot_encoding(text, vocabulari):\n",
    "    \"\"\"\n",
    "    Genera la representación one-hot de un texto utilizando un vocabulario predefinido.\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(len(vocabulari))\n",
    "    vocabulario_list = list(vocabulari)\n",
    "    for palabra in text:\n",
    "        if palabra in vocabulari:\n",
    "            index = vocabulario_list.index(palabra)\n",
    "            one_hot_vector[index] = 1\n",
    "        \n",
    "    return one_hot_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "one_hot_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic):\n",
    "    one_hot_corpus_semantic[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.7375 - val_loss: 0.7943\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.7853 - val_loss: 0.7524\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6658 - val_loss: 0.7523\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5912 - val_loss: 0.7481\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.5544 - val_loss: 0.7425\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.5000 - val_loss: 0.7424\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.4337 - val_loss: 0.7373\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.4066 - val_loss: 0.7502\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3877 - val_loss: 0.7488\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3574 - val_loss: 0.7587\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3502 - val_loss: 0.7401\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3376 - val_loss: 0.7672\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3040 - val_loss: 0.7594\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2838 - val_loss: 0.7728\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2693 - val_loss: 0.7691\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2465 - val_loss: 0.7793\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.2350 - val_loss: 0.7782\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2062 - val_loss: 0.7805\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1985 - val_loss: 0.7869\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2058 - val_loss: 0.7852\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2079 - val_loss: 0.7883\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2048 - val_loss: 0.7877\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1864 - val_loss: 0.7948\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1766 - val_loss: 0.7930\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1654 - val_loss: 0.7895\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1649 - val_loss: 0.7966\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1563 - val_loss: 0.8054\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1690 - val_loss: 0.7936\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1601 - val_loss: 0.7947\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1547 - val_loss: 0.7989\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1407 - val_loss: 0.7896\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1392 - val_loss: 0.7972\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1419 - val_loss: 0.7831\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1474 - val_loss: 0.8021\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1302 - val_loss: 0.7915\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1380 - val_loss: 0.7959\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1309 - val_loss: 0.7951\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1207 - val_loss: 0.7926\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1213 - val_loss: 0.7900\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1238 - val_loss: 0.7934\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1222 - val_loss: 0.7979\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1215 - val_loss: 0.7995\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1180 - val_loss: 0.7924\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1179 - val_loss: 0.7987\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1157 - val_loss: 0.8007\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1167 - val_loss: 0.8082\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1125 - val_loss: 0.8001\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1115 - val_loss: 0.7980\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1111 - val_loss: 0.7908\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1035 - val_loss: 0.8064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e0aeed3dd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "vectors_1 = np.clip(vectors_1, 0, 1)\n",
    "vectors_2 = np.clip(vectors_2, 0, 1)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Error cuadrático medio (MSE): 1.1408123492599302\n",
      "Coeficiente de determinación (R^2): -0.5109907388687134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "one_hot_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic_test):\n",
    "    one_hot_corpus_semantic_test[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Word2Vec pre-entrenats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,000</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_7         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │     \u001b[38;5;34m16,000\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_7         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m20,736\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ embedding_3[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_3[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,057</span> (176.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,057\u001b[0m (176.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_and_compile_model(input_length: int = 100, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16) -> tf.keras.Model:\n",
    "    \n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "    \n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True)\n",
    "    \n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "    \n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Exemple d'ús del model\n",
    "baseline_model = build_and_compile_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Word2Vec + Mean \n",
    "def compute_mean_embedding(text, model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in text if word in model]\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        mean_vector = np.zeros(embedding_dim)\n",
    "    return mean_vector\n",
    "\n",
    "\n",
    "# Word2Vec + Mean Ponderada\n",
    "def compute_weighted_mean_embedding(text, model, word2tfidf, embedding_dim):\n",
    "    \"\"\"\n",
    "    Utilitza una ponderació per a cada vector de paraula, com ara la freqüència \n",
    "    inversa del document (TF-IDF), per calcular una mitjana ponderada dels vectors.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] * word2tfidf[word] for word in text if word in model and word in word2tfidf]\n",
    "    if vectors:\n",
    "        weighted_mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        weighted_mean_vector = np.zeros(embedding_dim)\n",
    "    return weighted_mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "mean_embbeding_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(mean_embbeding_corpus_semantic):\n",
    "    mean_embbeding_corpus_semantic[i] = (compute_mean_embedding(frase1, model_100MB.wv, 100), \n",
    "                                        compute_mean_embedding(frase2, model_100MB.wv, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2073, 100)\n",
      "(2073, 100)\n",
      "(2073,)\n",
      "Epoch 1/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 113ms/step - loss: 1.0416 - val_loss: 0.6647\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 0.6796 - val_loss: 0.6506\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - loss: 0.6749 - val_loss: 0.6265\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - loss: 0.6844 - val_loss: 0.6309\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - loss: 0.6500 - val_loss: 0.6248\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - loss: 0.7021 - val_loss: 0.6774\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 132ms/step - loss: 0.7045 - val_loss: 0.6253\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - loss: 0.6799 - val_loss: 0.6241\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - loss: 0.6634 - val_loss: 0.6213\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 137ms/step - loss: 0.6674 - val_loss: 0.6254\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 132ms/step - loss: 0.7132 - val_loss: 0.6227\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 143ms/step - loss: 0.6588 - val_loss: 0.6241\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 192ms/step - loss: 0.7088 - val_loss: 0.6498\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 158ms/step - loss: 0.6777 - val_loss: 0.6617\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 132ms/step - loss: 0.6758 - val_loss: 0.6224\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - loss: 0.6757 - val_loss: 0.6387\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - loss: 0.6755 - val_loss: 0.6244\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - loss: 0.6735 - val_loss: 0.6292\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - loss: 0.6481 - val_loss: 0.6241\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - loss: 0.6750 - val_loss: 0.6316\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 0.6660 - val_loss: 0.6264\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.6670 - val_loss: 0.6274\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 0.6543 - val_loss: 0.6218\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 0.6726 - val_loss: 0.6320\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 0.6537 - val_loss: 0.6290\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 0.6618 - val_loss: 0.6238\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 98ms/step - loss: 0.6687 - val_loss: 0.6283\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - loss: 0.6607 - val_loss: 0.6379\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 0.6538 - val_loss: 0.6279\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 0.6382 - val_loss: 0.6428\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 0.6623 - val_loss: 0.6256\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 0.6771 - val_loss: 0.6279\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - loss: 0.6711 - val_loss: 0.6366\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 0.6628 - val_loss: 0.6243\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 0.6672 - val_loss: 0.6312\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 0.6534 - val_loss: 0.6540\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 0.6612 - val_loss: 0.6242\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 0.6590 - val_loss: 0.6252\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 0.6616 - val_loss: 0.6273\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - loss: 0.6502 - val_loss: 0.6205\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 0.6732 - val_loss: 0.6529\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.6563 - val_loss: 0.6520\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 0.6659 - val_loss: 0.6390\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.6390 - val_loss: 0.6262\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 0.6502 - val_loss: 0.6270\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 96ms/step - loss: 0.6490 - val_loss: 0.6322\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 0.6563 - val_loss: 0.6512\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 0.6415 - val_loss: 0.6243\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 0.6532 - val_loss: 0.6272\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 0.6357 - val_loss: 0.6332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e0b333b210>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in mean_embbeding_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in mean_embbeding_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step\n",
      "Error cuadrático medio (MSE): 0.7948092671741267\n",
      "Coeficiente de determinación (R^2): -0.05271422863006592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(mean_embbeding_corpus_semantic_test):\n",
    "    mean_embbeding_corpus_semantic_test[i] = (\n",
    "        compute_mean_embedding(frase1, model_100MB.wv, 100),\n",
    "        compute_mean_embedding(frase2, model_100MB.wv, 100)\n",
    "    )\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_md-3.7.0/ca_core_news_md-3.7.0-py3-none-any.whl (49.2 MB)\n",
      "     ---------------------------------------- 0.0/49.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/49.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/49.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/49.2 MB 217.9 kB/s eta 0:03:46\n",
      "     --------------------------------------- 0.0/49.2 MB 245.8 kB/s eta 0:03:21\n",
      "     --------------------------------------- 0.1/49.2 MB 599.1 kB/s eta 0:01:22\n",
      "     ---------------------------------------- 0.4/49.2 MB 1.8 MB/s eta 0:00:27\n",
      "      --------------------------------------- 0.9/49.2 MB 3.3 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 2.0/49.2 MB 6.2 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 2.9/49.2 MB 8.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.8/49.2 MB 9.4 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 4.8/49.2 MB 10.6 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 4.9/49.2 MB 10.4 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 6.1/49.2 MB 11.2 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 7.0/49.2 MB 12.1 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 8.0/49.2 MB 12.8 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 8.6/49.2 MB 13.1 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 9.6/49.2 MB 13.3 MB/s eta 0:00:03\n",
      "     ------- ------------------------------- 10.0/49.2 MB 13.3 MB/s eta 0:00:03\n",
      "     -------- ------------------------------ 11.0/49.2 MB 19.3 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.8/49.2 MB 19.8 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 12.8/49.2 MB 19.3 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 13.8/49.2 MB 18.7 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 14.7/49.2 MB 19.3 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 15.5/49.2 MB 19.8 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 16.3/49.2 MB 19.3 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 17.2/49.2 MB 19.9 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 17.9/49.2 MB 20.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 18.4/49.2 MB 19.3 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 19.4/49.2 MB 21.1 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 21.0/49.2 MB 21.1 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 21.8/49.2 MB 21.1 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 21.9/49.2 MB 20.5 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 23.3/49.2 MB 21.1 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 24.0/49.2 MB 21.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 25.4/49.2 MB 22.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 26.4/49.2 MB 21.8 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 27.8/49.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 28.0/49.2 MB 22.5 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 28.7/49.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 29.7/49.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 30.5/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 31.7/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 32.0/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 33.2/49.2 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 34.4/49.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 35.7/49.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 36.7/49.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 37.2/49.2 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 38.5/49.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 38.6/49.2 MB 19.3 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 39.9/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 41.1/49.2 MB 22.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 41.1/49.2 MB 22.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 42.4/49.2 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 42.4/49.2 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 42.9/49.2 MB 19.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 44.7/49.2 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 45.4/49.2 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 45.9/49.2 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 46.8/49.2 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 47.9/49.2 MB 18.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.2/49.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 49.2/49.2 MB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ca-core-news-md==3.7.0) (3.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pathy>=0.10.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!py -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_md = spacy.load('ca_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_mean_embedding(text1, text2, model):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    text1 = ' '.join(text1)\n",
    "    text2 = ' '.join(text2)\n",
    "    frase1 = model(text1).vector\n",
    "    frase2 = model(text2).vector\n",
    "    \n",
    "    return frase1, frase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "spacy_mean_embbeding_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2073, 300)\n",
      "(2073, 300)\n",
      "(2073,)\n",
      "Epoch 1/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 333ms/step - loss: 0.7655 - val_loss: 0.6411\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 360ms/step - loss: 0.6877 - val_loss: 0.6320\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 376ms/step - loss: 0.6797 - val_loss: 0.6466\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 370ms/step - loss: 0.7062 - val_loss: 0.6940\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 375ms/step - loss: 0.6924 - val_loss: 0.6324\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 372ms/step - loss: 0.6423 - val_loss: 0.6221\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 380ms/step - loss: 0.6710 - val_loss: 0.6314\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 276ms/step - loss: 0.6761 - val_loss: 0.6448\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 256ms/step - loss: 0.6416 - val_loss: 0.6529\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - loss: 0.6720 - val_loss: 0.6706\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 257ms/step - loss: 0.6504 - val_loss: 0.6472\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - loss: 0.6480 - val_loss: 0.6562\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 0.6444 - val_loss: 0.6463\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 253ms/step - loss: 0.6669 - val_loss: 0.6410\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - loss: 0.6382 - val_loss: 0.6422\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 259ms/step - loss: 0.6267 - val_loss: 0.6507\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - loss: 0.6295 - val_loss: 0.6574\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 377ms/step - loss: 0.6223 - val_loss: 0.6604\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 376ms/step - loss: 0.6124 - val_loss: 0.6813\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 368ms/step - loss: 0.6077 - val_loss: 0.6895\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 377ms/step - loss: 0.5932 - val_loss: 0.6499\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 398ms/step - loss: 0.5835 - val_loss: 0.6580\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 408ms/step - loss: 0.5861 - val_loss: 0.6573\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 434ms/step - loss: 0.5834 - val_loss: 0.6477\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 301ms/step - loss: 0.5626 - val_loss: 0.6721\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 364ms/step - loss: 0.5697 - val_loss: 0.6913\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 383ms/step - loss: 0.5709 - val_loss: 0.6821\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 370ms/step - loss: 0.5500 - val_loss: 0.6671\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 299ms/step - loss: 0.5137 - val_loss: 0.6784\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 257ms/step - loss: 0.5431 - val_loss: 0.6896\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 258ms/step - loss: 0.5102 - val_loss: 0.6700\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 0.5072 - val_loss: 0.7029\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 256ms/step - loss: 0.5120 - val_loss: 0.7053\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 257ms/step - loss: 0.4892 - val_loss: 0.7070\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - loss: 0.4925 - val_loss: 0.7240\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 256ms/step - loss: 0.4886 - val_loss: 0.6859\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 250ms/step - loss: 0.4526 - val_loss: 0.7212\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - loss: 0.4500 - val_loss: 0.6966\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - loss: 0.4599 - val_loss: 0.7187\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - loss: 0.4462 - val_loss: 0.7155\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - loss: 0.4572 - val_loss: 0.7242\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 253ms/step - loss: 0.4195 - val_loss: 0.7118\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 260ms/step - loss: 0.4375 - val_loss: 0.7264\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - loss: 0.4240 - val_loss: 0.7134\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 271ms/step - loss: 0.4131 - val_loss: 0.7021\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 250ms/step - loss: 0.3990 - val_loss: 0.7361\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - loss: 0.4166 - val_loss: 0.7068\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - loss: 0.3635 - val_loss: 0.7378\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - loss: 0.3811 - val_loss: 0.7404\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - loss: 0.3718 - val_loss: 0.7208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e08f980690>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step\n",
      "Error cuadrático medio (MSE): 0.9937982237121572\n",
      "Coeficiente de determinación (R^2): -0.3162723779678345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-trf==3.7.2\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_trf-3.7.2/ca_core_news_trf-3.7.2-py3-none-any.whl (457.1 MB)\n",
      "     ---------------------------------------- 0.0/457.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/457.1 MB 6.9 MB/s eta 0:01:06\n",
      "     ---------------------------------------- 0.3/457.1 MB 6.3 MB/s eta 0:01:13\n",
      "     ---------------------------------------- 0.6/457.1 MB 4.6 MB/s eta 0:01:40\n",
      "     ---------------------------------------- 0.6/457.1 MB 4.6 MB/s eta 0:01:40\n",
      "     ---------------------------------------- 0.9/457.1 MB 3.6 MB/s eta 0:02:06\n",
      "     ---------------------------------------- 1.3/457.1 MB 4.5 MB/s eta 0:01:42\n",
      "     ---------------------------------------- 1.6/457.1 MB 4.8 MB/s eta 0:01:35\n",
      "     ---------------------------------------- 1.8/457.1 MB 5.0 MB/s eta 0:01:31\n",
      "     ---------------------------------------- 2.0/457.1 MB 4.6 MB/s eta 0:01:39\n",
      "     ---------------------------------------- 2.4/457.1 MB 5.0 MB/s eta 0:01:31\n",
      "     ---------------------------------------- 2.6/457.1 MB 4.9 MB/s eta 0:01:33\n",
      "     ---------------------------------------- 2.8/457.1 MB 4.8 MB/s eta 0:01:34\n",
      "     ---------------------------------------- 3.2/457.1 MB 5.0 MB/s eta 0:01:31\n",
      "     ---------------------------------------- 3.4/457.1 MB 4.9 MB/s eta 0:01:33\n",
      "     ---------------------------------------- 3.6/457.1 MB 4.8 MB/s eta 0:01:34\n",
      "     ---------------------------------------- 3.8/457.1 MB 4.8 MB/s eta 0:01:35\n",
      "     ---------------------------------------- 4.0/457.1 MB 4.7 MB/s eta 0:01:38\n",
      "     ---------------------------------------- 4.1/457.1 MB 4.6 MB/s eta 0:01:40\n",
      "     ---------------------------------------- 4.5/457.1 MB 4.7 MB/s eta 0:01:37\n",
      "     ---------------------------------------- 4.6/457.1 MB 4.8 MB/s eta 0:01:35\n",
      "     ---------------------------------------- 5.0/457.1 MB 4.7 MB/s eta 0:01:36\n",
      "     ---------------------------------------- 5.4/457.1 MB 5.0 MB/s eta 0:01:31\n",
      "      --------------------------------------- 5.8/457.1 MB 5.0 MB/s eta 0:01:31\n",
      "      --------------------------------------- 6.1/457.1 MB 5.0 MB/s eta 0:01:30\n",
      "      --------------------------------------- 6.5/457.1 MB 5.2 MB/s eta 0:01:27\n",
      "      --------------------------------------- 6.8/457.1 MB 5.3 MB/s eta 0:01:25\n",
      "      --------------------------------------- 7.1/457.1 MB 5.2 MB/s eta 0:01:27\n",
      "      --------------------------------------- 7.3/457.1 MB 5.2 MB/s eta 0:01:27\n",
      "      --------------------------------------- 7.8/457.1 MB 5.2 MB/s eta 0:01:26\n",
      "      --------------------------------------- 8.2/457.1 MB 5.4 MB/s eta 0:01:24\n",
      "      --------------------------------------- 8.7/457.1 MB 5.5 MB/s eta 0:01:22\n",
      "      --------------------------------------- 8.9/457.1 MB 5.5 MB/s eta 0:01:22\n",
      "      --------------------------------------- 9.3/457.1 MB 5.5 MB/s eta 0:01:22\n",
      "      --------------------------------------- 9.7/457.1 MB 5.7 MB/s eta 0:01:19\n",
      "      -------------------------------------- 10.2/457.1 MB 5.7 MB/s eta 0:01:19\n",
      "      -------------------------------------- 10.5/457.1 MB 5.8 MB/s eta 0:01:17\n",
      "      -------------------------------------- 11.1/457.1 MB 6.1 MB/s eta 0:01:13\n",
      "      -------------------------------------- 11.3/457.1 MB 6.2 MB/s eta 0:01:13\n",
      "      -------------------------------------- 11.7/457.1 MB 6.1 MB/s eta 0:01:14\n",
      "     - ------------------------------------- 12.0/457.1 MB 6.1 MB/s eta 0:01:14\n",
      "     - ------------------------------------- 12.6/457.1 MB 6.2 MB/s eta 0:01:12\n",
      "     - ------------------------------------- 13.0/457.1 MB 6.5 MB/s eta 0:01:08\n",
      "     - ------------------------------------- 13.4/457.1 MB 6.4 MB/s eta 0:01:10\n",
      "     - ------------------------------------- 13.8/457.1 MB 6.7 MB/s eta 0:01:06\n",
      "     - ------------------------------------- 14.2/457.1 MB 7.0 MB/s eta 0:01:04\n",
      "     - ------------------------------------- 14.7/457.1 MB 7.4 MB/s eta 0:01:01\n",
      "     - ------------------------------------- 14.9/457.1 MB 7.5 MB/s eta 0:00:59\n",
      "     - ------------------------------------- 15.5/457.1 MB 7.4 MB/s eta 0:01:01\n",
      "     - ------------------------------------- 16.1/457.1 MB 7.6 MB/s eta 0:00:58\n",
      "     - ------------------------------------- 16.6/457.1 MB 7.7 MB/s eta 0:00:58\n",
      "     - ------------------------------------- 17.2/457.1 MB 8.1 MB/s eta 0:00:55\n",
      "     - ------------------------------------- 17.5/457.1 MB 8.0 MB/s eta 0:00:56\n",
      "     - ------------------------------------- 18.1/457.1 MB 8.5 MB/s eta 0:00:52\n",
      "     - ------------------------------------- 18.6/457.1 MB 8.4 MB/s eta 0:00:53\n",
      "     - ------------------------------------- 19.2/457.1 MB 8.7 MB/s eta 0:00:51\n",
      "     - ------------------------------------- 19.5/457.1 MB 8.6 MB/s eta 0:00:51\n",
      "     - ------------------------------------- 19.9/457.1 MB 8.8 MB/s eta 0:00:50\n",
      "     - ------------------------------------- 20.5/457.1 MB 8.8 MB/s eta 0:00:50\n",
      "     - ------------------------------------- 21.1/457.1 MB 9.0 MB/s eta 0:00:49\n",
      "     - ------------------------------------- 21.6/457.1 MB 9.4 MB/s eta 0:00:47\n",
      "     - ------------------------------------- 22.0/457.1 MB 9.4 MB/s eta 0:00:47\n",
      "     - ------------------------------------- 22.5/457.1 MB 9.5 MB/s eta 0:00:46\n",
      "     - ------------------------------------- 22.9/457.1 MB 9.5 MB/s eta 0:00:46\n",
      "     -- ------------------------------------ 23.6/457.1 MB 9.8 MB/s eta 0:00:45\n",
      "     -- ------------------------------------ 24.0/457.1 MB 9.8 MB/s eta 0:00:45\n",
      "     -- ------------------------------------ 24.2/457.1 MB 9.8 MB/s eta 0:00:45\n",
      "     -- ------------------------------------ 24.2/457.1 MB 9.8 MB/s eta 0:00:45\n",
      "     -- ------------------------------------ 24.3/457.1 MB 8.8 MB/s eta 0:00:49\n",
      "     -- ------------------------------------ 24.6/457.1 MB 8.6 MB/s eta 0:00:51\n",
      "     -- ------------------------------------ 25.0/457.1 MB 8.8 MB/s eta 0:00:49\n",
      "     -- ------------------------------------ 25.0/457.1 MB 8.8 MB/s eta 0:00:49\n",
      "     -- ------------------------------------ 25.0/457.1 MB 8.1 MB/s eta 0:00:54\n",
      "     -- ------------------------------------ 25.4/457.1 MB 7.9 MB/s eta 0:00:55\n",
      "     -- ------------------------------------ 25.6/457.1 MB 7.6 MB/s eta 0:00:57\n",
      "     -- ------------------------------------ 25.7/457.1 MB 7.5 MB/s eta 0:00:58\n",
      "     -- ------------------------------------ 26.0/457.1 MB 7.2 MB/s eta 0:01:00\n",
      "     -- ------------------------------------ 26.3/457.1 MB 7.3 MB/s eta 0:01:00\n",
      "     -- ------------------------------------ 27.0/457.1 MB 7.1 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 27.4/457.1 MB 7.2 MB/s eta 0:01:00\n",
      "     -- ------------------------------------ 28.1/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 28.7/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 29.1/457.1 MB 7.0 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 29.5/457.1 MB 7.0 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 30.0/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 30.5/457.1 MB 7.0 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 30.9/457.1 MB 6.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 31.3/457.1 MB 6.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 31.8/457.1 MB 6.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 32.4/457.1 MB 6.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 32.8/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 33.4/457.1 MB 6.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 34.0/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 34.4/457.1 MB 7.0 MB/s eta 0:01:01\n",
      "     --- ----------------------------------- 35.2/457.1 MB 7.7 MB/s eta 0:00:55\n",
      "     --- ----------------------------------- 35.7/457.1 MB 8.7 MB/s eta 0:00:49\n",
      "     --- ----------------------------------- 36.3/457.1 MB 9.4 MB/s eta 0:00:46\n",
      "     --- ----------------------------------- 36.7/457.1 MB 9.4 MB/s eta 0:00:45\n",
      "     --- ----------------------------------- 37.3/457.1 MB 9.4 MB/s eta 0:00:45\n",
      "     --- ----------------------------------- 37.9/457.1 MB 9.2 MB/s eta 0:00:46\n",
      "     --- ----------------------------------- 38.5/457.1 MB 9.6 MB/s eta 0:00:44\n",
      "     --- ----------------------------------- 39.1/457.1 MB 9.2 MB/s eta 0:00:46\n",
      "     --- ----------------------------------- 40.0/457.1 MB 9.4 MB/s eta 0:00:45\n",
      "     --- ----------------------------------- 40.6/457.1 MB 9.6 MB/s eta 0:00:44\n",
      "     --- ----------------------------------- 41.1/457.1 MB 9.8 MB/s eta 0:00:43\n",
      "     --- ----------------------------------- 41.6/457.1 MB 9.8 MB/s eta 0:00:43\n",
      "     --- ----------------------------------- 42.3/457.1 MB 9.6 MB/s eta 0:00:44\n",
      "     --- ----------------------------------- 43.1/457.1 MB 9.5 MB/s eta 0:00:44\n",
      "     --- ----------------------------------- 43.9/457.1 MB 9.8 MB/s eta 0:00:43\n",
      "     --- ----------------------------------- 44.5/457.1 MB 9.9 MB/s eta 0:00:42\n",
      "     --- ----------------------------------- 44.9/457.1 MB 9.9 MB/s eta 0:00:42\n",
      "     --- ----------------------------------- 45.2/457.1 MB 9.5 MB/s eta 0:00:44\n",
      "     --- ----------------------------------- 45.4/457.1 MB 9.0 MB/s eta 0:00:46\n",
      "     --- ---------------------------------- 47.0/457.1 MB 10.2 MB/s eta 0:00:41\n",
      "     ---- --------------------------------- 48.5/457.1 MB 11.1 MB/s eta 0:00:37\n",
      "     ---- --------------------------------- 49.2/457.1 MB 11.7 MB/s eta 0:00:35\n",
      "     ---- --------------------------------- 49.8/457.1 MB 11.7 MB/s eta 0:00:35\n",
      "     ---- --------------------------------- 50.3/457.1 MB 11.1 MB/s eta 0:00:37\n",
      "     ---- --------------------------------- 51.0/457.1 MB 11.3 MB/s eta 0:00:37\n",
      "     ---- --------------------------------- 51.7/457.1 MB 11.3 MB/s eta 0:00:36\n",
      "     ---- --------------------------------- 52.3/457.1 MB 11.5 MB/s eta 0:00:36\n",
      "     ---- --------------------------------- 52.9/457.1 MB 11.5 MB/s eta 0:00:36\n",
      "     ---- --------------------------------- 53.7/457.1 MB 11.9 MB/s eta 0:00:34\n",
      "     ---- --------------------------------- 55.1/457.1 MB 13.1 MB/s eta 0:00:31\n",
      "     ---- --------------------------------- 55.9/457.1 MB 15.6 MB/s eta 0:00:26\n",
      "     ---- --------------------------------- 56.1/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 56.7/457.1 MB 13.4 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 57.5/457.1 MB 12.6 MB/s eta 0:00:32\n",
      "     ---- --------------------------------- 57.7/457.1 MB 11.9 MB/s eta 0:00:34\n",
      "     ---- --------------------------------- 58.1/457.1 MB 11.1 MB/s eta 0:00:36\n",
      "     ---- --------------------------------- 58.4/457.1 MB 10.7 MB/s eta 0:00:38\n",
      "     ---- --------------------------------- 58.7/457.1 MB 10.9 MB/s eta 0:00:37\n",
      "     ---- --------------------------------- 59.2/457.1 MB 10.2 MB/s eta 0:00:39\n",
      "     ----- --------------------------------- 59.7/457.1 MB 9.9 MB/s eta 0:00:41\n",
      "     ----- -------------------------------- 60.3/457.1 MB 10.2 MB/s eta 0:00:39\n",
      "     ----- -------------------------------- 60.9/457.1 MB 10.2 MB/s eta 0:00:39\n",
      "     ----- -------------------------------- 61.0/457.1 MB 10.2 MB/s eta 0:00:39\n",
      "     ----- --------------------------------- 62.1/457.1 MB 9.8 MB/s eta 0:00:41\n",
      "     ----- --------------------------------- 62.6/457.1 MB 9.8 MB/s eta 0:00:41\n",
      "     ----- --------------------------------- 63.2/457.1 MB 9.8 MB/s eta 0:00:41\n",
      "     ----- --------------------------------- 63.5/457.1 MB 9.5 MB/s eta 0:00:42\n",
      "     ----- --------------------------------- 63.8/457.1 MB 8.7 MB/s eta 0:00:46\n",
      "     ----- --------------------------------- 65.3/457.1 MB 8.6 MB/s eta 0:00:46\n",
      "     ----- --------------------------------- 65.7/457.1 MB 8.5 MB/s eta 0:00:47\n",
      "     ----- --------------------------------- 66.3/457.1 MB 8.7 MB/s eta 0:00:45\n",
      "     ----- --------------------------------- 66.8/457.1 MB 8.5 MB/s eta 0:00:46\n",
      "     ----- --------------------------------- 68.0/457.1 MB 9.2 MB/s eta 0:00:43\n",
      "     ----- --------------------------------- 69.0/457.1 MB 9.8 MB/s eta 0:00:40\n",
      "     ----- -------------------------------- 69.7/457.1 MB 10.4 MB/s eta 0:00:38\n",
      "     ----- -------------------------------- 70.6/457.1 MB 10.6 MB/s eta 0:00:37\n",
      "     ----- -------------------------------- 71.1/457.1 MB 10.4 MB/s eta 0:00:38\n",
      "     ----- -------------------------------- 72.0/457.1 MB 10.7 MB/s eta 0:00:36\n",
      "     ------ ------------------------------- 72.5/457.1 MB 10.9 MB/s eta 0:00:36\n",
      "     ------ ------------------------------- 73.2/457.1 MB 11.1 MB/s eta 0:00:35\n",
      "     ------ ------------------------------- 73.9/457.1 MB 12.8 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 74.9/457.1 MB 12.6 MB/s eta 0:00:31\n",
      "     ------ ------------------------------- 75.6/457.1 MB 12.1 MB/s eta 0:00:32\n",
      "     ------ ------------------------------- 76.2/457.1 MB 12.6 MB/s eta 0:00:31\n",
      "     ------ ------------------------------- 77.0/457.1 MB 13.1 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 77.6/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 78.2/457.1 MB 12.6 MB/s eta 0:00:31\n",
      "     ------ ------------------------------- 78.9/457.1 MB 12.6 MB/s eta 0:00:31\n",
      "     ------ ------------------------------- 79.5/457.1 MB 12.6 MB/s eta 0:00:31\n",
      "     ------ ------------------------------- 80.0/457.1 MB 12.8 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 80.5/457.1 MB 11.9 MB/s eta 0:00:32\n",
      "     ------ ------------------------------- 81.2/457.1 MB 11.1 MB/s eta 0:00:34\n",
      "     ------ ------------------------------- 83.3/457.1 MB 12.6 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 83.9/457.1 MB 12.6 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 84.4/457.1 MB 12.3 MB/s eta 0:00:31\n",
      "     ------- ------------------------------ 85.5/457.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ------- ------------------------------ 86.2/457.1 MB 12.6 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 86.8/457.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 87.6/457.1 MB 12.6 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 88.2/457.1 MB 12.6 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 88.8/457.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ------- ------------------------------ 89.7/457.1 MB 12.1 MB/s eta 0:00:31\n",
      "     ------- ------------------------------ 90.5/457.1 MB 12.8 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 91.0/457.1 MB 13.9 MB/s eta 0:00:27\n",
      "     ------- ------------------------------ 91.9/457.1 MB 13.9 MB/s eta 0:00:27\n",
      "     ------- ------------------------------ 92.9/457.1 MB 13.1 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 93.7/457.1 MB 12.6 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 94.4/457.1 MB 13.1 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 95.1/457.1 MB 13.4 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 96.1/457.1 MB 10.4 MB/s eta 0:00:35\n",
      "     -------- ----------------------------- 97.1/457.1 MB 10.6 MB/s eta 0:00:35\n",
      "     -------- ------------------------------ 97.8/457.1 MB 9.9 MB/s eta 0:00:37\n",
      "     -------- ------------------------------ 98.1/457.1 MB 9.6 MB/s eta 0:00:38\n",
      "     -------- ----------------------------- 99.1/457.1 MB 10.2 MB/s eta 0:00:35\n",
      "     -------- ---------------------------- 101.8/457.1 MB 11.7 MB/s eta 0:00:31\n",
      "     -------- ---------------------------- 104.4/457.1 MB 13.6 MB/s eta 0:00:26\n",
      "     -------- ---------------------------- 105.1/457.1 MB 13.6 MB/s eta 0:00:26\n",
      "     -------- ---------------------------- 105.6/457.1 MB 13.9 MB/s eta 0:00:26\n",
      "     -------- ---------------------------- 106.5/457.1 MB 18.7 MB/s eta 0:00:19\n",
      "     -------- ---------------------------- 107.1/457.1 MB 18.2 MB/s eta 0:00:20\n",
      "     -------- ---------------------------- 107.4/457.1 MB 17.2 MB/s eta 0:00:21\n",
      "     -------- ---------------------------- 108.9/457.1 MB 18.7 MB/s eta 0:00:19\n",
      "     -------- ---------------------------- 110.2/457.1 MB 18.7 MB/s eta 0:00:19\n",
      "     -------- ---------------------------- 110.9/457.1 MB 17.2 MB/s eta 0:00:21\n",
      "     --------- --------------------------- 111.6/457.1 MB 16.0 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 112.4/457.1 MB 15.2 MB/s eta 0:00:23\n",
      "     --------- --------------------------- 113.2/457.1 MB 14.2 MB/s eta 0:00:25\n",
      "     --------- --------------------------- 113.9/457.1 MB 13.1 MB/s eta 0:00:27\n",
      "     --------- --------------------------- 114.3/457.1 MB 12.6 MB/s eta 0:00:28\n",
      "     --------- --------------------------- 114.7/457.1 MB 11.9 MB/s eta 0:00:29\n",
      "     --------- --------------------------- 115.1/457.1 MB 11.3 MB/s eta 0:00:31\n",
      "     --------- --------------------------- 115.7/457.1 MB 11.1 MB/s eta 0:00:31\n",
      "     --------- --------------------------- 116.0/457.1 MB 11.1 MB/s eta 0:00:31\n",
      "     --------- --------------------------- 116.3/457.1 MB 10.6 MB/s eta 0:00:33\n",
      "     --------- --------------------------- 116.9/457.1 MB 10.2 MB/s eta 0:00:34\n",
      "     --------- --------------------------- 117.5/457.1 MB 10.2 MB/s eta 0:00:34\n",
      "     --------- --------------------------- 117.9/457.1 MB 11.1 MB/s eta 0:00:31\n",
      "     --------- --------------------------- 118.3/457.1 MB 10.6 MB/s eta 0:00:33\n",
      "     --------- --------------------------- 119.0/457.1 MB 10.1 MB/s eta 0:00:34\n",
      "     --------- ---------------------------- 119.6/457.1 MB 9.6 MB/s eta 0:00:36\n",
      "     --------- ---------------------------- 120.2/457.1 MB 9.2 MB/s eta 0:00:37\n",
      "     ---------- --------------------------- 120.9/457.1 MB 9.2 MB/s eta 0:00:37\n",
      "     ---------- --------------------------- 121.5/457.1 MB 9.1 MB/s eta 0:00:37\n",
      "     ---------- --------------------------- 122.1/457.1 MB 9.0 MB/s eta 0:00:38\n",
      "     ---------- --------------------------- 123.0/457.1 MB 8.5 MB/s eta 0:00:40\n",
      "     ---------- --------------------------- 124.3/457.1 MB 9.1 MB/s eta 0:00:37\n",
      "     ---------- --------------------------- 124.5/457.1 MB 8.5 MB/s eta 0:00:40\n",
      "     ---------- --------------------------- 124.9/457.1 MB 8.2 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 126.6/457.1 MB 9.9 MB/s eta 0:00:34\n",
      "     ---------- --------------------------- 127.0/457.1 MB 9.9 MB/s eta 0:00:34\n",
      "     ---------- --------------------------- 127.3/457.1 MB 9.5 MB/s eta 0:00:35\n",
      "     ---------- --------------------------- 127.7/457.1 MB 9.0 MB/s eta 0:00:37\n",
      "     ---------- --------------------------- 128.9/457.1 MB 9.9 MB/s eta 0:00:34\n",
      "     ---------- -------------------------- 130.2/457.1 MB 10.7 MB/s eta 0:00:31\n",
      "     ---------- -------------------------- 130.7/457.1 MB 10.9 MB/s eta 0:00:30\n",
      "     ---------- -------------------------- 131.1/457.1 MB 10.4 MB/s eta 0:00:32\n",
      "     ---------- --------------------------- 131.5/457.1 MB 9.9 MB/s eta 0:00:33\n",
      "     ---------- -------------------------- 132.9/457.1 MB 10.7 MB/s eta 0:00:31\n",
      "     ---------- -------------------------- 134.1/457.1 MB 10.7 MB/s eta 0:00:31\n",
      "     ---------- -------------------------- 134.6/457.1 MB 10.7 MB/s eta 0:00:31\n",
      "     ----------- ------------------------- 136.2/457.1 MB 11.1 MB/s eta 0:00:29\n",
      "     ----------- -------------------------- 136.7/457.1 MB 9.8 MB/s eta 0:00:33\n",
      "     ----------- ------------------------- 138.0/457.1 MB 12.6 MB/s eta 0:00:26\n",
      "     ----------- ------------------------- 138.9/457.1 MB 11.7 MB/s eta 0:00:28\n",
      "     ----------- ------------------------- 139.3/457.1 MB 11.1 MB/s eta 0:00:29\n",
      "     ----------- -------------------------- 140.7/457.1 MB 9.9 MB/s eta 0:00:32\n",
      "     ----------- ------------------------- 141.4/457.1 MB 10.7 MB/s eta 0:00:30\n",
      "     ----------- ------------------------- 141.9/457.1 MB 11.7 MB/s eta 0:00:28\n",
      "     ----------- ------------------------- 142.4/457.1 MB 10.7 MB/s eta 0:00:30\n",
      "     ----------- ------------------------- 143.5/457.1 MB 10.2 MB/s eta 0:00:31\n",
      "     ----------- -------------------------- 144.1/457.1 MB 9.8 MB/s eta 0:00:33\n",
      "     ------------ ------------------------- 144.8/457.1 MB 9.6 MB/s eta 0:00:33\n",
      "     ----------- ------------------------- 145.5/457.1 MB 10.6 MB/s eta 0:00:30\n",
      "     ----------- ------------------------- 146.1/457.1 MB 10.1 MB/s eta 0:00:31\n",
      "     ----------- ------------------------- 146.8/457.1 MB 10.1 MB/s eta 0:00:31\n",
      "     ----------- ------------------------- 147.4/457.1 MB 10.4 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 148.3/457.1 MB 10.4 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 148.9/457.1 MB 10.2 MB/s eta 0:00:31\n",
      "     ------------ ------------------------ 149.4/457.1 MB 10.6 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 150.5/457.1 MB 11.3 MB/s eta 0:00:28\n",
      "     ------------ ------------------------ 151.0/457.1 MB 10.6 MB/s eta 0:00:29\n",
      "     ------------ ------------------------ 151.5/457.1 MB 10.2 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 152.2/457.1 MB 10.6 MB/s eta 0:00:29\n",
      "     ------------ ------------------------ 152.9/457.1 MB 11.1 MB/s eta 0:00:28\n",
      "     ------------ ------------------------ 153.4/457.1 MB 10.6 MB/s eta 0:00:29\n",
      "     ------------ ------------------------ 154.0/457.1 MB 10.1 MB/s eta 0:00:31\n",
      "     ------------ ------------------------- 154.8/457.1 MB 9.9 MB/s eta 0:00:31\n",
      "     ------------ ------------------------ 156.0/457.1 MB 10.2 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 156.5/457.1 MB 10.2 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 157.1/457.1 MB 10.1 MB/s eta 0:00:30\n",
      "     ------------ ------------------------ 158.0/457.1 MB 10.1 MB/s eta 0:00:30\n",
      "     ------------- ------------------------ 158.6/457.1 MB 9.9 MB/s eta 0:00:31\n",
      "     ------------- ------------------------ 159.1/457.1 MB 9.9 MB/s eta 0:00:31\n",
      "     ------------ ------------------------ 159.8/457.1 MB 10.1 MB/s eta 0:00:30\n",
      "     ------------- ------------------------ 160.4/457.1 MB 9.8 MB/s eta 0:00:31\n",
      "     ------------- ------------------------ 160.9/457.1 MB 9.4 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 161.5/457.1 MB 9.5 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 162.2/457.1 MB 9.5 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 162.6/457.1 MB 9.2 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 163.3/457.1 MB 9.2 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 163.9/457.1 MB 9.5 MB/s eta 0:00:31\n",
      "     ------------- ------------------------ 164.4/457.1 MB 9.9 MB/s eta 0:00:30\n",
      "     ------------- ------------------------ 165.1/457.1 MB 9.5 MB/s eta 0:00:31\n",
      "     ------------- ------------------------ 165.8/457.1 MB 9.1 MB/s eta 0:00:33\n",
      "     ------------- ------------------------ 166.5/457.1 MB 9.1 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 167.3/457.1 MB 9.1 MB/s eta 0:00:32\n",
      "     ------------- ------------------------ 167.8/457.1 MB 8.7 MB/s eta 0:00:34\n",
      "     ------------- ------------------------ 168.1/457.1 MB 8.5 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 169.0/457.1 MB 8.7 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 169.2/457.1 MB 8.4 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 169.7/457.1 MB 8.4 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 170.5/457.1 MB 8.4 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 170.9/457.1 MB 8.3 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 171.4/457.1 MB 8.3 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 172.0/457.1 MB 8.3 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 172.7/457.1 MB 8.4 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 173.1/457.1 MB 8.4 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 173.7/457.1 MB 8.3 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 174.3/457.1 MB 8.2 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 174.8/457.1 MB 8.3 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 175.5/457.1 MB 8.2 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 177.2/457.1 MB 9.0 MB/s eta 0:00:32\n",
      "     -------------- ----------------------- 177.7/457.1 MB 9.2 MB/s eta 0:00:31\n",
      "     -------------- ----------------------- 178.2/457.1 MB 9.5 MB/s eta 0:00:30\n",
      "     -------------- ----------------------- 178.7/457.1 MB 9.5 MB/s eta 0:00:30\n",
      "     -------------- ----------------------- 179.2/457.1 MB 9.0 MB/s eta 0:00:31\n",
      "     -------------- ----------------------- 179.9/457.1 MB 9.4 MB/s eta 0:00:30\n",
      "     -------------- ----------------------- 180.4/457.1 MB 9.4 MB/s eta 0:00:30\n",
      "     --------------- ---------------------- 180.9/457.1 MB 9.6 MB/s eta 0:00:29\n",
      "     --------------- ---------------------- 181.2/457.1 MB 9.5 MB/s eta 0:00:30\n",
      "     -------------- ---------------------- 183.0/457.1 MB 11.1 MB/s eta 0:00:25\n",
      "     -------------- ---------------------- 183.6/457.1 MB 11.3 MB/s eta 0:00:25\n",
      "     -------------- ---------------------- 184.1/457.1 MB 11.5 MB/s eta 0:00:24\n",
      "     -------------- ---------------------- 184.5/457.1 MB 11.1 MB/s eta 0:00:25\n",
      "     -------------- ---------------------- 185.0/457.1 MB 10.7 MB/s eta 0:00:26\n",
      "     --------------- --------------------- 186.3/457.1 MB 11.3 MB/s eta 0:00:24\n",
      "     --------------- --------------------- 186.8/457.1 MB 10.9 MB/s eta 0:00:25\n",
      "     --------------- --------------------- 187.2/457.1 MB 10.2 MB/s eta 0:00:27\n",
      "     --------------- ---------------------- 187.8/457.1 MB 9.8 MB/s eta 0:00:28\n",
      "     --------------- --------------------- 188.7/457.1 MB 10.2 MB/s eta 0:00:27\n",
      "     --------------- --------------------- 189.3/457.1 MB 10.7 MB/s eta 0:00:25\n",
      "     --------------- --------------------- 189.7/457.1 MB 10.4 MB/s eta 0:00:26\n",
      "     --------------- --------------------- 190.2/457.1 MB 10.4 MB/s eta 0:00:26\n",
      "     --------------- --------------------- 190.9/457.1 MB 10.4 MB/s eta 0:00:26\n",
      "     --------------- --------------------- 191.5/457.1 MB 10.9 MB/s eta 0:00:25\n",
      "     --------------- --------------------- 191.9/457.1 MB 10.2 MB/s eta 0:00:26\n",
      "     ---------------- --------------------- 192.5/457.1 MB 9.6 MB/s eta 0:00:28\n",
      "     ---------------- --------------------- 192.8/457.1 MB 9.0 MB/s eta 0:00:30\n",
      "     --------------- --------------------- 195.0/457.1 MB 10.6 MB/s eta 0:00:25\n",
      "     --------------- --------------------- 195.5/457.1 MB 10.7 MB/s eta 0:00:25\n",
      "     ---------------- --------------------- 196.0/457.1 MB 9.9 MB/s eta 0:00:27\n",
      "     --------------- --------------------- 197.4/457.1 MB 10.2 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 197.7/457.1 MB 10.4 MB/s eta 0:00:25\n",
      "     ---------------- -------------------- 198.2/457.1 MB 10.2 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 199.5/457.1 MB 10.4 MB/s eta 0:00:25\n",
      "     ---------------- -------------------- 199.9/457.1 MB 10.4 MB/s eta 0:00:25\n",
      "     ---------------- -------------------- 200.5/457.1 MB 10.6 MB/s eta 0:00:25\n",
      "     ---------------- -------------------- 201.2/457.1 MB 10.4 MB/s eta 0:00:25\n",
      "     ---------------- -------------------- 201.5/457.1 MB 10.1 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 201.5/457.1 MB 10.1 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 201.5/457.1 MB 10.1 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 201.5/457.1 MB 10.1 MB/s eta 0:00:26\n",
      "     ---------------- -------------------- 201.5/457.1 MB 10.1 MB/s eta 0:00:26\n",
      "     ---------------- --------------------- 202.0/457.1 MB 8.1 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 202.6/457.1 MB 8.1 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 203.0/457.1 MB 8.2 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 203.9/457.1 MB 8.1 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 204.5/457.1 MB 7.6 MB/s eta 0:00:34\n",
      "     ----------------- -------------------- 204.8/457.1 MB 7.3 MB/s eta 0:00:35\n",
      "     ----------------- -------------------- 205.4/457.1 MB 7.0 MB/s eta 0:00:36\n",
      "     ----------------- -------------------- 206.1/457.1 MB 7.1 MB/s eta 0:00:36\n",
      "     ----------------- -------------------- 206.5/457.1 MB 7.1 MB/s eta 0:00:36\n",
      "     ----------------- -------------------- 206.8/457.1 MB 6.9 MB/s eta 0:00:37\n",
      "     ----------------- -------------------- 207.0/457.1 MB 6.7 MB/s eta 0:00:38\n",
      "     ----------------- -------------------- 207.1/457.1 MB 6.4 MB/s eta 0:00:39\n",
      "     ----------------- -------------------- 207.3/457.1 MB 6.1 MB/s eta 0:00:41\n",
      "     ----------------- -------------------- 207.5/457.1 MB 5.9 MB/s eta 0:00:43\n",
      "     ----------------- -------------------- 207.8/457.1 MB 5.8 MB/s eta 0:00:44\n",
      "     ----------------- -------------------- 208.1/457.1 MB 5.8 MB/s eta 0:00:43\n",
      "     ----------------- -------------------- 208.4/457.1 MB 5.8 MB/s eta 0:00:43\n",
      "     ----------------- -------------------- 208.6/457.1 MB 5.7 MB/s eta 0:00:44\n",
      "     ----------------- -------------------- 208.9/457.1 MB 5.5 MB/s eta 0:00:46\n",
      "     ----------------- -------------------- 209.4/457.1 MB 5.3 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 209.7/457.1 MB 5.1 MB/s eta 0:00:49\n",
      "     ----------------- -------------------- 209.8/457.1 MB 5.0 MB/s eta 0:00:50\n",
      "     ----------------- -------------------- 210.4/457.1 MB 5.0 MB/s eta 0:00:49\n",
      "     ----------------- -------------------- 210.7/457.1 MB 4.9 MB/s eta 0:00:51\n",
      "     ----------------- -------------------- 211.1/457.1 MB 5.0 MB/s eta 0:00:50\n",
      "     ----------------- -------------------- 211.6/457.1 MB 4.9 MB/s eta 0:00:50\n",
      "     ----------------- -------------------- 211.9/457.1 MB 5.6 MB/s eta 0:00:44\n",
      "     ----------------- -------------------- 212.3/457.1 MB 5.5 MB/s eta 0:00:45\n",
      "     ----------------- -------------------- 212.6/457.1 MB 5.4 MB/s eta 0:00:46\n",
      "     ----------------- -------------------- 213.0/457.1 MB 5.3 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 213.3/457.1 MB 5.3 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 213.8/457.1 MB 5.2 MB/s eta 0:00:48\n",
      "     ----------------- -------------------- 214.4/457.1 MB 5.1 MB/s eta 0:00:48\n",
      "     ----------------- -------------------- 214.8/457.1 MB 5.2 MB/s eta 0:00:48\n",
      "     ----------------- -------------------- 215.3/457.1 MB 5.2 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 215.8/457.1 MB 5.2 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 216.2/457.1 MB 5.1 MB/s eta 0:00:48\n",
      "     ------------------ ------------------- 216.8/457.1 MB 5.2 MB/s eta 0:00:46\n",
      "     ------------------ ------------------- 217.1/457.1 MB 5.1 MB/s eta 0:00:47\n",
      "     ------------------ ------------------- 217.5/457.1 MB 5.6 MB/s eta 0:00:44\n",
      "     ------------------ ------------------- 218.1/457.1 MB 5.7 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 218.4/457.1 MB 5.7 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 218.9/457.1 MB 5.8 MB/s eta 0:00:41\n",
      "     ------------------ ------------------- 219.3/457.1 MB 5.9 MB/s eta 0:00:41\n",
      "     ------------------ ------------------- 219.7/457.1 MB 6.0 MB/s eta 0:00:40\n",
      "     ------------------ ------------------- 220.3/457.1 MB 6.1 MB/s eta 0:00:40\n",
      "     ------------------ ------------------- 221.2/457.1 MB 6.2 MB/s eta 0:00:38\n",
      "     ------------------ ------------------- 221.6/457.1 MB 6.2 MB/s eta 0:00:38\n",
      "     ------------------ ------------------- 222.0/457.1 MB 6.2 MB/s eta 0:00:38\n",
      "     ------------------ ------------------- 222.4/457.1 MB 6.3 MB/s eta 0:00:38\n",
      "     ------------------ ------------------- 223.0/457.1 MB 6.5 MB/s eta 0:00:37\n",
      "     ------------------ ------------------- 223.4/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 223.9/457.1 MB 6.6 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 224.3/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 225.0/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 225.4/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 225.7/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 226.3/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 226.6/457.1 MB 6.5 MB/s eta 0:00:36\n",
      "     ------------------ ------------------- 226.9/457.1 MB 6.3 MB/s eta 0:00:37\n",
      "     ------------------ ------------------- 227.1/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------ ------------------- 227.6/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------ ------------------- 228.0/457.1 MB 6.3 MB/s eta 0:00:37\n",
      "     ------------------ ------------------- 228.4/457.1 MB 6.2 MB/s eta 0:00:38\n",
      "     ------------------- ------------------ 228.9/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 229.3/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 229.6/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 230.0/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 230.3/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 230.8/457.1 MB 6.1 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 231.0/457.1 MB 5.9 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 231.4/457.1 MB 5.9 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 231.9/457.1 MB 5.8 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 232.5/457.1 MB 5.9 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 232.9/457.1 MB 6.0 MB/s eta 0:00:38\n",
      "     ------------------- ------------------ 233.3/457.1 MB 5.8 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 234.4/457.1 MB 6.2 MB/s eta 0:00:37\n",
      "     ------------------- ------------------ 234.9/457.1 MB 6.2 MB/s eta 0:00:36\n",
      "     ------------------- ------------------ 235.7/457.1 MB 6.4 MB/s eta 0:00:35\n",
      "     ------------------- ------------------ 236.1/457.1 MB 6.5 MB/s eta 0:00:35\n",
      "     ------------------- ------------------ 236.7/457.1 MB 6.5 MB/s eta 0:00:34\n",
      "     ------------------- ------------------ 237.0/457.1 MB 6.6 MB/s eta 0:00:34\n",
      "     ------------------- ------------------ 237.4/457.1 MB 6.8 MB/s eta 0:00:33\n",
      "     ------------------- ------------------ 237.9/457.1 MB 6.8 MB/s eta 0:00:33\n",
      "     ------------------- ------------------ 238.0/457.1 MB 6.6 MB/s eta 0:00:34\n",
      "     ------------------- ------------------ 238.5/457.1 MB 6.5 MB/s eta 0:00:34\n",
      "     ------------------- ------------------ 239.2/457.1 MB 6.7 MB/s eta 0:00:33\n",
      "     ------------------- ------------------ 239.5/457.1 MB 6.7 MB/s eta 0:00:33\n",
      "     ------------------- ------------------ 240.1/457.1 MB 6.9 MB/s eta 0:00:32\n",
      "     ------------------- ------------------ 240.5/457.1 MB 6.9 MB/s eta 0:00:32\n",
      "     -------------------- ----------------- 241.0/457.1 MB 7.1 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 241.4/457.1 MB 7.1 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 241.9/457.1 MB 7.1 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 242.6/457.1 MB 7.1 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 243.1/457.1 MB 7.1 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 243.7/457.1 MB 7.4 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 244.3/457.1 MB 7.1 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 244.8/457.1 MB 7.1 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 245.4/457.1 MB 7.2 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 245.8/457.1 MB 7.0 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 247.0/457.1 MB 7.4 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 247.4/457.1 MB 7.4 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 247.4/457.1 MB 7.4 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 247.4/457.1 MB 7.0 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 247.8/457.1 MB 6.9 MB/s eta 0:00:31\n",
      "     -------------------- ----------------- 248.5/457.1 MB 7.2 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 248.9/457.1 MB 7.2 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 249.3/457.1 MB 7.0 MB/s eta 0:00:30\n",
      "     -------------------- ----------------- 250.1/457.1 MB 7.2 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 250.5/457.1 MB 7.2 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 251.2/457.1 MB 7.1 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 251.7/457.1 MB 7.3 MB/s eta 0:00:29\n",
      "     -------------------- ----------------- 252.1/457.1 MB 7.4 MB/s eta 0:00:28\n",
      "     --------------------- ---------------- 253.4/457.1 MB 8.2 MB/s eta 0:00:25\n",
      "     --------------------- ---------------- 254.3/457.1 MB 8.4 MB/s eta 0:00:25\n",
      "     --------------------- ---------------- 255.0/457.1 MB 8.7 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 255.6/457.1 MB 8.6 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 256.1/457.1 MB 9.1 MB/s eta 0:00:23\n",
      "     --------------------- ---------------- 256.6/457.1 MB 8.6 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 257.2/457.1 MB 8.5 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 257.7/457.1 MB 9.5 MB/s eta 0:00:22\n",
      "     --------------------- ---------------- 258.1/457.1 MB 9.6 MB/s eta 0:00:21\n",
      "     --------------------- ---------------- 258.6/457.1 MB 9.5 MB/s eta 0:00:21\n",
      "     --------------------- ---------------- 259.1/457.1 MB 9.4 MB/s eta 0:00:22\n",
      "     --------------------- ---------------- 259.6/457.1 MB 9.8 MB/s eta 0:00:21\n",
      "     --------------------- --------------- 260.8/457.1 MB 10.7 MB/s eta 0:00:19\n",
      "     --------------------- --------------- 261.6/457.1 MB 11.3 MB/s eta 0:00:18\n",
      "     --------------------- --------------- 262.1/457.1 MB 11.3 MB/s eta 0:00:18\n",
      "     --------------------- --------------- 262.7/457.1 MB 10.9 MB/s eta 0:00:18\n",
      "     --------------------- --------------- 263.1/457.1 MB 10.2 MB/s eta 0:00:19\n",
      "     --------------------- ---------------- 263.9/457.1 MB 9.9 MB/s eta 0:00:20\n",
      "     --------------------- ---------------- 264.4/457.1 MB 9.4 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 265.1/457.1 MB 9.0 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 266.3/457.1 MB 9.2 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 266.7/457.1 MB 9.5 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 267.2/457.1 MB 9.4 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 267.9/457.1 MB 9.4 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 268.4/457.1 MB 9.5 MB/s eta 0:00:20\n",
      "     ---------------------- --------------- 269.0/457.1 MB 9.8 MB/s eta 0:00:20\n",
      "     ---------------------- --------------- 269.5/457.1 MB 9.8 MB/s eta 0:00:20\n",
      "     ---------------------- --------------- 270.0/457.1 MB 9.6 MB/s eta 0:00:20\n",
      "     ---------------------- --------------- 270.8/457.1 MB 9.2 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 271.5/457.1 MB 8.8 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 272.1/457.1 MB 8.7 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 272.8/457.1 MB 8.8 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 273.4/457.1 MB 9.1 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 274.0/457.1 MB 8.8 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 274.6/457.1 MB 8.7 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 275.3/457.1 MB 9.0 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 275.7/457.1 MB 8.7 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 276.1/457.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 276.6/457.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 277.1/457.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 277.7/457.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 278.4/457.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 278.9/457.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 279.4/457.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 280.3/457.1 MB 8.6 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 280.6/457.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 281.2/457.1 MB 8.2 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 281.9/457.1 MB 8.2 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 282.4/457.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 283.1/457.1 MB 8.2 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 283.6/457.1 MB 8.0 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 284.1/457.1 MB 7.9 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 284.6/457.1 MB 8.0 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 285.2/457.1 MB 8.0 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 285.7/457.1 MB 7.8 MB/s eta 0:00:23\n",
      "     ----------------------- -------------- 286.2/457.1 MB 7.9 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 286.8/457.1 MB 7.8 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 288.3/457.1 MB 8.7 MB/s eta 0:00:20\n",
      "     ----------------------- -------------- 288.6/457.1 MB 8.6 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 289.2/457.1 MB 8.8 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 289.8/457.1 MB 8.7 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 290.4/457.1 MB 8.5 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 291.1/457.1 MB 8.8 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 291.5/457.1 MB 8.8 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 292.2/457.1 MB 8.7 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 292.9/457.1 MB 8.8 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 293.4/457.1 MB 8.8 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 294.0/457.1 MB 9.1 MB/s eta 0:00:18\n",
      "     ------------------------ ------------- 294.6/457.1 MB 9.0 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 295.6/457.1 MB 9.2 MB/s eta 0:00:18\n",
      "     ------------------------ ------------ 298.1/457.1 MB 10.9 MB/s eta 0:00:15\n",
      "     ------------------------ ------------ 299.1/457.1 MB 11.9 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 299.7/457.1 MB 11.9 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 300.3/457.1 MB 11.9 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 300.7/457.1 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 301.1/457.1 MB 11.1 MB/s eta 0:00:15\n",
      "     ------------------------ ------------ 301.8/457.1 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 302.5/457.1 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 303.1/457.1 MB 11.3 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 303.9/457.1 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 304.4/457.1 MB 11.9 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 305.2/457.1 MB 12.6 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 306.4/457.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 307.3/457.1 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 307.9/457.1 MB 11.3 MB/s eta 0:00:14\n",
      "     ------------------------ ------------ 308.5/457.1 MB 10.7 MB/s eta 0:00:14\n",
      "     ------------------------- ------------ 308.9/457.1 MB 9.9 MB/s eta 0:00:15\n",
      "     ------------------------- ------------ 309.5/457.1 MB 9.9 MB/s eta 0:00:15\n",
      "     ------------------------- ----------- 311.0/457.1 MB 11.1 MB/s eta 0:00:14\n",
      "     ------------------------- ----------- 311.3/457.1 MB 11.5 MB/s eta 0:00:13\n",
      "     ------------------------- ----------- 311.5/457.1 MB 10.7 MB/s eta 0:00:14\n",
      "     ------------------------- ----------- 311.9/457.1 MB 10.2 MB/s eta 0:00:15\n",
      "     ------------------------- ----------- 312.5/457.1 MB 10.2 MB/s eta 0:00:15\n",
      "     ------------------------- ----------- 313.1/457.1 MB 10.2 MB/s eta 0:00:15\n",
      "     ------------------------- ----------- 313.7/457.1 MB 10.1 MB/s eta 0:00:15\n",
      "     ------------------------- ----------- 314.3/457.1 MB 10.2 MB/s eta 0:00:14\n",
      "     ------------------------- ----------- 314.9/457.1 MB 10.2 MB/s eta 0:00:14\n",
      "     -------------------------- ----------- 315.6/457.1 MB 9.8 MB/s eta 0:00:15\n",
      "     -------------------------- ----------- 316.1/457.1 MB 9.4 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 316.6/457.1 MB 8.8 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 317.1/457.1 MB 8.5 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 318.0/457.1 MB 8.4 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 318.7/457.1 MB 8.6 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 319.3/457.1 MB 8.8 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 319.7/457.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 320.4/457.1 MB 8.5 MB/s eta 0:00:17\n",
      "     -------------------------- ---------- 322.6/457.1 MB 10.6 MB/s eta 0:00:13\n",
      "     -------------------------- ---------- 323.0/457.1 MB 10.7 MB/s eta 0:00:13\n",
      "     -------------------------- ---------- 323.7/457.1 MB 10.6 MB/s eta 0:00:13\n",
      "     -------------------------- ---------- 324.3/457.1 MB 10.6 MB/s eta 0:00:13\n",
      "     -------------------------- ---------- 324.8/457.1 MB 10.4 MB/s eta 0:00:13\n",
      "     -------------------------- ---------- 325.2/457.1 MB 10.2 MB/s eta 0:00:13\n",
      "     --------------------------- ---------- 325.5/457.1 MB 9.6 MB/s eta 0:00:14\n",
      "     --------------------------- ---------- 325.9/457.1 MB 9.8 MB/s eta 0:00:14\n",
      "     --------------------------- ---------- 326.0/457.1 MB 9.2 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 326.2/457.1 MB 8.8 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 326.3/457.1 MB 8.5 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 326.3/457.1 MB 8.5 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 326.4/457.1 MB 7.7 MB/s eta 0:00:17\n",
      "     --------------------------- ---------- 327.0/457.1 MB 7.9 MB/s eta 0:00:17\n",
      "     --------------------------- ---------- 327.2/457.1 MB 7.4 MB/s eta 0:00:18\n",
      "     --------------------------- ---------- 327.5/457.1 MB 7.4 MB/s eta 0:00:18\n",
      "     --------------------------- ---------- 327.9/457.1 MB 7.1 MB/s eta 0:00:19\n",
      "     --------------------------- ---------- 328.2/457.1 MB 7.0 MB/s eta 0:00:19\n",
      "     --------------------------- ---------- 328.5/457.1 MB 6.9 MB/s eta 0:00:19\n",
      "     --------------------------- ---------- 329.1/457.1 MB 6.8 MB/s eta 0:00:19\n",
      "     --------------------------- ---------- 329.3/457.1 MB 6.7 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 329.7/457.1 MB 6.6 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 330.2/457.1 MB 6.5 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 330.8/457.1 MB 6.4 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 331.0/457.1 MB 6.2 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 331.4/457.1 MB 6.0 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 331.6/457.1 MB 5.7 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 332.3/457.1 MB 5.5 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 333.1/457.1 MB 5.4 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 333.9/457.1 MB 5.5 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 334.3/457.1 MB 5.5 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 334.5/457.1 MB 5.3 MB/s eta 0:00:24\n",
      "     --------------------------- ---------- 334.9/457.1 MB 5.2 MB/s eta 0:00:24\n",
      "     --------------------------- ---------- 335.2/457.1 MB 5.3 MB/s eta 0:00:24\n",
      "     --------------------------- ---------- 335.6/457.1 MB 5.3 MB/s eta 0:00:24\n",
      "     --------------------------- ---------- 336.1/457.1 MB 5.3 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 336.7/457.1 MB 6.3 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 337.4/457.1 MB 6.3 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 337.8/457.1 MB 6.6 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 338.6/457.1 MB 6.8 MB/s eta 0:00:18\n",
      "     ---------------------------- --------- 339.0/457.1 MB 7.0 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 339.7/457.1 MB 7.1 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 340.3/457.1 MB 7.3 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 340.6/457.1 MB 7.2 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 342.8/457.1 MB 9.0 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 343.3/457.1 MB 8.7 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 343.7/457.1 MB 8.5 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 344.4/457.1 MB 8.5 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 345.2/457.1 MB 9.4 MB/s eta 0:00:12\n",
      "     ---------------------------- --------- 345.8/457.1 MB 9.8 MB/s eta 0:00:12\n",
      "     ---------------------------- --------- 346.4/457.1 MB 9.9 MB/s eta 0:00:12\n",
      "     ---------------------------- --------- 347.0/457.1 MB 9.9 MB/s eta 0:00:12\n",
      "     ---------------------------- --------- 347.6/457.1 MB 9.6 MB/s eta 0:00:12\n",
      "     ---------------------------- -------- 348.4/457.1 MB 10.1 MB/s eta 0:00:11\n",
      "     ---------------------------- -------- 349.0/457.1 MB 10.1 MB/s eta 0:00:11\n",
      "     ---------------------------- -------- 349.5/457.1 MB 10.1 MB/s eta 0:00:11\n",
      "     ---------------------------- -------- 350.5/457.1 MB 10.7 MB/s eta 0:00:10\n",
      "     ---------------------------- -------- 350.8/457.1 MB 10.7 MB/s eta 0:00:10\n",
      "     ----------------------------- -------- 351.4/457.1 MB 9.9 MB/s eta 0:00:11\n",
      "     ----------------------------- -------- 352.1/457.1 MB 9.5 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 352.5/457.1 MB 8.8 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 353.1/457.1 MB 8.8 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 353.5/457.1 MB 8.7 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 353.7/457.1 MB 8.3 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 354.1/457.1 MB 8.3 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 354.5/457.1 MB 8.0 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 354.8/457.1 MB 7.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 355.0/457.1 MB 7.5 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 355.4/457.1 MB 7.3 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 355.8/457.1 MB 7.1 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 356.4/457.1 MB 7.2 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 357.1/457.1 MB 7.2 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 357.8/457.1 MB 7.0 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 359.0/457.1 MB 7.3 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 359.6/457.1 MB 7.3 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 360.0/457.1 MB 7.2 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 360.3/457.1 MB 7.0 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 360.3/457.1 MB 7.0 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 360.3/457.1 MB 7.0 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 360.9/457.1 MB 6.4 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 361.3/457.1 MB 6.4 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 361.5/457.1 MB 6.2 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 361.8/457.1 MB 6.0 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 362.3/457.1 MB 5.9 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 362.4/457.1 MB 5.9 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 362.6/457.1 MB 5.8 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 362.7/457.1 MB 5.6 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 362.9/457.1 MB 5.5 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 363.1/457.1 MB 5.4 MB/s eta 0:00:18\n",
      "     ------------------------------ ------- 363.1/457.1 MB 5.2 MB/s eta 0:00:18\n",
      "     ------------------------------ ------- 363.6/457.1 MB 5.2 MB/s eta 0:00:19\n",
      "     ------------------------------ ------- 364.3/457.1 MB 5.5 MB/s eta 0:00:18\n",
      "     ------------------------------ ------- 364.8/457.1 MB 5.5 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 365.0/457.1 MB 5.4 MB/s eta 0:00:18\n",
      "     ------------------------------ ------- 365.6/457.1 MB 5.6 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 366.2/457.1 MB 5.7 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 366.5/457.1 MB 5.5 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 367.0/457.1 MB 5.5 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 367.4/457.1 MB 5.6 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 368.1/457.1 MB 5.6 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 368.7/457.1 MB 5.5 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 369.1/457.1 MB 5.3 MB/s eta 0:00:17\n",
      "     ------------------------------ ------- 370.8/457.1 MB 6.4 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 371.3/457.1 MB 6.2 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 372.6/457.1 MB 7.2 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 373.3/457.1 MB 8.5 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 373.9/457.1 MB 9.0 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 374.4/457.1 MB 9.0 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 375.2/457.1 MB 9.6 MB/s eta 0:00:09\n",
      "     ------------------------------ ------ 375.9/457.1 MB 10.2 MB/s eta 0:00:08\n",
      "     ------------------------------- ------ 376.1/457.1 MB 9.6 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 376.2/457.1 MB 9.2 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 376.7/457.1 MB 9.0 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 377.0/457.1 MB 8.8 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 377.6/457.1 MB 8.7 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 378.4/457.1 MB 9.1 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 378.5/457.1 MB 8.8 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 378.7/457.1 MB 8.3 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 378.9/457.1 MB 8.0 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 379.1/457.1 MB 7.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 379.2/457.1 MB 7.5 MB/s eta 0:00:11\n",
      "     ------------------------------- ------ 379.4/457.1 MB 7.2 MB/s eta 0:00:11\n",
      "     ------------------------------- ------ 379.5/457.1 MB 7.0 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 379.9/457.1 MB 6.7 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 380.2/457.1 MB 6.4 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 380.7/457.1 MB 6.2 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 381.1/457.1 MB 6.2 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 381.2/457.1 MB 6.0 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 381.5/457.1 MB 5.8 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 381.6/457.1 MB 5.8 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 381.8/457.1 MB 5.6 MB/s eta 0:00:14\n",
      "     ------------------------------- ------ 382.0/457.1 MB 5.4 MB/s eta 0:00:14\n",
      "     ------------------------------- ------ 382.5/457.1 MB 5.2 MB/s eta 0:00:15\n",
      "     ------------------------------- ------ 383.0/457.1 MB 5.1 MB/s eta 0:00:15\n",
      "     ------------------------------- ------ 383.0/457.1 MB 5.0 MB/s eta 0:00:15\n",
      "     ------------------------------- ------ 383.4/457.1 MB 4.9 MB/s eta 0:00:16\n",
      "     ------------------------------- ------ 383.6/457.1 MB 4.9 MB/s eta 0:00:16\n",
      "     ------------------------------- ------ 384.1/457.1 MB 4.8 MB/s eta 0:00:16\n",
      "     ------------------------------- ------ 384.6/457.1 MB 4.7 MB/s eta 0:00:16\n",
      "     -------------------------------- ----- 385.1/457.1 MB 4.7 MB/s eta 0:00:16\n",
      "     -------------------------------- ----- 385.8/457.1 MB 4.7 MB/s eta 0:00:16\n",
      "     -------------------------------- ----- 386.3/457.1 MB 4.7 MB/s eta 0:00:16\n",
      "     -------------------------------- ----- 387.0/457.1 MB 5.0 MB/s eta 0:00:15\n",
      "     -------------------------------- ----- 387.6/457.1 MB 5.0 MB/s eta 0:00:14\n",
      "     -------------------------------- ----- 388.2/457.1 MB 5.0 MB/s eta 0:00:14\n",
      "     -------------------------------- ----- 388.7/457.1 MB 5.3 MB/s eta 0:00:13\n",
      "     -------------------------------- ----- 389.6/457.1 MB 6.0 MB/s eta 0:00:12\n",
      "     -------------------------------- ----- 390.2/457.1 MB 6.3 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 390.7/457.1 MB 6.4 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 391.3/457.1 MB 6.4 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 392.0/457.1 MB 7.3 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 392.3/457.1 MB 7.4 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 392.3/457.1 MB 7.3 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 392.5/457.1 MB 6.9 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 392.6/457.1 MB 6.7 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 392.7/457.1 MB 6.5 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 392.8/457.1 MB 6.4 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.0/457.1 MB 6.3 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.2/457.1 MB 6.2 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.3/457.1 MB 6.2 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.5/457.1 MB 6.0 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.7/457.1 MB 6.0 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 393.9/457.1 MB 6.0 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.2/457.1 MB 5.8 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.5/457.1 MB 5.8 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 5.7 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 394.8/457.1 MB 4.9 MB/s eta 0:00:13\n",
      "     -------------------------------- ----- 394.8/457.1 MB 4.9 MB/s eta 0:00:13\n",
      "     -------------------------------- ----- 395.0/457.1 MB 4.5 MB/s eta 0:00:14\n",
      "     -------------------------------- ----- 395.1/457.1 MB 4.5 MB/s eta 0:00:14\n",
      "     -------------------------------- ----- 395.3/457.1 MB 4.4 MB/s eta 0:00:15\n",
      "     -------------------------------- ----- 395.8/457.1 MB 4.4 MB/s eta 0:00:15\n",
      "     -------------------------------- ----- 396.3/457.1 MB 4.4 MB/s eta 0:00:14\n",
      "     -------------------------------- ----- 396.5/457.1 MB 4.3 MB/s eta 0:00:15\n",
      "     --------------------------------- ---- 397.2/457.1 MB 4.3 MB/s eta 0:00:15\n",
      "     --------------------------------- ---- 397.7/457.1 MB 4.3 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 398.3/457.1 MB 4.3 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 398.7/457.1 MB 4.2 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 399.2/457.1 MB 4.2 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 399.8/457.1 MB 4.1 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 400.3/457.1 MB 4.1 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 400.9/457.1 MB 4.1 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 401.5/457.1 MB 4.2 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 402.0/457.1 MB 4.1 MB/s eta 0:00:14\n",
      "     --------------------------------- ---- 402.5/457.1 MB 4.3 MB/s eta 0:00:13\n",
      "     --------------------------------- ---- 403.0/457.1 MB 4.5 MB/s eta 0:00:13\n",
      "     --------------------------------- ---- 403.5/457.1 MB 4.8 MB/s eta 0:00:12\n",
      "     --------------------------------- ---- 404.1/457.1 MB 5.1 MB/s eta 0:00:11\n",
      "     --------------------------------- ---- 404.8/457.1 MB 5.4 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 405.4/457.1 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 405.9/457.1 MB 7.5 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 406.5/457.1 MB 7.6 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 407.0/457.1 MB 7.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 408.4/457.1 MB 8.4 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 409.5/457.1 MB 9.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 410.2/457.1 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 410.7/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 411.1/457.1 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 411.6/457.1 MB 9.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 412.2/457.1 MB 9.4 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 412.8/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 413.3/457.1 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 413.9/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 414.4/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 415.0/457.1 MB 9.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 415.7/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 416.3/457.1 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 416.8/457.1 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 418.0/457.1 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------------------------------- -- 420.8/457.1 MB 11.1 MB/s eta 0:00:04\n",
      "     ---------------------------------- -- 422.1/457.1 MB 13.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 422.6/457.1 MB 12.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 423.3/457.1 MB 12.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 423.9/457.1 MB 12.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 424.4/457.1 MB 12.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 425.0/457.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 425.5/457.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 426.1/457.1 MB 12.1 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 426.6/457.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 427.3/457.1 MB 12.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 427.6/457.1 MB 11.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 428.3/457.1 MB 11.3 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 428.8/457.1 MB 10.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 429.3/457.1 MB 10.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- -- 429.9/457.1 MB 9.5 MB/s eta 0:00:03\n",
      "     ----------------------------------- -- 430.3/457.1 MB 9.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- -- 430.9/457.1 MB 8.6 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 431.4/457.1 MB 8.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 431.9/457.1 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 432.7/457.1 MB 8.0 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 433.2/457.1 MB 8.0 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 433.8/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 434.4/457.1 MB 8.1 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 434.9/457.1 MB 8.1 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 435.4/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 436.1/457.1 MB 8.3 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 436.7/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 437.4/457.1 MB 8.3 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 437.8/457.1 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 438.2/457.1 MB 8.3 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 438.7/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 439.3/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 439.7/457.1 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 440.0/457.1 MB 7.7 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 440.7/457.1 MB 8.0 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 441.9/457.1 MB 8.5 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 443.1/457.1 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 443.6/457.1 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 444.1/457.1 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 444.8/457.1 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------------------------------  445.4/457.1 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------------------------------  445.9/457.1 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------------------------------  446.4/457.1 MB 9.1 MB/s eta 0:00:02\n",
      "     -------------------------------------  446.8/457.1 MB 8.7 MB/s eta 0:00:02\n",
      "     -------------------------------------  447.4/457.1 MB 8.6 MB/s eta 0:00:02\n",
      "     -------------------------------------  447.8/457.1 MB 8.5 MB/s eta 0:00:02\n",
      "     -------------------------------------  448.5/457.1 MB 8.4 MB/s eta 0:00:02\n",
      "     -------------------------------------  449.8/457.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  450.8/457.1 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.3/457.1 MB 10.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  451.8/457.1 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  452.5/457.1 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  453.5/457.1 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  454.1/457.1 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  454.5/457.1 MB 10.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  454.9/457.1 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  455.4/457.1 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  456.3/457.1 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 457.1/457.1 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ca-core-news-trf==3.7.2) (3.7.0)\n",
      "Requirement already satisfied: spacy-curated-transformers<0.3.0,>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ca-core-news-trf==3.7.2) (0.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.26.4)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2024.5.15)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pathy>=0.10.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.1.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.12.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_trf')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!py -m spacy download ca_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_trf = spacy.load('ca_core_news_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragged(data=array([[ 2.09487468e-01, -6.82299376e-01,  6.87269866e-01,\n",
      "        -2.15709984e-01, -1.83599502e-01,  1.96781412e-01,\n",
      "        -3.10865909e-01,  5.01355380e-02,  2.09747404e-01,\n",
      "         4.76011753e-01, -1.53212249e-01,  5.28716087e-01,\n",
      "         3.63347411e-01, -3.85383755e-01,  1.04330353e-01,\n",
      "         3.42985511e-01,  7.03844368e-01,  5.85401282e-02,\n",
      "        -7.82101750e-02, -1.23748779e-02,  9.58655953e-01,\n",
      "        -2.27109060e-01, -3.11816663e-01,  3.88369352e-01,\n",
      "        -3.64216655e-01, -5.47646403e-01, -9.23638940e-01,\n",
      "        -6.52197957e-01,  1.06132418e-01, -8.77194107e-02,\n",
      "        -1.60520092e-01, -6.95155263e-01,  1.77061737e-01,\n",
      "         1.20308086e-01, -2.87298769e-01, -2.34697625e-01,\n",
      "        -7.32626542e-02, -1.40464321e-01, -2.27952570e-01,\n",
      "        -5.01841426e-01,  2.91805565e-02,  2.90764600e-01,\n",
      "        -2.10933387e-01,  7.32212216e-02, -8.17313492e-01,\n",
      "         6.53707981e-03, -1.85016036e-01,  1.21130973e-01,\n",
      "        -4.40823376e-01,  4.19431001e-01, -7.30811357e-02,\n",
      "         5.79906702e-01,  8.26137006e-01, -7.27764070e-02,\n",
      "         5.82239747e-01, -5.32361031e-01,  4.64751631e-01,\n",
      "        -2.91401118e-01, -7.63586044e-01,  2.95286309e-02,\n",
      "         4.00611013e-01, -4.25578892e-01,  3.23285341e-01,\n",
      "        -1.53327778e-01, -4.92756993e-01, -9.11339894e-02,\n",
      "        -2.45587498e-01, -1.02923691e-01, -3.14292878e-01,\n",
      "         3.70332569e-01,  2.35338286e-01, -6.30050540e-01,\n",
      "        -5.61260700e-01,  5.28796077e-01, -5.47306955e-01,\n",
      "         1.54354841e-01, -1.25185996e-01,  3.72741342e-01,\n",
      "         2.52510328e-02, -4.67950143e-02, -8.12917724e-02,\n",
      "        -7.43832961e-02, -5.12393236e-01,  6.34258032e-01,\n",
      "         1.90891862e-01, -1.51945367e-01,  6.16273731e-02,\n",
      "         1.71282500e-01,  2.03443944e-01, -2.29667455e-01,\n",
      "        -3.64261925e-01,  1.93293557e-01,  3.94562334e-01,\n",
      "         1.03565052e-01, -2.14130551e-01, -2.23342568e-01,\n",
      "        -9.94791090e-02, -5.67353785e-01,  3.95315975e-01,\n",
      "        -1.81279808e-01, -2.61496902e-02, -3.24480504e-01,\n",
      "         1.93072021e-01, -9.75198746e-02, -1.27615005e-01,\n",
      "         2.74525136e-01, -1.36055827e-01, -3.81070286e-01,\n",
      "         5.46114296e-02,  2.62023479e-01,  5.33228874e-01,\n",
      "        -1.76582217e-01, -6.02354646e-01, -4.58980501e-01,\n",
      "         9.77269471e-01, -6.99894875e-03,  3.99266660e-01,\n",
      "        -2.97489434e-01,  1.56173572e-01,  1.38535655e+00,\n",
      "        -5.84451139e-01,  5.83676636e-01,  3.81599784e-01,\n",
      "         2.19879955e-01, -2.13967204e-01, -6.46456718e-01,\n",
      "         4.07025993e-01, -1.06326008e+00,  1.14479482e-01,\n",
      "         4.92874861e-01, -3.36023897e-01,  4.45714176e-01,\n",
      "        -2.23300844e-01,  5.26056886e-01, -6.93685561e-03,\n",
      "         5.95242798e-01,  3.98020655e-01,  8.33989903e-02,\n",
      "        -2.02305660e-01, -4.68591392e-01,  2.26132214e-01,\n",
      "         3.73161793e-01, -2.17083871e-01, -5.89221299e-01,\n",
      "         2.56416053e-01,  8.25268477e-02, -1.79138780e-01,\n",
      "         1.45890154e-02, -5.48659451e-02, -1.19344912e-01,\n",
      "        -1.14401616e-01,  1.19834021e-01,  8.69812012e-01,\n",
      "        -2.73590475e-01, -3.66530269e-01, -1.27914637e-01,\n",
      "        -2.84156144e-01, -3.59697491e-01,  4.44789410e-01,\n",
      "        -1.84229314e-01, -6.04312122e-01, -2.38331482e-02,\n",
      "        -3.82100701e-01, -3.17528963e-01,  2.25972503e-01,\n",
      "         7.83279300e-01, -3.65064830e-01,  2.90531963e-02,\n",
      "        -1.40380651e-01, -2.96226412e-01,  3.44292760e-01,\n",
      "        -3.83913547e-01,  2.18550667e-01, -7.56567717e-01,\n",
      "        -9.59508061e-01, -3.04560632e-01,  4.23880160e-01,\n",
      "         6.71459138e-01,  4.71582323e-01, -9.42460820e-02,\n",
      "        -9.66360122e-02,  2.81110793e-01,  7.02851862e-02,\n",
      "        -2.91329533e-01, -5.54498434e-01, -3.65364440e-02,\n",
      "        -1.26872644e-01,  5.04799068e-01, -4.59949940e-01,\n",
      "         6.27089918e-01, -1.48024380e-01, -2.27763683e-01,\n",
      "         1.10705793e-02, -5.63208222e-01,  1.96249872e-01,\n",
      "         3.48663539e-01,  2.90507078e-03,  3.55010152e-01,\n",
      "        -6.56491280e-01,  1.22819869e-02, -2.43288070e-01,\n",
      "        -4.41904932e-01,  1.44435182e-01,  1.79221928e-02,\n",
      "         6.36772692e-01, -5.01818806e-02, -5.46861708e-01,\n",
      "        -1.89968660e-01,  5.54323554e-01,  2.28152528e-01,\n",
      "         5.28751165e-02,  5.36742806e-03, -8.71801525e-02,\n",
      "        -2.82331258e-01,  4.70154673e-01, -1.35132521e-02,\n",
      "        -5.98311484e-01, -2.35815674e-01,  3.86045367e-01,\n",
      "        -2.99472451e-01,  1.64305553e-01, -3.54488581e-01,\n",
      "        -9.42343697e-02, -2.39513963e-01,  5.50094619e-02,\n",
      "         5.04295230e-01, -9.53204781e-02, -1.76490843e-02,\n",
      "         7.36773014e-02, -2.51339197e-01, -3.12027991e-01,\n",
      "        -2.27009863e-01,  1.50912926e-02, -1.23586869e+00,\n",
      "        -2.21507579e-01,  6.96720541e-01, -2.91856945e-01,\n",
      "         3.47549856e-01,  5.38792722e-02, -9.77460518e-02,\n",
      "         2.62119263e-01,  6.86855316e-02,  6.07300460e-01,\n",
      "         3.89570408e-02, -1.39274403e-01,  8.91828910e-02,\n",
      "         2.82000117e-02, -7.22216889e-02, -2.63874888e-01,\n",
      "        -3.84219825e-01,  1.03077687e-01, -8.21645409e-02,\n",
      "        -9.84964669e-02, -3.66595536e-02,  5.98635435e-01,\n",
      "        -3.37360710e-01, -2.57005811e-01,  8.19469571e-01,\n",
      "         8.45236421e-01, -2.43473858e-01,  3.91640633e-01,\n",
      "        -6.51343584e-01, -4.87897396e-01,  6.95397973e-01,\n",
      "         1.32719487e-01,  3.12239528e-01,  4.30308580e-02,\n",
      "        -3.62066150e-01, -2.50601500e-01,  1.73057258e-01,\n",
      "        -7.74923623e-01, -4.46696430e-01, -4.97723162e-01,\n",
      "         8.00598189e-02,  3.18748802e-01, -2.96906650e-01,\n",
      "        -6.86958432e-01, -8.85026231e-02,  3.43324244e-02,\n",
      "         3.72887462e-01, -5.30757368e-01, -3.13436836e-01,\n",
      "        -8.57334509e-02,  1.52605668e-01,  2.58725226e-01,\n",
      "         7.69942999e-04,  2.32586965e-01, -2.75166303e-01,\n",
      "        -2.28733569e-03, -1.02854885e-01, -6.84864819e-03,\n",
      "        -3.03179026e-01, -1.53189570e-01, -2.58532345e-01,\n",
      "        -3.23679388e-01, -1.08254626e-02,  1.82126462e-03,\n",
      "        -1.08781666e-01,  1.90892905e-01, -6.76580012e-01,\n",
      "        -6.74382627e-01,  7.83530831e-01, -4.52710167e-02,\n",
      "        -3.61355543e-01, -4.88395989e-01, -5.29934525e-01,\n",
      "         5.27889282e-03,  1.62223071e-01, -1.05637871e-03,\n",
      "         6.31546378e-02,  3.06480139e-01, -2.03603864e-01,\n",
      "         3.57325435e-01, -4.61913943e-01,  2.88514271e-02,\n",
      "         1.12615466e-01, -3.64071801e-02,  1.16904721e-01,\n",
      "        -1.93521738e-01, -2.43833929e-01, -5.63381426e-03,\n",
      "         5.28669834e-01,  5.14489710e-02, -2.13263974e-01,\n",
      "        -2.21039340e-01, -1.45961046e-02,  3.90570700e-01,\n",
      "         7.08322227e-01, -7.05464602e-01, -1.17936380e-01,\n",
      "        -1.00773871e+00,  2.69654870e-01,  3.05166125e-01,\n",
      "        -2.05136254e-01,  3.13881069e-01, -8.41323286e-02,\n",
      "         6.67425394e-02, -3.24941099e-01, -2.86342725e-02,\n",
      "        -2.07622647e-02, -2.62835592e-01, -2.98379719e-01,\n",
      "         8.47798064e-02, -2.52503127e-01, -1.06255800e-01,\n",
      "         2.42153332e-02, -5.86905062e-01,  2.62410045e-01,\n",
      "        -9.01888758e-02,  4.12834525e-01,  2.61452287e-01,\n",
      "         1.56129748e-01,  6.03386402e-01, -2.67001987e-01,\n",
      "        -2.64894009e-01,  5.44545799e-03, -1.44157231e-01,\n",
      "         3.93611640e-01, -1.39773399e-01, -3.66793871e-02,\n",
      "        -5.90495169e-01,  2.99144506e-01,  4.16371673e-01,\n",
      "        -1.55411303e-01,  1.06105387e-01, -1.71855122e-01,\n",
      "        -2.18706399e-01,  2.40161359e-01,  4.30223227e-01,\n",
      "        -1.67301446e-02, -5.93012631e-01, -1.76631242e-01,\n",
      "        -2.37457514e-01,  5.14164627e-01, -4.05639678e-01,\n",
      "        -3.94870371e-01,  8.09213221e-01, -9.69906032e-01,\n",
      "         2.74809778e-01,  2.15203986e-02,  6.08537018e-01,\n",
      "        -9.52871218e-02, -2.80355334e-01, -1.13567457e-01,\n",
      "         3.52452576e-01, -2.68553853e-01, -5.66852614e-02,\n",
      "         1.95409223e-01,  4.52140182e-01, -1.11827292e-01,\n",
      "        -1.65458933e-01,  1.15369976e-01,  1.46134406e-01,\n",
      "         1.09288275e-01,  1.26276225e-01, -1.12657025e-01,\n",
      "        -2.08435282e-01,  1.44475460e-01,  2.33452886e-01,\n",
      "        -1.95006594e-01,  1.95928812e-01, -5.27749658e-02,\n",
      "         3.32138300e-01, -2.50707895e-01,  6.85539424e-01,\n",
      "         1.92219794e-01, -2.56051332e-01, -1.86712086e-01,\n",
      "         3.63238007e-02, -1.86473057e-01,  6.38095975e-01,\n",
      "         2.19698086e-01,  1.95998490e-01,  6.77371264e-01,\n",
      "        -5.45310378e-01,  2.81219542e-01,  4.32241887e-01,\n",
      "        -9.49485302e-01, -8.18026066e-01,  2.04557180e-01,\n",
      "        -8.64883065e-02,  8.40745121e-03,  1.16677709e-01,\n",
      "         6.38397038e-02,  2.24381566e-01, -3.00185740e-01,\n",
      "        -1.49338454e-01,  1.94383144e-01,  6.83516502e-01,\n",
      "         3.95720482e-01, -3.58345270e-01,  5.57435825e-02,\n",
      "        -4.99625623e-01, -1.96705952e-01,  5.05300939e-01,\n",
      "        -1.07279569e-02,  7.24011183e-01, -1.57435834e-01,\n",
      "        -5.35959750e-02,  5.04207425e-02, -3.39354694e-01,\n",
      "        -3.54740500e-01, -3.52565974e-01, -1.99494123e-01,\n",
      "        -2.47776017e-01, -5.39643504e-02, -2.43749887e-01,\n",
      "        -6.04296029e-01, -1.11152299e-01,  1.96872622e-01,\n",
      "        -4.84960675e-01,  1.72139689e-01,  3.49618733e-01,\n",
      "         6.29439577e-02,  7.58110881e-01, -3.47120553e-01,\n",
      "        -4.66846883e-01,  3.61889392e-01, -9.36126709e-03,\n",
      "        -4.33158040e-01,  5.85978553e-02, -4.37834591e-01,\n",
      "         1.33597270e-01, -4.02438939e-01,  1.62193790e-01,\n",
      "         4.64649647e-01, -5.54046750e-01, -2.14953136e+00,\n",
      "         3.81450951e-01,  1.15081906e-01, -1.20215274e-01,\n",
      "         2.77502865e-01,  3.06048483e-01,  4.71303016e-01,\n",
      "         9.72849309e-01, -1.63348913e-01,  7.22489059e-01,\n",
      "         9.35018212e-02, -4.56253409e-01,  1.52666733e-01,\n",
      "        -2.22203225e-01,  4.45741475e-01,  1.20444521e-01,\n",
      "         6.04338199e-02, -3.37242931e-01, -4.32698220e-01,\n",
      "        -5.50296426e-01,  6.10818386e-01, -1.78403184e-01,\n",
      "         7.52082616e-02, -1.87553689e-02,  1.98772162e-01,\n",
      "        -4.49556291e-01,  1.94866434e-01, -3.77566218e-01,\n",
      "         5.48780918e-01, -9.56190154e-02,  3.25295210e-01,\n",
      "        -5.38477004e-02, -9.76924181e-01,  2.00742513e-01,\n",
      "         3.17956060e-02, -4.82895732e-01, -4.05056715e-01,\n",
      "        -1.52333334e-01,  3.42659593e-01, -2.71450937e-01,\n",
      "         2.47155458e-01,  3.63477528e-01,  6.15350842e-01,\n",
      "         4.04938042e-01,  4.85381305e-01, -3.56769450e-02,\n",
      "        -2.32702643e-01,  1.78081110e-01, -4.49506819e-01,\n",
      "         4.49382752e-01, -3.51597816e-02,  4.33151543e-01,\n",
      "        -1.76672526e-02, -1.62783384e-01,  1.52489409e-01,\n",
      "         1.93609267e-01, -4.53282744e-01,  6.38629869e-02,\n",
      "        -3.51600826e-01,  3.01471561e-01,  4.33206677e-01,\n",
      "         3.80835772e-01, -7.61207044e-01, -8.69877696e-01,\n",
      "         7.84307480e-01,  8.91940147e-02, -6.54645741e-01,\n",
      "         6.58978701e-01,  4.37705368e-01, -5.38919687e-01,\n",
      "        -6.49923757e-02, -1.93074793e-01, -1.80913225e-01,\n",
      "         1.75062731e-01, -4.22377825e-01, -3.08460504e-01,\n",
      "         3.38005990e-01, -6.75392151e-01,  6.09195292e-01,\n",
      "        -5.11225536e-02,  4.61396992e-01, -9.88251269e-02,\n",
      "         6.85612381e-01, -2.65911758e-01, -2.06859306e-01,\n",
      "        -4.64986980e-01, -1.81869745e-01,  3.74494612e-01,\n",
      "         2.50269443e-01,  3.57066482e-01, -1.95039272e-01,\n",
      "         3.70394498e-01,  1.72352344e-01,  5.60779572e-01,\n",
      "        -6.99957013e-01, -1.41640604e-01,  2.12720051e-01,\n",
      "         1.77591681e-01, -3.57573539e-01,  2.32771471e-01,\n",
      "        -6.00988865e-01, -4.60550636e-01,  2.01752424e-01,\n",
      "         5.98659635e-01, -5.41949794e-02, -1.45361573e-02,\n",
      "         3.74883056e-01,  6.22681916e-01, -2.40575254e-01,\n",
      "        -8.60764235e-02, -3.55531305e-01, -4.13635999e-01,\n",
      "         1.87645257e-01, -1.07395463e-01, -5.90334058e-01,\n",
      "         2.94147551e-01, -1.23407520e-01, -3.33437026e-02,\n",
      "         7.97853947e-01,  2.02191293e-01, -3.87810767e-01,\n",
      "        -6.48847446e-02, -2.37387523e-01, -8.32500681e-02,\n",
      "        -9.54125643e-01, -2.92044997e-01,  2.78109550e-01,\n",
      "        -6.20759428e-01,  1.39485359e-01,  2.20097408e-01,\n",
      "        -3.59452158e-01,  3.68384980e-02, -1.75044179e-01,\n",
      "         5.47913492e-01, -1.59377754e-01, -1.52973920e-01,\n",
      "         9.99494642e-03, -6.64759099e-01, -1.22268930e-01,\n",
      "         5.72988331e-01,  7.69164786e-03,  3.10915142e-01,\n",
      "        -1.95781067e-01,  1.93613708e-01,  2.02473581e-01,\n",
      "         2.21079588e-02,  6.53551996e-01,  7.35361755e-01,\n",
      "         2.84016132e-01, -3.07080865e-01,  2.03652918e-01,\n",
      "         4.42510247e-01, -7.83165336e-01, -4.12785023e-01,\n",
      "        -4.62051302e-01, -4.89832997e-01, -1.02116019e-01,\n",
      "         1.75815240e-01, -9.53847244e-02, -7.23971367e-01,\n",
      "        -5.47810793e-02, -4.57924545e-01,  5.28754771e-01,\n",
      "        -6.18065000e-01,  2.49040693e-01, -3.14362943e-02,\n",
      "         1.85929328e-01,  3.43938857e-01,  6.43316656e-03,\n",
      "         3.04271221e-01, -2.65834421e-01, -2.80411512e-01,\n",
      "        -3.77382457e-01,  2.10386276e-01, -5.53719513e-02,\n",
      "         4.16632950e-01,  4.99984145e-01,  2.82812566e-01,\n",
      "        -2.93531358e-01, -3.09672415e-01, -5.86318038e-02,\n",
      "        -3.62507850e-02, -3.38511467e-01,  9.37829167e-02,\n",
      "         6.01502180e-01, -1.89679861e-01, -3.84049654e-01,\n",
      "         3.38774741e-01, -4.16304648e-01,  5.27140915e-01,\n",
      "         3.32230717e-01,  3.96336377e-01,  7.96584785e-03,\n",
      "         7.64003277e-01, -5.33229649e-01, -1.51585788e-02,\n",
      "        -1.38791338e-01, -8.20695698e-01,  8.74522805e-01,\n",
      "         5.19650280e-01,  1.71633009e-02, -3.69593024e-01,\n",
      "        -8.02614689e-02, -5.41878700e-01, -4.52454269e-01,\n",
      "         6.29248857e-01, -9.25729394e-01,  8.59696120e-02,\n",
      "         7.08113611e-02, -5.57892621e-01,  1.45811647e-01,\n",
      "         4.18468416e-01,  2.22557157e-01, -8.71322036e-01,\n",
      "         1.03178769e-01,  2.72951663e-01, -3.91322404e-01,\n",
      "         1.91091061e-01,  3.04147989e-01,  1.28370672e-01,\n",
      "        -1.20774671e-01, -8.20726007e-02,  1.89948380e-02,\n",
      "         1.08232819e-01,  2.90570915e-01, -1.90853283e-01,\n",
      "         8.71506855e-02, -8.02577659e-02,  3.57865214e-01,\n",
      "         6.13394901e-02,  3.08313370e-01,  1.03715301e-01,\n",
      "        -2.06374347e-01,  4.14860934e-01,  7.30836019e-02,\n",
      "         1.28184676e-01,  8.64197686e-02, -5.22416607e-02,\n",
      "        -1.85080498e-01, -2.49684840e-01,  6.74399361e-02,\n",
      "         2.08111584e-01,  5.16160011e-01,  8.99980962e-03,\n",
      "        -5.00469387e-01,  9.28633809e-01,  7.13290453e-01,\n",
      "        -5.08066893e-01,  2.41853312e-01, -8.25704753e-01,\n",
      "         3.84098031e-02, -3.90461802e-01, -1.04186289e-01,\n",
      "        -3.27865392e-01,  5.13534307e-01, -1.64529696e-01,\n",
      "         1.22696787e-01,  1.45434290e-01, -9.53416526e-02,\n",
      "        -1.93530262e-01,  1.60262287e-02,  1.37296528e-01,\n",
      "        -4.21640247e-01, -5.22669137e-01,  4.43379760e-01,\n",
      "         8.78234059e-02, -4.09402046e-03, -9.61997926e-01,\n",
      "        -5.51723719e-01,  1.51194811e-01, -1.93665534e-01,\n",
      "         8.77205968e-01, -5.40399194e-01, -1.92646742e-01,\n",
      "        -2.25691706e-01,  8.31208900e-02,  3.66210580e-01,\n",
      "         3.00489217e-01, -3.80386442e-01, -2.53438801e-01,\n",
      "        -2.81748533e-01,  5.97800493e-01, -6.65363073e-01,\n",
      "        -1.44117698e-02, -4.01932091e-01, -1.20447524e-01,\n",
      "        -4.08354878e-01,  1.26266629e-01, -3.31203163e-01,\n",
      "        -1.63841203e-01, -1.05562523e-01,  2.00656667e-01,\n",
      "        -1.66390479e-01, -9.03046668e-01,  3.26644331e-01,\n",
      "        -2.18792230e-01,  4.41526026e-01, -1.52798817e-02,\n",
      "        -5.15090644e-01, -4.42179024e-01, -2.33587801e-01]], dtype=float32), lengths=array([], dtype=int32), data_shape=(-1, 768), starts_ends=None)\n"
     ]
    }
   ],
   "source": [
    "frase1 = 'atorga per primer cop les mencions encarna sanahuja a la inclusió de la perspectiva de gènere en docència universitària'\n",
    "doc = nlp_trf(frase1)\n",
    "embeddings = doc._.trf_data.last_hidden_layer_state\n",
    "print(embeddings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atorga per primer cop les mencions encarna sanahuja a la inclusió de la perspectiva de gènere en docència universitària'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'all_hidden_layer_states', 'all_outputs', 'embedding_layer', 'from_dict', 'last_hidden_layer_state', 'last_layer_only', 'num_outputs', 'to_dict']\n"
     ]
    }
   ],
   "source": [
    "print(dir(doc._.trf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tu frase\n",
    "frase = \"Tu frase aquí\"\n",
    "\n",
    "def spacy_trf_CLS(frase1, frase2, model):# Procesar la frase\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    doc1 = model(frase1)\n",
    "    doc2 = model(frase2)\n",
    "\n",
    "    # Obtener el embedding CLS\n",
    "    embedding_CLS1 = doc1._.trf_data.tensors[-1][0]\n",
    "    embedding_CLS2 = doc2._.trf_data.tensors[-1][0]\n",
    "    \n",
    "    return embedding_CLS1, embedding_CLS2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DocTransformerOutput' object has no attribute 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m spacy_trf_CLS_corpus_semantic \u001b[38;5;241m=\u001b[39m corpus_semantic\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (frase1, frase2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(spacy_trf_CLS_corpus_semantic):\n\u001b[1;32m----> 4\u001b[0m     frase1, frase2 \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_trf_CLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrase1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrase2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_trf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     spacy_trf_CLS_corpus_semantic[i] \u001b[38;5;241m=\u001b[39m (frase1, frase2)\n",
      "Cell \u001b[1;32mIn[79], line 11\u001b[0m, in \u001b[0;36mspacy_trf_CLS\u001b[1;34m(frase1, frase2, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m doc2 \u001b[38;5;241m=\u001b[39m model(frase2)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Obtener el embedding CLS\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m embedding_CLS1 \u001b[38;5;241m=\u001b[39m \u001b[43mdoc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m embedding_CLS2 \u001b[38;5;241m=\u001b[39m doc2\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mtrf_data\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding_CLS1, embedding_CLS2\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DocTransformerOutput' object has no attribute 'tensors'"
     ]
    }
   ],
   "source": [
    "spacy_trf_CLS_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_CLS_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_CLS_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from scipy.special import logit\n",
    "\n",
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2-cased-sts and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (frase1, frase2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fine_tuned_corpus_semantic):\n\u001b[0;32m     21\u001b[0m     fine_tuned_corpus_semantic[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frase1), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frase2))\n\u001b[1;32m---> 23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuned_corpus_semantic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 15\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(sentence_pairs)\u001b[0m\n\u001b[0;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(pair, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pytorch_utils.py:242\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:467\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:365\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Convert sentence pairs to embeddings using the transformer model\n",
    "def get_embeddings(sentence_pairs):\n",
    "    prepared_pairs = prepare(sentence_pairs)\n",
    "    embeddings = []\n",
    "    for pair in prepared_pairs:\n",
    "        inputs = tokenizer(pair, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings\n",
    "\n",
    "fine_tuned_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_tuned_corpus_semantic):\n",
    "    fine_tuned_corpus_semantic[i] = (' '.join(frase1), ' '.join(frase2))\n",
    "\n",
    "embeddings = get_embeddings(fine_tuned_corpus_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2-cased-sts and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (frase1, frase2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fine_tuned_corpus_semantic):\n\u001b[0;32m     21\u001b[0m     fine_tuned_corpus_semantic[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frase1), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frase2))\n\u001b[1;32m---> 23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuned_corpus_semantic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Separate embeddings into two arrays for the sentence pairs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m vectors_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embedding[:\u001b[38;5;28mlen\u001b[39m(embedding)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings])\n",
      "Cell \u001b[1;32mIn[73], line 15\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(sentence_pairs)\u001b[0m\n\u001b[0;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(pair, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:349\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    341\u001b[0m         hidden_states,\n\u001b[0;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m         output_attentions,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[1;32m--> 349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:298\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 298\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    300\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Separate embeddings into two arrays for the sentence pairs\n",
    "vectors_1 = np.array([embedding[:len(embedding)//2] for embedding in embeddings])\n",
    "vectors_2 = np.array([embedding[len(embedding)//2:] for embedding in embeddings])\n",
    "\n",
    "# Assuming 'semantic_score' is already defined as the target similarity scores\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "# Define and compile the Keras model\n",
    "def build_and_compile_model(input_length: int, hidden_size: int = 64) -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input(shape=(input_length,)), tf.keras.Input(shape=(input_length,))\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    _dense_1, _dense_2 = dense(input_1), dense(input_2)\n",
    "    _concatenated = concatenate([_dense_1, _dense_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "input_length = vectors_1.shape[1]\n",
    "baseline_model = build_and_compile_model(input_length)\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_runed_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_runed_corpus_semantic):\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    fine_runed_corpus_semantic[i] = (frase1, frase2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'SIMILARITY', 'score': 3.639443974101274}, {'label': 'SIMILARITY', 'score': 1.3343049254405437}, {'label': 'SIMILARITY', 'score': 3.6255003414699982}, {'label': 'SIMILARITY', 'score': 2.8571212701413073}, {'label': 'SIMILARITY', 'score': 1.7412681993979517}, {'label': 'SIMILARITY', 'score': 2.902458331608203}, {'label': 'SIMILARITY', 'score': 3.3925175786892536}, {'label': 'SIMILARITY', 'score': 2.7214751443641485}, {'label': 'SIMILARITY', 'score': 2.848001481281147}, {'label': 'SIMILARITY', 'score': 2.8075600107730767}, {'label': 'SIMILARITY', 'score': 2.9737861311233997}, {'label': 'SIMILARITY', 'score': 0.946479625573919}, {'label': 'SIMILARITY', 'score': 2.0864164114103474}, {'label': 'SIMILARITY', 'score': 3.390759591752991}, {'label': 'SIMILARITY', 'score': 3.119538929859854}, {'label': 'SIMILARITY', 'score': 2.835003453924068}, {'label': 'SIMILARITY', 'score': 1.6847840009938235}, {'label': 'SIMILARITY', 'score': 3.1470772247192316}, {'label': 'SIMILARITY', 'score': 2.6824705621661047}, {'label': 'SIMILARITY', 'score': 4.226412654742718}, {'label': 'SIMILARITY', 'score': 3.126528135836064}, {'label': 'SIMILARITY', 'score': 1.7081137790117091}, {'label': 'SIMILARITY', 'score': 4.230167032371548}, {'label': 'SIMILARITY', 'score': 1.8934870039501301}, {'label': 'SIMILARITY', 'score': 2.7132040190339723}, {'label': 'SIMILARITY', 'score': 2.6130709971824206}, {'label': 'SIMILARITY', 'score': 2.037800000515877}, {'label': 'SIMILARITY', 'score': 2.33263418193179}, {'label': 'SIMILARITY', 'score': 2.5973923458820583}, {'label': 'SIMILARITY', 'score': 0.8618329165908697}, {'label': 'SIMILARITY', 'score': 4.2248132103366265}, {'label': 'SIMILARITY', 'score': 2.7458963517477017}, {'label': 'SIMILARITY', 'score': 3.3074664974471806}, {'label': 'SIMILARITY', 'score': 3.7756875804485848}, {'label': 'SIMILARITY', 'score': 3.0761924276671877}, {'label': 'SIMILARITY', 'score': 2.4497194024832134}, {'label': 'SIMILARITY', 'score': 2.3104285309815897}, {'label': 'SIMILARITY', 'score': 2.055789338626736}, {'label': 'SIMILARITY', 'score': 3.5544121850107184}, {'label': 'SIMILARITY', 'score': 3.0630930880441944}, {'label': 'SIMILARITY', 'score': 1.9726317429036382}, {'label': 'SIMILARITY', 'score': 2.1199992941213046}, {'label': 'SIMILARITY', 'score': 2.503829783954064}, {'label': 'SIMILARITY', 'score': 4.207686592066606}, {'label': 'SIMILARITY', 'score': 3.0206260061968004}, {'label': 'SIMILARITY', 'score': 2.553386819701617}, {'label': 'SIMILARITY', 'score': 1.6169672689316072}, {'label': 'SIMILARITY', 'score': 2.5593145534221518}, {'label': 'SIMILARITY', 'score': 2.4372375847977272}, {'label': 'SIMILARITY', 'score': 2.0298332542640454}, {'label': 'SIMILARITY', 'score': 2.014581006361909}, {'label': 'SIMILARITY', 'score': 1.4988620081551278}, {'label': 'SIMILARITY', 'score': 4.21061992739215}, {'label': 'SIMILARITY', 'score': 2.096065271846751}, {'label': 'SIMILARITY', 'score': 2.902290375164996}, {'label': 'SIMILARITY', 'score': 3.1974557566228814}, {'label': 'SIMILARITY', 'score': 3.2517584576470235}, {'label': 'SIMILARITY', 'score': 1.4670121191708474}, {'label': 'SIMILARITY', 'score': 1.6989868085832516}, {'label': 'SIMILARITY', 'score': 1.456438533620495}, {'label': 'SIMILARITY', 'score': 2.5803544010880874}, {'label': 'SIMILARITY', 'score': 1.6024959773586511}, {'label': 'SIMILARITY', 'score': 1.2920962565510448}, {'label': 'SIMILARITY', 'score': 1.71866389412072}, {'label': 'SIMILARITY', 'score': 3.1044316994336874}, {'label': 'SIMILARITY', 'score': 3.0982684270197693}, {'label': 'SIMILARITY', 'score': 3.704224110226398}, {'label': 'SIMILARITY', 'score': 2.0405611365364362}, {'label': 'SIMILARITY', 'score': 1.9703906099460806}, {'label': 'SIMILARITY', 'score': 2.9243630147918447}, {'label': 'SIMILARITY', 'score': 2.353398937622096}, {'label': 'SIMILARITY', 'score': 1.5367582813635792}, {'label': 'SIMILARITY', 'score': 3.5366171942583695}, {'label': 'SIMILARITY', 'score': 1.55160417783913}, {'label': 'SIMILARITY', 'score': 1.6197496913091034}, {'label': 'SIMILARITY', 'score': 4.219516293539999}, {'label': 'SIMILARITY', 'score': 3.4760505156255235}, {'label': 'SIMILARITY', 'score': 3.200305403284124}, {'label': 'SIMILARITY', 'score': 0.46994832393887703}, {'label': 'SIMILARITY', 'score': 0.20668384803437304}, {'label': 'SIMILARITY', 'score': 3.5399204653194913}, {'label': 'SIMILARITY', 'score': 2.4224328154873165}, {'label': 'SIMILARITY', 'score': 2.961230503858237}, {'label': 'SIMILARITY', 'score': 2.777953287644126}, {'label': 'SIMILARITY', 'score': 3.571589811709348}, {'label': 'SIMILARITY', 'score': 1.4796314170156202}, {'label': 'SIMILARITY', 'score': 3.5147780437246907}, {'label': 'SIMILARITY', 'score': 3.024355901512201}, {'label': 'SIMILARITY', 'score': 2.8038727765075016}, {'label': 'SIMILARITY', 'score': 3.056932181334793}, {'label': 'SIMILARITY', 'score': 2.790148050464573}, {'label': 'SIMILARITY', 'score': 1.3172318304470148}, {'label': 'SIMILARITY', 'score': 2.6925369972500164}, {'label': 'SIMILARITY', 'score': 3.051272714582565}, {'label': 'SIMILARITY', 'score': 1.5833144259105116}, {'label': 'SIMILARITY', 'score': 4.227400428976332}, {'label': 'SIMILARITY', 'score': 1.9046106329307044}, {'label': 'SIMILARITY', 'score': 2.4374190338672723}, {'label': 'SIMILARITY', 'score': 4.229804466303161}, {'label': 'SIMILARITY', 'score': 1.743852826246043}, {'label': 'SIMILARITY', 'score': 2.7442518021171507}, {'label': 'SIMILARITY', 'score': 1.8025528931227843}, {'label': 'SIMILARITY', 'score': 2.6133538965626832}, {'label': 'SIMILARITY', 'score': 2.939813010704966}, {'label': 'SIMILARITY', 'score': 4.228612271574886}, {'label': 'SIMILARITY', 'score': 3.20009629087962}, {'label': 'SIMILARITY', 'score': 2.6888448654536274}, {'label': 'SIMILARITY', 'score': 2.9412785338206935}, {'label': 'SIMILARITY', 'score': 3.016453271243749}, {'label': 'SIMILARITY', 'score': 4.226463071325598}, {'label': 'SIMILARITY', 'score': 3.0369117963217027}, {'label': 'SIMILARITY', 'score': 2.746696893899641}, {'label': 'SIMILARITY', 'score': 3.2980775397822106}, {'label': 'SIMILARITY', 'score': 2.5416401420442147}, {'label': 'SIMILARITY', 'score': 4.218023357515997}, {'label': 'SIMILARITY', 'score': 2.705917995193654}, {'label': 'SIMILARITY', 'score': 2.8755191545986456}, {'label': 'SIMILARITY', 'score': 4.225971614913898}, {'label': 'SIMILARITY', 'score': 3.0028536739078375}, {'label': 'SIMILARITY', 'score': 2.9187071777655644}, {'label': 'SIMILARITY', 'score': 2.8499991583563253}, {'label': 'SIMILARITY', 'score': 1.2641152837976914}, {'label': 'SIMILARITY', 'score': 3.2181226744779114}, {'label': 'SIMILARITY', 'score': 2.8157523992899356}, {'label': 'SIMILARITY', 'score': 1.8078392116373434}, {'label': 'SIMILARITY', 'score': 3.016415774831342}, {'label': 'SIMILARITY', 'score': 3.0239877369556427}, {'label': 'SIMILARITY', 'score': 2.8940109510668064}, {'label': 'SIMILARITY', 'score': 2.92026179035961}, {'label': 'SIMILARITY', 'score': 2.965900465325177}, {'label': 'SIMILARITY', 'score': 1.6589238770747456}, {'label': 'SIMILARITY', 'score': 3.1170292617630935}, {'label': 'SIMILARITY', 'score': 1.7856236135356318}, {'label': 'SIMILARITY', 'score': 0.9128038532361863}, {'label': 'SIMILARITY', 'score': 1.8424520908191835}, {'label': 'SIMILARITY', 'score': 2.862966469302841}, {'label': 'SIMILARITY', 'score': 4.224804820854788}, {'label': 'SIMILARITY', 'score': 2.818794392297546}, {'label': 'SIMILARITY', 'score': 2.0296770289593535}, {'label': 'SIMILARITY', 'score': 2.9674777282964238}, {'label': 'SIMILARITY', 'score': 2.8211363562399567}, {'label': 'SIMILARITY', 'score': 1.4061379221917623}, {'label': 'SIMILARITY', 'score': 2.710597186200766}, {'label': 'SIMILARITY', 'score': 2.134538006652288}, {'label': 'SIMILARITY', 'score': 2.2909085852105817}, {'label': 'SIMILARITY', 'score': 1.134789361279017}, {'label': 'SIMILARITY', 'score': 4.228780695913693}, {'label': 'SIMILARITY', 'score': 4.159190106927247}, {'label': 'SIMILARITY', 'score': 3.1006963152521454}, {'label': 'SIMILARITY', 'score': 3.0203531557415038}, {'label': 'SIMILARITY', 'score': 2.840387895230038}, {'label': 'SIMILARITY', 'score': 1.2675995411442682}, {'label': 'SIMILARITY', 'score': 4.162833404558112}, {'label': 'SIMILARITY', 'score': 1.3560100688925327}, {'label': 'SIMILARITY', 'score': 2.813026515839836}, {'label': 'SIMILARITY', 'score': 3.017687346482905}, {'label': 'SIMILARITY', 'score': 2.780039680429342}, {'label': 'SIMILARITY', 'score': 2.720131691573945}, {'label': 'SIMILARITY', 'score': 2.7093184012717004}, {'label': 'SIMILARITY', 'score': 2.911358635471955}, {'label': 'SIMILARITY', 'score': 2.937407592407723}, {'label': 'SIMILARITY', 'score': 3.7306836791077}, {'label': 'SIMILARITY', 'score': 4.227875738868506}, {'label': 'SIMILARITY', 'score': 3.579928509461553}, {'label': 'SIMILARITY', 'score': 1.4368905254749262}, {'label': 'SIMILARITY', 'score': 4.12379352214613}, {'label': 'SIMILARITY', 'score': 3.372992670624899}, {'label': 'SIMILARITY', 'score': 2.7491124534783973}, {'label': 'SIMILARITY', 'score': 4.227156549795726}, {'label': 'SIMILARITY', 'score': 3.0359790263060322}, {'label': 'SIMILARITY', 'score': 2.92102700950639}, {'label': 'SIMILARITY', 'score': 3.0056628621089567}, {'label': 'SIMILARITY', 'score': 1.200823651899384}, {'label': 'SIMILARITY', 'score': 2.948393467660903}, {'label': 'SIMILARITY', 'score': 2.8618588484233443}, {'label': 'SIMILARITY', 'score': 3.118068869700141}, {'label': 'SIMILARITY', 'score': 2.91315920733368}, {'label': 'SIMILARITY', 'score': 3.1645571460735336}, {'label': 'SIMILARITY', 'score': 2.0219172518979365}, {'label': 'SIMILARITY', 'score': 4.2129394506399285}, {'label': 'SIMILARITY', 'score': 2.1081360281427433}, {'label': 'SIMILARITY', 'score': 1.467554322415775}, {'label': 'SIMILARITY', 'score': 1.4159468401856514}, {'label': 'SIMILARITY', 'score': 3.069211480982673}, {'label': 'SIMILARITY', 'score': 1.5595818937241979}, {'label': 'SIMILARITY', 'score': 2.9752946697799416}, {'label': 'SIMILARITY', 'score': 0.5193965554466606}, {'label': 'SIMILARITY', 'score': 3.0996037489678576}, {'label': 'SIMILARITY', 'score': 3.1757290051480753}, {'label': 'SIMILARITY', 'score': 2.9916760260942765}, {'label': 'SIMILARITY', 'score': 2.212808438342496}, {'label': 'SIMILARITY', 'score': 3.169885292001065}, {'label': 'SIMILARITY', 'score': 2.751802677665469}, {'label': 'SIMILARITY', 'score': 2.829011834007042}, {'label': 'SIMILARITY', 'score': 1.089342739933811}, {'label': 'SIMILARITY', 'score': 1.7078095333131873}, {'label': 'SIMILARITY', 'score': 3.0458884135586968}, {'label': 'SIMILARITY', 'score': 2.9698799370264077}, {'label': 'SIMILARITY', 'score': 3.9511607969277325}, {'label': 'SIMILARITY', 'score': 3.140984256099914}, {'label': 'SIMILARITY', 'score': 1.35784116560879}, {'label': 'SIMILARITY', 'score': 2.4284178622283226}, {'label': 'SIMILARITY', 'score': 2.6780969243768835}, {'label': 'SIMILARITY', 'score': 2.38691817636789}, {'label': 'SIMILARITY', 'score': 3.052426077523179}, {'label': 'SIMILARITY', 'score': 2.4681657409480815}, {'label': 'SIMILARITY', 'score': 2.1257151111651003}, {'label': 'SIMILARITY', 'score': 3.9096723416869654}, {'label': 'SIMILARITY', 'score': 1.4612729069468748}, {'label': 'SIMILARITY', 'score': 3.0711970851571184}, {'label': 'SIMILARITY', 'score': 2.186390860872444}, {'label': 'SIMILARITY', 'score': 3.009113587207771}, {'label': 'SIMILARITY', 'score': 1.2888792002721865}, {'label': 'SIMILARITY', 'score': 1.5713178624541315}, {'label': 'SIMILARITY', 'score': 2.813513116625873}, {'label': 'SIMILARITY', 'score': 2.992460676313603}, {'label': 'SIMILARITY', 'score': 3.03980650181559}, {'label': 'SIMILARITY', 'score': 3.18149373759479}, {'label': 'SIMILARITY', 'score': 2.9256244631885444}, {'label': 'SIMILARITY', 'score': 2.8360256659771808}, {'label': 'SIMILARITY', 'score': 3.7783166215523583}, {'label': 'SIMILARITY', 'score': 1.700310418721196}, {'label': 'SIMILARITY', 'score': 2.065903589576782}, {'label': 'SIMILARITY', 'score': 1.2895039753385458}, {'label': 'SIMILARITY', 'score': 3.4716539838011466}, {'label': 'SIMILARITY', 'score': 4.229353544843205}, {'label': 'SIMILARITY', 'score': 2.5031049546499413}, {'label': 'SIMILARITY', 'score': 3.3787713295742705}, {'label': 'SIMILARITY', 'score': 2.948452656785678}, {'label': 'SIMILARITY', 'score': 2.992358469954168}, {'label': 'SIMILARITY', 'score': 4.169244276535476}, {'label': 'SIMILARITY', 'score': 2.5174634936460625}, {'label': 'SIMILARITY', 'score': 2.745734418328459}, {'label': 'SIMILARITY', 'score': 2.6113797267034666}, {'label': 'SIMILARITY', 'score': 3.103897913148543}, {'label': 'SIMILARITY', 'score': 3.0289446637824105}, {'label': 'SIMILARITY', 'score': 2.9448867943929145}, {'label': 'SIMILARITY', 'score': 4.2278084250164705}, {'label': 'SIMILARITY', 'score': 3.984131509949227}, {'label': 'SIMILARITY', 'score': 1.3068973287949879}, {'label': 'SIMILARITY', 'score': 1.4124480194022744}, {'label': 'SIMILARITY', 'score': 1.7510200828232885}, {'label': 'SIMILARITY', 'score': 2.485364293442714}, {'label': 'SIMILARITY', 'score': 1.0983106324412342}, {'label': 'SIMILARITY', 'score': 2.820636343616339}, {'label': 'SIMILARITY', 'score': 2.855755117024779}, {'label': 'SIMILARITY', 'score': 0.35137223249830174}, {'label': 'SIMILARITY', 'score': 3.0348223441718796}, {'label': 'SIMILARITY', 'score': 2.2220823803984606}, {'label': 'SIMILARITY', 'score': 3.2017036897047384}, {'label': 'SIMILARITY', 'score': 2.61559423829568}, {'label': 'SIMILARITY', 'score': 1.0293488830716857}, {'label': 'SIMILARITY', 'score': 0.7828790246387515}, {'label': 'SIMILARITY', 'score': 3.2126075798148674}, {'label': 'SIMILARITY', 'score': 1.8668347701398926}, {'label': 'SIMILARITY', 'score': 3.248296571171331}, {'label': 'SIMILARITY', 'score': 2.899799372309607}, {'label': 'SIMILARITY', 'score': 2.8083442412354382}, {'label': 'SIMILARITY', 'score': 0.6485375282493143}, {'label': 'SIMILARITY', 'score': 3.0110202252283753}, {'label': 'SIMILARITY', 'score': 3.23897640364101}, {'label': 'SIMILARITY', 'score': 4.1438053621014035}, {'label': 'SIMILARITY', 'score': 2.5780710016600707}, {'label': 'SIMILARITY', 'score': 2.486840668806698}, {'label': 'SIMILARITY', 'score': 1.8580878809792731}, {'label': 'SIMILARITY', 'score': 2.3066425000770687}, {'label': 'SIMILARITY', 'score': 2.497385088706873}, {'label': 'SIMILARITY', 'score': 4.229749670911981}, {'label': 'SIMILARITY', 'score': 1.4824638751848673}, {'label': 'SIMILARITY', 'score': 1.9703079039813782}, {'label': 'SIMILARITY', 'score': 3.0260864382474075}, {'label': 'SIMILARITY', 'score': 3.8321135515605946}, {'label': 'SIMILARITY', 'score': 3.394109638380895}, {'label': 'SIMILARITY', 'score': 2.714427831314883}, {'label': 'SIMILARITY', 'score': 4.225623117068601}, {'label': 'SIMILARITY', 'score': 0.8934796135497209}, {'label': 'SIMILARITY', 'score': 2.963847753801709}, {'label': 'SIMILARITY', 'score': 1.5892025721688132}, {'label': 'SIMILARITY', 'score': 2.4632011969765046}, {'label': 'SIMILARITY', 'score': 0.9980283397580882}, {'label': 'SIMILARITY', 'score': 3.0770793217423704}, {'label': 'SIMILARITY', 'score': 2.515052317331713}, {'label': 'SIMILARITY', 'score': 2.856845122056044}, {'label': 'SIMILARITY', 'score': 1.8623467562244573}, {'label': 'SIMILARITY', 'score': 2.8842874423553786}, {'label': 'SIMILARITY', 'score': 2.8372346229500303}, {'label': 'SIMILARITY', 'score': 2.2104247369403187}, {'label': 'SIMILARITY', 'score': 2.699187747718144}, {'label': 'SIMILARITY', 'score': 4.195852510728515}, {'label': 'SIMILARITY', 'score': 2.8933407413200087}, {'label': 'SIMILARITY', 'score': 2.7510270540550974}, {'label': 'SIMILARITY', 'score': 1.6474001377688259}, {'label': 'SIMILARITY', 'score': 1.9173333737018692}, {'label': 'SIMILARITY', 'score': 0.034052324215461185}, {'label': 'SIMILARITY', 'score': 2.0433950588434104}, {'label': 'SIMILARITY', 'score': 0.7159323739709014}, {'label': 'SIMILARITY', 'score': 2.48592202383072}, {'label': 'SIMILARITY', 'score': 0.8541683205392421}, {'label': 'SIMILARITY', 'score': 2.866539292114396}, {'label': 'SIMILARITY', 'score': 3.2467260433932044}, {'label': 'SIMILARITY', 'score': 1.9688755237304902}, {'label': 'SIMILARITY', 'score': 3.097886175354952}, {'label': 'SIMILARITY', 'score': 3.098480526089397}, {'label': 'SIMILARITY', 'score': 4.230167032371548}, {'label': 'SIMILARITY', 'score': 3.1101061997782398}, {'label': 'SIMILARITY', 'score': 2.765823148158267}, {'label': 'SIMILARITY', 'score': 2.9877963359391626}, {'label': 'SIMILARITY', 'score': 1.3643189749948856}, {'label': 'SIMILARITY', 'score': 2.1209620972293406}, {'label': 'SIMILARITY', 'score': 3.2086568270099796}, {'label': 'SIMILARITY', 'score': 3.013043636227416}, {'label': 'SIMILARITY', 'score': 0.7457657547402406}, {'label': 'SIMILARITY', 'score': 1.5660171181849194}, {'label': 'SIMILARITY', 'score': 0.9738820866812639}, {'label': 'SIMILARITY', 'score': 3.1085118646409886}, {'label': 'SIMILARITY', 'score': 3.102806493008044}, {'label': 'SIMILARITY', 'score': 3.4300606256815946}, {'label': 'SIMILARITY', 'score': 2.3298859069852504}, {'label': 'SIMILARITY', 'score': 3.1218022763698436}, {'label': 'SIMILARITY', 'score': 1.5513877201122228}, {'label': 'SIMILARITY', 'score': 2.829264244965164}, {'label': 'SIMILARITY', 'score': 2.811416427275298}, {'label': 'SIMILARITY', 'score': 3.311793054509829}, {'label': 'SIMILARITY', 'score': 0.8590896535149101}, {'label': 'SIMILARITY', 'score': 2.4052958543760634}, {'label': 'SIMILARITY', 'score': 3.0398653217774854}, {'label': 'SIMILARITY', 'score': 1.9133161044507512}, {'label': 'SIMILARITY', 'score': 0.7133416484106022}, {'label': 'SIMILARITY', 'score': 2.248183004619509}, {'label': 'SIMILARITY', 'score': 2.9969118767397562}, {'label': 'SIMILARITY', 'score': 3.332055372678551}, {'label': 'SIMILARITY', 'score': 2.6205714885977587}, {'label': 'SIMILARITY', 'score': 1.774445288008859}, {'label': 'SIMILARITY', 'score': 1.5920464405826367}, {'label': 'SIMILARITY', 'score': 2.8744014173839467}, {'label': 'SIMILARITY', 'score': 0.9816306592670437}, {'label': 'SIMILARITY', 'score': 1.9793031310431384}, {'label': 'SIMILARITY', 'score': 1.953010200587775}, {'label': 'SIMILARITY', 'score': 1.541124038752437}, {'label': 'SIMILARITY', 'score': 3.169429373791626}, {'label': 'SIMILARITY', 'score': 1.90844617790744}, {'label': 'SIMILARITY', 'score': 3.1420694698280487}, {'label': 'SIMILARITY', 'score': 1.2640142965612113}, {'label': 'SIMILARITY', 'score': 3.1005459504047295}, {'label': 'SIMILARITY', 'score': 2.5661151096577695}, {'label': 'SIMILARITY', 'score': 2.1049073924039363}, {'label': 'SIMILARITY', 'score': 2.6056299335326463}, {'label': 'SIMILARITY', 'score': 3.0325003893976716}, {'label': 'SIMILARITY', 'score': 3.368248036406151}, {'label': 'SIMILARITY', 'score': 4.229960438380887}, {'label': 'SIMILARITY', 'score': 3.1989010051702502}, {'label': 'SIMILARITY', 'score': 3.152963248371442}, {'label': 'SIMILARITY', 'score': 1.4202876279520948}, {'label': 'SIMILARITY', 'score': 1.2184930929344007}, {'label': 'SIMILARITY', 'score': 2.054270984496596}, {'label': 'SIMILARITY', 'score': 4.155347853156986}, {'label': 'SIMILARITY', 'score': 3.2524465152866155}, {'label': 'SIMILARITY', 'score': 2.581495959758173}, {'label': 'SIMILARITY', 'score': 4.226698381359096}, {'label': 'SIMILARITY', 'score': 1.0818150867199323}, {'label': 'SIMILARITY', 'score': 2.6994350772841975}, {'label': 'SIMILARITY', 'score': 2.2326376133418586}, {'label': 'SIMILARITY', 'score': 2.656800011875543}, {'label': 'SIMILARITY', 'score': 1.4062023929447576}, {'label': 'SIMILARITY', 'score': 2.8663801539082456}, {'label': 'SIMILARITY', 'score': 2.9656305583752953}, {'label': 'SIMILARITY', 'score': 3.7292119794925833}, {'label': 'SIMILARITY', 'score': 2.6037255401127077}, {'label': 'SIMILARITY', 'score': 3.148629220845153}, {'label': 'SIMILARITY', 'score': 2.7923896314801295}, {'label': 'SIMILARITY', 'score': 2.542273687886832}, {'label': 'SIMILARITY', 'score': 1.720448154949384}, {'label': 'SIMILARITY', 'score': 1.8322806104864793}, {'label': 'SIMILARITY', 'score': 3.036433039299446}, {'label': 'SIMILARITY', 'score': 2.59583265782504}, {'label': 'SIMILARITY', 'score': 1.6213009440315818}, {'label': 'SIMILARITY', 'score': 3.017558648749842}, {'label': 'SIMILARITY', 'score': 2.283177847237938}, {'label': 'SIMILARITY', 'score': 1.9253536672823575}, {'label': 'SIMILARITY', 'score': 2.4115774669013468}, {'label': 'SIMILARITY', 'score': 4.086524097063126}, {'label': 'SIMILARITY', 'score': 2.29016229909194}, {'label': 'SIMILARITY', 'score': 4.231027583825688}, {'label': 'SIMILARITY', 'score': 3.2353227471191643}, {'label': 'SIMILARITY', 'score': 2.7267241982743515}, {'label': 'SIMILARITY', 'score': 2.8975918089897665}, {'label': 'SIMILARITY', 'score': 3.287468925487733}, {'label': 'SIMILARITY', 'score': 2.110124854872156}, {'label': 'SIMILARITY', 'score': 3.9238726834672772}, {'label': 'SIMILARITY', 'score': 2.512956364454831}, {'label': 'SIMILARITY', 'score': 2.185333579696101}, {'label': 'SIMILARITY', 'score': 3.0644687498957324}, {'label': 'SIMILARITY', 'score': 1.9422166461990815}, {'label': 'SIMILARITY', 'score': 1.9049668452910087}, {'label': 'SIMILARITY', 'score': 2.581028450395889}, {'label': 'SIMILARITY', 'score': 3.13324801459684}, {'label': 'SIMILARITY', 'score': 1.3236724694777469}, {'label': 'SIMILARITY', 'score': 2.731833409399457}, {'label': 'SIMILARITY', 'score': 0.6287364646973483}, {'label': 'SIMILARITY', 'score': 3.2451660508255973}, {'label': 'SIMILARITY', 'score': 2.8610317614182805}, {'label': 'SIMILARITY', 'score': 3.0752596291267427}, {'label': 'SIMILARITY', 'score': 2.7156939191884533}, {'label': 'SIMILARITY', 'score': 2.175382097936648}, {'label': 'SIMILARITY', 'score': 3.038857609550097}, {'label': 'SIMILARITY', 'score': 1.1734607026969053}, {'label': 'SIMILARITY', 'score': 1.2267997837415492}, {'label': 'SIMILARITY', 'score': 0.8718432105190944}, {'label': 'SIMILARITY', 'score': 2.078656095552567}, {'label': 'SIMILARITY', 'score': 2.229766873297817}, {'label': 'SIMILARITY', 'score': 2.856843961913359}, {'label': 'SIMILARITY', 'score': 2.831286707225621}, {'label': 'SIMILARITY', 'score': 2.829813406014644}, {'label': 'SIMILARITY', 'score': 2.896349641193539}, {'label': 'SIMILARITY', 'score': 1.2448291946162573}, {'label': 'SIMILARITY', 'score': 2.8406577382292646}, {'label': 'SIMILARITY', 'score': 1.2466831643369258}, {'label': 'SIMILARITY', 'score': 2.0773853286854314}, {'label': 'SIMILARITY', 'score': 2.726219702393298}, {'label': 'SIMILARITY', 'score': 1.9037590639442183}, {'label': 'SIMILARITY', 'score': 3.7963658713228634}, {'label': 'SIMILARITY', 'score': 2.9663982386028462}, {'label': 'SIMILARITY', 'score': 4.224611881635764}, {'label': 'SIMILARITY', 'score': 1.795178900092425}, {'label': 'SIMILARITY', 'score': 3.2482916027865123}, {'label': 'SIMILARITY', 'score': 2.951172459999267}, {'label': 'SIMILARITY', 'score': 1.4263001649085536}, {'label': 'SIMILARITY', 'score': 2.2877209433583725}, {'label': 'SIMILARITY', 'score': 0.6276269853898965}, {'label': 'SIMILARITY', 'score': 3.1478287911810323}, {'label': 'SIMILARITY', 'score': 3.4519580964312424}, {'label': 'SIMILARITY', 'score': 1.9241328204539592}, {'label': 'SIMILARITY', 'score': 2.645226657791094}, {'label': 'SIMILARITY', 'score': 3.065686473625363}, {'label': 'SIMILARITY', 'score': 4.1070058178102515}, {'label': 'SIMILARITY', 'score': 3.146531230453979}, {'label': 'SIMILARITY', 'score': 4.229100778083209}, {'label': 'SIMILARITY', 'score': 1.766716402053653}, {'label': 'SIMILARITY', 'score': 3.0609937749194973}, {'label': 'SIMILARITY', 'score': 2.0112402859495546}, {'label': 'SIMILARITY', 'score': 2.429273836091733}, {'label': 'SIMILARITY', 'score': 3.1401219588712226}, {'label': 'SIMILARITY', 'score': 2.9773583752526873}, {'label': 'SIMILARITY', 'score': 1.8925926506138588}, {'label': 'SIMILARITY', 'score': 2.8616456112957995}, {'label': 'SIMILARITY', 'score': 2.951534846555649}, {'label': 'SIMILARITY', 'score': 2.126406745944653}, {'label': 'SIMILARITY', 'score': 3.304829672079567}, {'label': 'SIMILARITY', 'score': 2.898595691141948}, {'label': 'SIMILARITY', 'score': 2.6529881110551083}, {'label': 'SIMILARITY', 'score': 2.8862663603964696}, {'label': 'SIMILARITY', 'score': 3.1521853967034037}, {'label': 'SIMILARITY', 'score': 1.5310821816166167}, {'label': 'SIMILARITY', 'score': 3.020359875385059}, {'label': 'SIMILARITY', 'score': 2.8169565664205627}, {'label': 'SIMILARITY', 'score': 2.7191883114701536}, {'label': 'SIMILARITY', 'score': 1.560078220427731}, {'label': 'SIMILARITY', 'score': 2.8607895753662484}, {'label': 'SIMILARITY', 'score': 1.4971357666566159}, {'label': 'SIMILARITY', 'score': 2.738227769517429}, {'label': 'SIMILARITY', 'score': 2.989543649114411}, {'label': 'SIMILARITY', 'score': 4.190134074804389}, {'label': 'SIMILARITY', 'score': 3.096578874343152}, {'label': 'SIMILARITY', 'score': 2.452049796547854}, {'label': 'SIMILARITY', 'score': 4.176539484930962}, {'label': 'SIMILARITY', 'score': 3.043220523384986}, {'label': 'SIMILARITY', 'score': 3.2459094105694937}, {'label': 'SIMILARITY', 'score': 3.1148446120579347}, {'label': 'SIMILARITY', 'score': 1.508364024414071}, {'label': 'SIMILARITY', 'score': 2.7887132771167074}, {'label': 'SIMILARITY', 'score': 3.4568450042219707}, {'label': 'SIMILARITY', 'score': 2.2747578301587374}, {'label': 'SIMILARITY', 'score': 2.470745823562973}, {'label': 'SIMILARITY', 'score': 3.305562931081749}, {'label': 'SIMILARITY', 'score': 2.6745546514431253}, {'label': 'SIMILARITY', 'score': 4.2245783306790425}, {'label': 'SIMILARITY', 'score': 1.2658801847968424}, {'label': 'SIMILARITY', 'score': 2.8796105242126964}, {'label': 'SIMILARITY', 'score': 2.4558328435380177}, {'label': 'SIMILARITY', 'score': 3.118099720638933}, {'label': 'SIMILARITY', 'score': 2.8829530853714846}, {'label': 'SIMILARITY', 'score': 2.8801410624499586}, {'label': 'SIMILARITY', 'score': 2.7724897418736245}, {'label': 'SIMILARITY', 'score': 1.651869615586584}, {'label': 'SIMILARITY', 'score': 2.8632091260689854}, {'label': 'SIMILARITY', 'score': 1.5085026416184253}, {'label': 'SIMILARITY', 'score': 3.118786011942754}, {'label': 'SIMILARITY', 'score': 3.1738812036974124}, {'label': 'SIMILARITY', 'score': 2.036070261758442}, {'label': 'SIMILARITY', 'score': 2.7528263810990383}, {'label': 'SIMILARITY', 'score': 4.1806625683754435}, {'label': 'SIMILARITY', 'score': 1.2320985441052705}, {'label': 'SIMILARITY', 'score': 1.2957484218034554}, {'label': 'SIMILARITY', 'score': 3.3478988054886796}, {'label': 'SIMILARITY', 'score': 2.264673684756741}, {'label': 'SIMILARITY', 'score': 2.5168742515180464}, {'label': 'SIMILARITY', 'score': 3.03755046308862}, {'label': 'SIMILARITY', 'score': 3.0524205429172473}, {'label': 'SIMILARITY', 'score': 2.9403379760568096}, {'label': 'SIMILARITY', 'score': 2.871683655514791}, {'label': 'SIMILARITY', 'score': 2.0827360931317638}, {'label': 'SIMILARITY', 'score': 1.8351070701707577}, {'label': 'SIMILARITY', 'score': 1.0263297873540853}, {'label': 'SIMILARITY', 'score': 4.129740008998624}, {'label': 'SIMILARITY', 'score': 1.8084622494830973}, {'label': 'SIMILARITY', 'score': 2.8217377889373774}, {'label': 'SIMILARITY', 'score': 4.146795569336921}, {'label': 'SIMILARITY', 'score': 2.8372733857675}, {'label': 'SIMILARITY', 'score': 4.059118799415456}, {'label': 'SIMILARITY', 'score': 3.0723758025459795}, {'label': 'SIMILARITY', 'score': 4.143778218092622}, {'label': 'SIMILARITY', 'score': 2.6453027428007974}, {'label': 'SIMILARITY', 'score': 4.229673805184637}, {'label': 'SIMILARITY', 'score': 4.228380733086852}, {'label': 'SIMILARITY', 'score': 1.640688083980907}, {'label': 'SIMILARITY', 'score': 2.1972157028983883}, {'label': 'SIMILARITY', 'score': 3.099826220316177}, {'label': 'SIMILARITY', 'score': 3.2209319608525147}, {'label': 'SIMILARITY', 'score': 0.7663876843057055}, {'label': 'SIMILARITY', 'score': 1.9594957809155105}, {'label': 'SIMILARITY', 'score': 3.132086715480112}, {'label': 'SIMILARITY', 'score': 1.572129910105391}, {'label': 'SIMILARITY', 'score': 2.635405792931488}, {'label': 'SIMILARITY', 'score': 2.7638791842344297}, {'label': 'SIMILARITY', 'score': 1.3088853470286295}, {'label': 'SIMILARITY', 'score': 2.3816104191587297}, {'label': 'SIMILARITY', 'score': 3.5452368287572056}, {'label': 'SIMILARITY', 'score': 3.313915353448332}, {'label': 'SIMILARITY', 'score': 2.2461106340221577}, {'label': 'SIMILARITY', 'score': 0.4022297195265531}, {'label': 'SIMILARITY', 'score': 2.80970516326548}, {'label': 'SIMILARITY', 'score': 2.9112490127689985}, {'label': 'SIMILARITY', 'score': 2.984432247604211}, {'label': 'SIMILARITY', 'score': 1.3321492823069252}, {'label': 'SIMILARITY', 'score': 1.2312225276076645}, {'label': 'SIMILARITY', 'score': 3.160572360243504}, {'label': 'SIMILARITY', 'score': 2.5560084401552845}, {'label': 'SIMILARITY', 'score': 2.81678862207351}, {'label': 'SIMILARITY', 'score': 1.0611997015151478}, {'label': 'SIMILARITY', 'score': 2.1393188085232318}, {'label': 'SIMILARITY', 'score': 2.942049584170683}, {'label': 'SIMILARITY', 'score': 2.9394406855822157}, {'label': 'SIMILARITY', 'score': 1.5315857292043482}, {'label': 'SIMILARITY', 'score': 2.570199026746082}, {'label': 'SIMILARITY', 'score': 2.7713072690742084}, {'label': 'SIMILARITY', 'score': 3.4346272811980345}, {'label': 'SIMILARITY', 'score': 2.0100698705269973}, {'label': 'SIMILARITY', 'score': 4.2013691069237264}, {'label': 'SIMILARITY', 'score': 1.8717140451123586}, {'label': 'SIMILARITY', 'score': 2.394114021017703}, {'label': 'SIMILARITY', 'score': 4.2109509682516695}, {'label': 'SIMILARITY', 'score': 3.6559267472340986}, {'label': 'SIMILARITY', 'score': 2.537581780809042}, {'label': 'SIMILARITY', 'score': 1.6939611855811272}, {'label': 'SIMILARITY', 'score': 1.450488958231045}, {'label': 'SIMILARITY', 'score': 3.4263966903970497}, {'label': 'SIMILARITY', 'score': 1.913825189573365}, {'label': 'SIMILARITY', 'score': 3.6174109020736145}, {'label': 'SIMILARITY', 'score': 4.229168176485003}, {'label': 'SIMILARITY', 'score': 3.3049256656679984}, {'label': 'SIMILARITY', 'score': 2.692563089895942}, {'label': 'SIMILARITY', 'score': 3.228079871492045}, {'label': 'SIMILARITY', 'score': 2.440754551253095}, {'label': 'SIMILARITY', 'score': 2.1092980653753135}, {'label': 'SIMILARITY', 'score': 2.903265841969788}, {'label': 'SIMILARITY', 'score': 2.4662612133112285}, {'label': 'SIMILARITY', 'score': 2.091977475644338}, {'label': 'SIMILARITY', 'score': 4.092082307836056}, {'label': 'SIMILARITY', 'score': 1.704500017335719}, {'label': 'SIMILARITY', 'score': 3.214508105024464}, {'label': 'SIMILARITY', 'score': 2.8700589757826926}, {'label': 'SIMILARITY', 'score': 2.974992797638109}, {'label': 'SIMILARITY', 'score': 4.198179864945577}, {'label': 'SIMILARITY', 'score': 2.1428625308935483}, {'label': 'SIMILARITY', 'score': 2.2372577963930076}, {'label': 'SIMILARITY', 'score': 1.5118939406010605}, {'label': 'SIMILARITY', 'score': 3.312270953679279}, {'label': 'SIMILARITY', 'score': 3.5276928850977005}, {'label': 'SIMILARITY', 'score': 4.080995599647323}, {'label': 'SIMILARITY', 'score': 1.7417548677908263}, {'label': 'SIMILARITY', 'score': 1.9153259030153231}, {'label': 'SIMILARITY', 'score': 2.213783867836392}, {'label': 'SIMILARITY', 'score': 4.214557923901399}, {'label': 'SIMILARITY', 'score': 2.0441171322987484}, {'label': 'SIMILARITY', 'score': 4.219992152420881}, {'label': 'SIMILARITY', 'score': 2.8858495802738804}, {'label': 'SIMILARITY', 'score': 1.7291024917240958}, {'label': 'SIMILARITY', 'score': 3.164178545977202}, {'label': 'SIMILARITY', 'score': 3.173469976808441}, {'label': 'SIMILARITY', 'score': 1.933746712586543}, {'label': 'SIMILARITY', 'score': 3.0770977178876064}, {'label': 'SIMILARITY', 'score': 2.087687643905302}, {'label': 'SIMILARITY', 'score': 2.83682199578123}, {'label': 'SIMILARITY', 'score': 4.22678663648792}, {'label': 'SIMILARITY', 'score': 4.205782334241574}, {'label': 'SIMILARITY', 'score': 3.2409221830839403}, {'label': 'SIMILARITY', 'score': 2.6564129046941636}, {'label': 'SIMILARITY', 'score': 2.9921580156875924}, {'label': 'SIMILARITY', 'score': 2.800613530251556}, {'label': 'SIMILARITY', 'score': 0.5038581481197284}, {'label': 'SIMILARITY', 'score': 1.7460476574900716}, {'label': 'SIMILARITY', 'score': 3.286596864499533}, {'label': 'SIMILARITY', 'score': 3.0091242296733003}, {'label': 'SIMILARITY', 'score': 2.9190530049314423}, {'label': 'SIMILARITY', 'score': 1.80701024024446}, {'label': 'SIMILARITY', 'score': 3.3373759383931576}, {'label': 'SIMILARITY', 'score': 1.5672632619394746}, {'label': 'SIMILARITY', 'score': 3.0918542063285397}, {'label': 'SIMILARITY', 'score': 1.3979198782971145}, {'label': 'SIMILARITY', 'score': 1.520881304631895}, {'label': 'SIMILARITY', 'score': 2.985693559101881}, {'label': 'SIMILARITY', 'score': 2.566757567487595}, {'label': 'SIMILARITY', 'score': 4.213814770412939}, {'label': 'SIMILARITY', 'score': 3.0034371744112622}, {'label': 'SIMILARITY', 'score': 2.9034315263489696}, {'label': 'SIMILARITY', 'score': 2.459516232270932}, {'label': 'SIMILARITY', 'score': 3.2172275565700432}, {'label': 'SIMILARITY', 'score': 1.532377057163061}, {'label': 'SIMILARITY', 'score': 2.3941109097216797}, {'label': 'SIMILARITY', 'score': 1.7902489290434058}, {'label': 'SIMILARITY', 'score': 2.6381237877163084}, {'label': 'SIMILARITY', 'score': 2.4106165610887986}, {'label': 'SIMILARITY', 'score': 3.6796678547732595}, {'label': 'SIMILARITY', 'score': 2.7801892427792105}, {'label': 'SIMILARITY', 'score': 2.5140150593543593}, {'label': 'SIMILARITY', 'score': 2.0392705110381937}, {'label': 'SIMILARITY', 'score': 1.4291220456604727}, {'label': 'SIMILARITY', 'score': 1.1553854649989794}, {'label': 'SIMILARITY', 'score': 2.30180675519254}, {'label': 'SIMILARITY', 'score': 2.9728510954078793}, {'label': 'SIMILARITY', 'score': 3.1760543203474896}, {'label': 'SIMILARITY', 'score': 1.2759631107723173}, {'label': 'SIMILARITY', 'score': 3.1489042210445066}, {'label': 'SIMILARITY', 'score': 2.5204080878449506}, {'label': 'SIMILARITY', 'score': 2.5560494636174975}, {'label': 'SIMILARITY', 'score': 2.7988337059246016}, {'label': 'SIMILARITY', 'score': 3.0716757423788392}, {'label': 'SIMILARITY', 'score': 3.6975428602103366}, {'label': 'SIMILARITY', 'score': 3.6450970324131324}, {'label': 'SIMILARITY', 'score': 1.443513244027849}, {'label': 'SIMILARITY', 'score': 1.7574139892345304}, {'label': 'SIMILARITY', 'score': 2.6049665744478205}, {'label': 'SIMILARITY', 'score': 2.773317884312862}, {'label': 'SIMILARITY', 'score': 1.7254965152467348}, {'label': 'SIMILARITY', 'score': 2.4598079459790974}, {'label': 'SIMILARITY', 'score': 3.9863905474083907}, {'label': 'SIMILARITY', 'score': 3.2971708579547587}, {'label': 'SIMILARITY', 'score': 2.161590121513415}, {'label': 'SIMILARITY', 'score': 2.4803548545526866}, {'label': 'SIMILARITY', 'score': 3.0236965290412474}, {'label': 'SIMILARITY', 'score': 1.9101483588927066}, {'label': 'SIMILARITY', 'score': 2.9913722710357113}, {'label': 'SIMILARITY', 'score': 1.9437391744941144}, {'label': 'SIMILARITY', 'score': 1.3810110357799548}, {'label': 'SIMILARITY', 'score': 1.4859968147807474}, {'label': 'SIMILARITY', 'score': 3.1202835347002207}, {'label': 'SIMILARITY', 'score': 3.1880690915792447}, {'label': 'SIMILARITY', 'score': 2.5317576064248852}, {'label': 'SIMILARITY', 'score': 1.978577949383262}, {'label': 'SIMILARITY', 'score': 4.126985645635536}, {'label': 'SIMILARITY', 'score': 2.655806244833704}, {'label': 'SIMILARITY', 'score': 3.0836023777948522}, {'label': 'SIMILARITY', 'score': 3.9385748555753715}, {'label': 'SIMILARITY', 'score': 3.7404621818175445}, {'label': 'SIMILARITY', 'score': 3.0963124168881144}, {'label': 'SIMILARITY', 'score': 0.583272814588352}, {'label': 'SIMILARITY', 'score': 2.423776359675736}, {'label': 'SIMILARITY', 'score': 2.992350608317846}, {'label': 'SIMILARITY', 'score': 2.1555848807897977}, {'label': 'SIMILARITY', 'score': 2.788451215110736}, {'label': 'SIMILARITY', 'score': 2.963532387252444}, {'label': 'SIMILARITY', 'score': 2.4938189242146382}, {'label': 'SIMILARITY', 'score': 2.4598342448223276}, {'label': 'SIMILARITY', 'score': 1.53093909790328}, {'label': 'SIMILARITY', 'score': 2.3934880350649075}, {'label': 'SIMILARITY', 'score': 3.071827830147404}, {'label': 'SIMILARITY', 'score': 1.8728389008479969}, {'label': 'SIMILARITY', 'score': 0.27437341759919387}, {'label': 'SIMILARITY', 'score': 2.1803525473680727}, {'label': 'SIMILARITY', 'score': 4.201090319983246}, {'label': 'SIMILARITY', 'score': 1.4732320694862793}, {'label': 'SIMILARITY', 'score': 1.8058085173904337}, {'label': 'SIMILARITY', 'score': 1.885932146710877}, {'label': 'SIMILARITY', 'score': 2.1267004133503695}, {'label': 'SIMILARITY', 'score': 1.3847048671712126}, {'label': 'SIMILARITY', 'score': 3.689000673107928}, {'label': 'SIMILARITY', 'score': 2.8596759600788557}, {'label': 'SIMILARITY', 'score': 3.274991029838873}, {'label': 'SIMILARITY', 'score': 3.0085058066537074}, {'label': 'SIMILARITY', 'score': 2.1163454051533157}, {'label': 'SIMILARITY', 'score': 2.8633013027395955}, {'label': 'SIMILARITY', 'score': 2.8342015882493565}, {'label': 'SIMILARITY', 'score': 3.2237793133772112}, {'label': 'SIMILARITY', 'score': 2.106809191505433}, {'label': 'SIMILARITY', 'score': 1.5022161222540622}, {'label': 'SIMILARITY', 'score': 2.9858264124157996}, {'label': 'SIMILARITY', 'score': 3.5188494449678545}, {'label': 'SIMILARITY', 'score': 0.6860419093768225}, {'label': 'SIMILARITY', 'score': 3.2256476573666353}, {'label': 'SIMILARITY', 'score': 4.228309177178264}, {'label': 'SIMILARITY', 'score': 4.19250951414441}, {'label': 'SIMILARITY', 'score': 3.086681504918212}, {'label': 'SIMILARITY', 'score': 3.0624729264066364}, {'label': 'SIMILARITY', 'score': 0.3571664924157631}, {'label': 'SIMILARITY', 'score': 3.284395819596529}, {'label': 'SIMILARITY', 'score': 2.7638984150841748}, {'label': 'SIMILARITY', 'score': 2.9656139311859335}, {'label': 'SIMILARITY', 'score': 3.7356086738274725}, {'label': 'SIMILARITY', 'score': 4.229214515445745}, {'label': 'SIMILARITY', 'score': 2.8433861025705705}, {'label': 'SIMILARITY', 'score': 4.180329060981676}, {'label': 'SIMILARITY', 'score': 1.9135145024035232}, {'label': 'SIMILARITY', 'score': 4.228347059098615}, {'label': 'SIMILARITY', 'score': 1.5362027413694244}, {'label': 'SIMILARITY', 'score': 1.4132070111942352}, {'label': 'SIMILARITY', 'score': 1.6403581207525495}, {'label': 'SIMILARITY', 'score': 2.4434499729399985}, {'label': 'SIMILARITY', 'score': 4.226429459996051}, {'label': 'SIMILARITY', 'score': 3.1965033247883654}, {'label': 'SIMILARITY', 'score': 2.162742934054114}, {'label': 'SIMILARITY', 'score': 2.9795023446324804}, {'label': 'SIMILARITY', 'score': 2.6476407101603083}, {'label': 'SIMILARITY', 'score': 1.2158042721429376}, {'label': 'SIMILARITY', 'score': 4.221550667780023}, {'label': 'SIMILARITY', 'score': 1.9536475253455343}, {'label': 'SIMILARITY', 'score': 3.4577646814912466}, {'label': 'SIMILARITY', 'score': 2.9828111865291462}, {'label': 'SIMILARITY', 'score': 2.056740698156049}, {'label': 'SIMILARITY', 'score': 3.289117124286104}, {'label': 'SIMILARITY', 'score': 2.824101661700424}, {'label': 'SIMILARITY', 'score': 1.5818898856173353}, {'label': 'SIMILARITY', 'score': 1.867759652932041}, {'label': 'SIMILARITY', 'score': 1.8354581477799998}, {'label': 'SIMILARITY', 'score': 2.7490903077361035}, {'label': 'SIMILARITY', 'score': 4.216133188253521}, {'label': 'SIMILARITY', 'score': 3.1801196652609915}, {'label': 'SIMILARITY', 'score': 3.2899361455810605}, {'label': 'SIMILARITY', 'score': 3.2128303381647205}, {'label': 'SIMILARITY', 'score': 2.7434739654649887}, {'label': 'SIMILARITY', 'score': 3.042655355013823}, {'label': 'SIMILARITY', 'score': 2.4378731808745564}, {'label': 'SIMILARITY', 'score': 4.225383855520518}, {'label': 'SIMILARITY', 'score': 3.7022916328832305}, {'label': 'SIMILARITY', 'score': 1.952100500492617}, {'label': 'SIMILARITY', 'score': 2.8481660918916325}, {'label': 'SIMILARITY', 'score': 3.1521550823489197}, {'label': 'SIMILARITY', 'score': 1.9510179956170803}, {'label': 'SIMILARITY', 'score': 2.253177569281694}, {'label': 'SIMILARITY', 'score': 3.7337666060858856}, {'label': 'SIMILARITY', 'score': 1.7381379538955632}, {'label': 'SIMILARITY', 'score': 2.7724133118946934}, {'label': 'SIMILARITY', 'score': 1.6421578171353095}, {'label': 'SIMILARITY', 'score': 2.734367484029553}, {'label': 'SIMILARITY', 'score': 2.0496670147201166}, {'label': 'SIMILARITY', 'score': 2.388607990629304}, {'label': 'SIMILARITY', 'score': 4.202825764803667}, {'label': 'SIMILARITY', 'score': 2.8712793211582937}, {'label': 'SIMILARITY', 'score': 2.4772023795853952}, {'label': 'SIMILARITY', 'score': 2.1173756932849703}, {'label': 'SIMILARITY', 'score': 1.6187660775273702}, {'label': 'SIMILARITY', 'score': 1.9826419774907291}, {'label': 'SIMILARITY', 'score': 2.807401231581593}, {'label': 'SIMILARITY', 'score': 2.328320354164411}, {'label': 'SIMILARITY', 'score': 2.287342658248627}, {'label': 'SIMILARITY', 'score': 2.656915780266112}, {'label': 'SIMILARITY', 'score': 2.3285199900356384}, {'label': 'SIMILARITY', 'score': 3.2527641002704306}, {'label': 'SIMILARITY', 'score': 2.4196858863217985}, {'label': 'SIMILARITY', 'score': 3.484349247112656}, {'label': 'SIMILARITY', 'score': 2.978443426881}, {'label': 'SIMILARITY', 'score': 1.9126455974829242}, {'label': 'SIMILARITY', 'score': 2.0699265348980918}, {'label': 'SIMILARITY', 'score': 2.1006642817293506}, {'label': 'SIMILARITY', 'score': 2.4510516621728335}, {'label': 'SIMILARITY', 'score': 3.2032054450171583}, {'label': 'SIMILARITY', 'score': 2.589246749384223}, {'label': 'SIMILARITY', 'score': 2.462269411605626}, {'label': 'SIMILARITY', 'score': 2.8100868274151316}, {'label': 'SIMILARITY', 'score': 3.335709642254214}, {'label': 'SIMILARITY', 'score': 4.211327656607019}, {'label': 'SIMILARITY', 'score': 2.4052361875598693}, {'label': 'SIMILARITY', 'score': 2.6787253842410856}, {'label': 'SIMILARITY', 'score': 2.706443055210308}, {'label': 'SIMILARITY', 'score': 2.9367606282751795}, {'label': 'SIMILARITY', 'score': 1.9900430200478845}, {'label': 'SIMILARITY', 'score': 2.833275320569283}, {'label': 'SIMILARITY', 'score': 2.4320381215951206}, {'label': 'SIMILARITY', 'score': 2.8873674069459336}, {'label': 'SIMILARITY', 'score': 0.8420838545702727}, {'label': 'SIMILARITY', 'score': 2.472086966713493}, {'label': 'SIMILARITY', 'score': 3.052586593219088}, {'label': 'SIMILARITY', 'score': 3.003781332823669}, {'label': 'SIMILARITY', 'score': 4.22629922645644}, {'label': 'SIMILARITY', 'score': 3.7530479233772023}, {'label': 'SIMILARITY', 'score': 3.3980126161401203}, {'label': 'SIMILARITY', 'score': 3.4833258861992302}, {'label': 'SIMILARITY', 'score': 3.7597556300336206}, {'label': 'SIMILARITY', 'score': 3.5017078881819867}, {'label': 'SIMILARITY', 'score': 1.8946499698672221}, {'label': 'SIMILARITY', 'score': 3.0773326496446067}, {'label': 'SIMILARITY', 'score': 2.100001186094684}, {'label': 'SIMILARITY', 'score': 1.8214170260301572}, {'label': 'SIMILARITY', 'score': 3.47071815893391}, {'label': 'SIMILARITY', 'score': 2.841641612949123}, {'label': 'SIMILARITY', 'score': 4.229332478576151}, {'label': 'SIMILARITY', 'score': 2.2784131844276683}, {'label': 'SIMILARITY', 'score': 3.3014317840560725}, {'label': 'SIMILARITY', 'score': 3.6569614343483217}, {'label': 'SIMILARITY', 'score': 2.9020149329806117}, {'label': 'SIMILARITY', 'score': 2.9496093666991263}, {'label': 'SIMILARITY', 'score': 2.835827525579301}, {'label': 'SIMILARITY', 'score': 2.5917489897037065}, {'label': 'SIMILARITY', 'score': 1.760787490904456}, {'label': 'SIMILARITY', 'score': 1.6429033260745773}, {'label': 'SIMILARITY', 'score': 1.6843649665593783}, {'label': 'SIMILARITY', 'score': 3.1068107937664733}, {'label': 'SIMILARITY', 'score': 2.3408740957913414}, {'label': 'SIMILARITY', 'score': 0.9801058609500465}, {'label': 'SIMILARITY', 'score': 0.7149070479345434}, {'label': 'SIMILARITY', 'score': 4.2239117320919615}, {'label': 'SIMILARITY', 'score': 2.9963554816748026}, {'label': 'SIMILARITY', 'score': 3.026652624639244}, {'label': 'SIMILARITY', 'score': 1.5732968349391887}, {'label': 'SIMILARITY', 'score': 4.228431246134341}, {'label': 'SIMILARITY', 'score': 4.032336827903548}, {'label': 'SIMILARITY', 'score': 2.1819707841847538}, {'label': 'SIMILARITY', 'score': 2.66022545120832}, {'label': 'SIMILARITY', 'score': 3.397563447673433}, {'label': 'SIMILARITY', 'score': 2.1320678735938956}, {'label': 'SIMILARITY', 'score': 2.6779165783298433}, {'label': 'SIMILARITY', 'score': 3.0281363584297116}, {'label': 'SIMILARITY', 'score': 2.5823284637120016}, {'label': 'SIMILARITY', 'score': 3.3589149370684326}, {'label': 'SIMILARITY', 'score': 2.939661816674804}, {'label': 'SIMILARITY', 'score': 2.1143505819415687}, {'label': 'SIMILARITY', 'score': 0.7585168732431353}, {'label': 'SIMILARITY', 'score': 4.222211555021264}, {'label': 'SIMILARITY', 'score': 3.4048456889775243}, {'label': 'SIMILARITY', 'score': 3.5503053954489685}, {'label': 'SIMILARITY', 'score': 1.4650617076025332}, {'label': 'SIMILARITY', 'score': 2.848699227502611}, {'label': 'SIMILARITY', 'score': 3.1768339237737027}, {'label': 'SIMILARITY', 'score': 4.188695233366358}, {'label': 'SIMILARITY', 'score': 1.3238336768341024}, {'label': 'SIMILARITY', 'score': 2.7135504793899385}, {'label': 'SIMILARITY', 'score': 2.881903828506379}, {'label': 'SIMILARITY', 'score': 1.6690233523164926}, {'label': 'SIMILARITY', 'score': 2.8705110180228957}, {'label': 'SIMILARITY', 'score': 2.945267227683559}, {'label': 'SIMILARITY', 'score': 3.268012793924745}, {'label': 'SIMILARITY', 'score': 3.045040086768937}, {'label': 'SIMILARITY', 'score': 2.0400687330725678}, {'label': 'SIMILARITY', 'score': 4.1089197699232685}, {'label': 'SIMILARITY', 'score': 1.9717390572596238}, {'label': 'SIMILARITY', 'score': 3.199614851911162}, {'label': 'SIMILARITY', 'score': 4.162718862851803}, {'label': 'SIMILARITY', 'score': 1.9350747325455304}, {'label': 'SIMILARITY', 'score': 2.8426698812670965}, {'label': 'SIMILARITY', 'score': 3.2228875991309285}, {'label': 'SIMILARITY', 'score': 2.956521766743539}, {'label': 'SIMILARITY', 'score': 2.697699851453724}, {'label': 'SIMILARITY', 'score': 2.991267547256921}, {'label': 'SIMILARITY', 'score': 2.3414711367135577}, {'label': 'SIMILARITY', 'score': 2.400758669523344}, {'label': 'SIMILARITY', 'score': 4.226404252218907}, {'label': 'SIMILARITY', 'score': 2.932463332209097}, {'label': 'SIMILARITY', 'score': 3.0360376420816206}, {'label': 'SIMILARITY', 'score': 2.7591295427679596}, {'label': 'SIMILARITY', 'score': 3.863877941192224}, {'label': 'SIMILARITY', 'score': 2.2003468570144293}, {'label': 'SIMILARITY', 'score': 2.6983472268514026}, {'label': 'SIMILARITY', 'score': 2.306338619426483}, {'label': 'SIMILARITY', 'score': 1.8585296722542137}, {'label': 'SIMILARITY', 'score': 2.845462858746485}, {'label': 'SIMILARITY', 'score': 3.117027794089612}, {'label': 'SIMILARITY', 'score': 4.2271229158209715}, {'label': 'SIMILARITY', 'score': 1.6256606867739714}, {'label': 'SIMILARITY', 'score': 2.934059664463582}, {'label': 'SIMILARITY', 'score': 3.200983711754559}, {'label': 'SIMILARITY', 'score': 1.7496044303338367}, {'label': 'SIMILARITY', 'score': 1.318372504101822}, {'label': 'SIMILARITY', 'score': 2.7581256559866127}, {'label': 'SIMILARITY', 'score': 0.7920408694421555}, {'label': 'SIMILARITY', 'score': 1.3510702732417834}, {'label': 'SIMILARITY', 'score': 3.0515851370645026}, {'label': 'SIMILARITY', 'score': 1.6404737967030765}, {'label': 'SIMILARITY', 'score': 2.4279448978066327}, {'label': 'SIMILARITY', 'score': 4.22374829266195}, {'label': 'SIMILARITY', 'score': 3.097511265680129}, {'label': 'SIMILARITY', 'score': 3.0438354050842604}, {'label': 'SIMILARITY', 'score': 2.9903974145046677}, {'label': 'SIMILARITY', 'score': 4.225677693278653}, {'label': 'SIMILARITY', 'score': 3.13810459945878}, {'label': 'SIMILARITY', 'score': 2.8442507599796523}, {'label': 'SIMILARITY', 'score': 2.854575857863823}, {'label': 'SIMILARITY', 'score': 2.688924894055085}, {'label': 'SIMILARITY', 'score': 2.7850564531909803}, {'label': 'SIMILARITY', 'score': 2.9045363143866765}, {'label': 'SIMILARITY', 'score': 1.4204986941567583}, {'label': 'SIMILARITY', 'score': 1.8351959612052422}, {'label': 'SIMILARITY', 'score': 2.044181727552023}, {'label': 'SIMILARITY', 'score': 2.91151212536389}, {'label': 'SIMILARITY', 'score': 3.017329443106786}, {'label': 'SIMILARITY', 'score': 3.9057076686338474}, {'label': 'SIMILARITY', 'score': 2.4575558048312836}, {'label': 'SIMILARITY', 'score': 1.8706715443998922}, {'label': 'SIMILARITY', 'score': 1.9190543148483499}, {'label': 'SIMILARITY', 'score': 2.3289893739463348}, {'label': 'SIMILARITY', 'score': 1.9447395509491203}, {'label': 'SIMILARITY', 'score': 2.7316132309543693}, {'label': 'SIMILARITY', 'score': 2.6829561154376695}, {'label': 'SIMILARITY', 'score': 3.345541588891716}, {'label': 'SIMILARITY', 'score': 0.8788336193439932}, {'label': 'SIMILARITY', 'score': 3.2332645021613216}, {'label': 'SIMILARITY', 'score': 2.8652200641485415}, {'label': 'SIMILARITY', 'score': 1.3439022599585424}, {'label': 'SIMILARITY', 'score': 0.9954229912269812}, {'label': 'SIMILARITY', 'score': 2.0842187063009905}, {'label': 'SIMILARITY', 'score': 2.505767693303969}, {'label': 'SIMILARITY', 'score': 4.171735356092364}, {'label': 'SIMILARITY', 'score': 4.1527590715974005}, {'label': 'SIMILARITY', 'score': 2.6928712287908154}, {'label': 'SIMILARITY', 'score': 2.3624362088752213}, {'label': 'SIMILARITY', 'score': 3.006339405901469}, {'label': 'SIMILARITY', 'score': 2.404342326090762}, {'label': 'SIMILARITY', 'score': 1.7020017105875225}, {'label': 'SIMILARITY', 'score': 2.9124505785267885}, {'label': 'SIMILARITY', 'score': 2.8650692938911697}, {'label': 'SIMILARITY', 'score': 3.0404235839553375}, {'label': 'SIMILARITY', 'score': 1.7803740760362319}, {'label': 'SIMILARITY', 'score': 1.625294140606527}, {'label': 'SIMILARITY', 'score': 4.227425661154405}, {'label': 'SIMILARITY', 'score': 2.8495530018333852}, {'label': 'SIMILARITY', 'score': 2.5490801211430067}, {'label': 'SIMILARITY', 'score': 2.947730009738462}, {'label': 'SIMILARITY', 'score': 3.2068796894543086}, {'label': 'SIMILARITY', 'score': 2.9642641239334337}, {'label': 'SIMILARITY', 'score': 1.8952916540332576}, {'label': 'SIMILARITY', 'score': 2.8937411402993285}, {'label': 'SIMILARITY', 'score': 4.222575642441926}, {'label': 'SIMILARITY', 'score': 3.051752444011898}, {'label': 'SIMILARITY', 'score': 3.619134952907555}, {'label': 'SIMILARITY', 'score': 0.8150621956757004}, {'label': 'SIMILARITY', 'score': 1.6133769893961507}, {'label': 'SIMILARITY', 'score': 2.8016635518236543}, {'label': 'SIMILARITY', 'score': 2.9359758015138424}, {'label': 'SIMILARITY', 'score': 2.9442429912836863}, {'label': 'SIMILARITY', 'score': 3.2343887767010036}, {'label': 'SIMILARITY', 'score': 2.54216880752728}, {'label': 'SIMILARITY', 'score': 2.6264700641869023}, {'label': 'SIMILARITY', 'score': 1.0988370505450546}, {'label': 'SIMILARITY', 'score': 2.5326043457005576}, {'label': 'SIMILARITY', 'score': 2.4510655307425053}, {'label': 'SIMILARITY', 'score': 1.7838146318236396}, {'label': 'SIMILARITY', 'score': 2.7455861748316788}, {'label': 'SIMILARITY', 'score': 1.9153429480391804}, {'label': 'SIMILARITY', 'score': 3.0387250485390367}, {'label': 'SIMILARITY', 'score': 2.4813118618224785}, {'label': 'SIMILARITY', 'score': 3.4791970679559454}, {'label': 'SIMILARITY', 'score': 4.144651061548484}, {'label': 'SIMILARITY', 'score': 2.467783432208337}, {'label': 'SIMILARITY', 'score': 3.0348768131162296}, {'label': 'SIMILARITY', 'score': 2.4071068990824163}, {'label': 'SIMILARITY', 'score': 3.0374071402218594}, {'label': 'SIMILARITY', 'score': 2.0340555255577333}, {'label': 'SIMILARITY', 'score': 3.2429274978453013}, {'label': 'SIMILARITY', 'score': 2.9608114504691185}, {'label': 'SIMILARITY', 'score': 3.32864008053628}, {'label': 'SIMILARITY', 'score': 3.3287364390816965}, {'label': 'SIMILARITY', 'score': 3.260742598901819}, {'label': 'SIMILARITY', 'score': 2.9295895024058645}, {'label': 'SIMILARITY', 'score': 3.208131678028741}, {'label': 'SIMILARITY', 'score': 3.2729050145978147}, {'label': 'SIMILARITY', 'score': 2.8356373891711506}, {'label': 'SIMILARITY', 'score': 3.201906765461919}, {'label': 'SIMILARITY', 'score': 1.8802936010479676}, {'label': 'SIMILARITY', 'score': 1.9569359941917523}, {'label': 'SIMILARITY', 'score': 2.8302597259448485}, {'label': 'SIMILARITY', 'score': 3.125979010588841}, {'label': 'SIMILARITY', 'score': 2.6181935477243874}, {'label': 'SIMILARITY', 'score': 3.20976062833578}, {'label': 'SIMILARITY', 'score': 3.150928059364242}, {'label': 'SIMILARITY', 'score': 2.768923481476384}, {'label': 'SIMILARITY', 'score': 3.7056968032453086}, {'label': 'SIMILARITY', 'score': 2.021846248082909}, {'label': 'SIMILARITY', 'score': 4.196668527813787}, {'label': 'SIMILARITY', 'score': 3.553345442029783}, {'label': 'SIMILARITY', 'score': 1.7299517025025237}, {'label': 'SIMILARITY', 'score': 2.5770753505386312}, {'label': 'SIMILARITY', 'score': 2.609718939295725}, {'label': 'SIMILARITY', 'score': 2.479320083746472}, {'label': 'SIMILARITY', 'score': 1.584883431673738}, {'label': 'SIMILARITY', 'score': 3.434142757031424}, {'label': 'SIMILARITY', 'score': 2.0613193747001266}, {'label': 'SIMILARITY', 'score': 1.9204539365656026}, {'label': 'SIMILARITY', 'score': 1.847818275063934}, {'label': 'SIMILARITY', 'score': 3.231714005710603}, {'label': 'SIMILARITY', 'score': 2.535330469565657}, {'label': 'SIMILARITY', 'score': 2.828818319546671}, {'label': 'SIMILARITY', 'score': 2.9482851723217247}, {'label': 'SIMILARITY', 'score': 2.8112994397930446}, {'label': 'SIMILARITY', 'score': 2.9738544222280825}, {'label': 'SIMILARITY', 'score': 1.76210378025411}, {'label': 'SIMILARITY', 'score': 3.0705751427725207}, {'label': 'SIMILARITY', 'score': 4.2066433482568195}, {'label': 'SIMILARITY', 'score': 1.931673574839772}, {'label': 'SIMILARITY', 'score': 1.9572855152658692}, {'label': 'SIMILARITY', 'score': 1.4202705156685276}, {'label': 'SIMILARITY', 'score': 2.3199665547161277}, {'label': 'SIMILARITY', 'score': 2.941670250674881}, {'label': 'SIMILARITY', 'score': 2.901236089450833}, {'label': 'SIMILARITY', 'score': 2.6022640318532786}, {'label': 'SIMILARITY', 'score': 1.7794043923022156}, {'label': 'SIMILARITY', 'score': 2.328259217630623}, {'label': 'SIMILARITY', 'score': 4.228111372013784}, {'label': 'SIMILARITY', 'score': 2.7614491063615154}, {'label': 'SIMILARITY', 'score': 2.891216472865493}, {'label': 'SIMILARITY', 'score': 2.4531313064931064}, {'label': 'SIMILARITY', 'score': 1.218569239243021}, {'label': 'SIMILARITY', 'score': 1.6067044074372747}, {'label': 'SIMILARITY', 'score': 2.9980149719171383}, {'label': 'SIMILARITY', 'score': 2.2433947901213807}, {'label': 'SIMILARITY', 'score': 2.083799599226641}, {'label': 'SIMILARITY', 'score': 3.6033747618612275}, {'label': 'SIMILARITY', 'score': 1.9036180680633412}, {'label': 'SIMILARITY', 'score': 2.9971605693608625}, {'label': 'SIMILARITY', 'score': 2.6411648512905237}, {'label': 'SIMILARITY', 'score': 1.8396962043401504}, {'label': 'SIMILARITY', 'score': 3.075693423098079}, {'label': 'SIMILARITY', 'score': 4.160615926951101}, {'label': 'SIMILARITY', 'score': 2.0948449531058495}, {'label': 'SIMILARITY', 'score': 3.077389267232347}, {'label': 'SIMILARITY', 'score': 2.2749848697238817}, {'label': 'SIMILARITY', 'score': 1.8656111244079292}, {'label': 'SIMILARITY', 'score': 3.242036336560086}, {'label': 'SIMILARITY', 'score': 3.393937050739828}, {'label': 'SIMILARITY', 'score': 2.5248553629883337}, {'label': 'SIMILARITY', 'score': 3.1644513700343624}, {'label': 'SIMILARITY', 'score': 1.7567960917046934}, {'label': 'SIMILARITY', 'score': 4.23086301101293}, {'label': 'SIMILARITY', 'score': 2.4543545003813763}, {'label': 'SIMILARITY', 'score': 2.182417719932124}, {'label': 'SIMILARITY', 'score': 3.295627960949742}, {'label': 'SIMILARITY', 'score': 3.4958949806662263}, {'label': 'SIMILARITY', 'score': 1.8133814740968996}, {'label': 'SIMILARITY', 'score': 2.9066254514977454}, {'label': 'SIMILARITY', 'score': 2.9602130772656627}, {'label': 'SIMILARITY', 'score': 2.837208401801777}, {'label': 'SIMILARITY', 'score': 2.9610508901621553}, {'label': 'SIMILARITY', 'score': 2.4250384039013926}, {'label': 'SIMILARITY', 'score': 2.5680561604701735}, {'label': 'SIMILARITY', 'score': 2.483392509740989}, {'label': 'SIMILARITY', 'score': 4.222454265664663}, {'label': 'SIMILARITY', 'score': 2.987938577356085}, {'label': 'SIMILARITY', 'score': 2.0756512031598358}, {'label': 'SIMILARITY', 'score': 1.6238245237073552}, {'label': 'SIMILARITY', 'score': 2.731756549824859}, {'label': 'SIMILARITY', 'score': 3.016581840050175}, {'label': 'SIMILARITY', 'score': 3.0427074700412273}, {'label': 'SIMILARITY', 'score': 2.180572112868481}, {'label': 'SIMILARITY', 'score': 2.860760469986587}, {'label': 'SIMILARITY', 'score': 2.46447936969561}, {'label': 'SIMILARITY', 'score': 2.4135506083465765}, {'label': 'SIMILARITY', 'score': 2.7788763698297014}, {'label': 'SIMILARITY', 'score': 1.7458101642351165}, {'label': 'SIMILARITY', 'score': 3.243478045668779}, {'label': 'SIMILARITY', 'score': 1.6594528588832609}, {'label': 'SIMILARITY', 'score': 2.8722904308771082}, {'label': 'SIMILARITY', 'score': 2.9595870407061398}, {'label': 'SIMILARITY', 'score': 1.9603146995745615}, {'label': 'SIMILARITY', 'score': 1.7801041964149957}, {'label': 'SIMILARITY', 'score': 4.096114205933234}, {'label': 'SIMILARITY', 'score': 1.805118843562925}, {'label': 'SIMILARITY', 'score': 2.8947475663821227}, {'label': 'SIMILARITY', 'score': 1.4727115042802743}, {'label': 'SIMILARITY', 'score': 2.297759203035799}, {'label': 'SIMILARITY', 'score': 2.020989888435087}, {'label': 'SIMILARITY', 'score': 4.222420784933289}, {'label': 'SIMILARITY', 'score': 1.1749835957685313}, {'label': 'SIMILARITY', 'score': 3.131946847456983}, {'label': 'SIMILARITY', 'score': 1.3724336586352714}, {'label': 'SIMILARITY', 'score': 2.949386247158979}, {'label': 'SIMILARITY', 'score': 4.203487069289574}, {'label': 'SIMILARITY', 'score': 4.068676985291318}, {'label': 'SIMILARITY', 'score': 1.5888338120844998}, {'label': 'SIMILARITY', 'score': 2.9441727351595137}, {'label': 'SIMILARITY', 'score': 1.9846425024728889}, {'label': 'SIMILARITY', 'score': 2.9238098095046348}, {'label': 'SIMILARITY', 'score': 1.9002475292606436}, {'label': 'SIMILARITY', 'score': 3.2854786382103045}, {'label': 'SIMILARITY', 'score': 4.22744248294999}, {'label': 'SIMILARITY', 'score': 2.4583624062024403}, {'label': 'SIMILARITY', 'score': 3.107287816123369}, {'label': 'SIMILARITY', 'score': 1.718624597787437}, {'label': 'SIMILARITY', 'score': 3.039372972575258}, {'label': 'SIMILARITY', 'score': 1.362271883675236}, {'label': 'SIMILARITY', 'score': 1.5558502643454826}, {'label': 'SIMILARITY', 'score': 2.9924816428885346}, {'label': 'SIMILARITY', 'score': 2.1397458097595976}, {'label': 'SIMILARITY', 'score': 3.19094914532495}, {'label': 'SIMILARITY', 'score': 0.5618130186177763}, {'label': 'SIMILARITY', 'score': 3.7025682085940614}, {'label': 'SIMILARITY', 'score': 1.561080085665873}, {'label': 'SIMILARITY', 'score': 2.5920340945228624}, {'label': 'SIMILARITY', 'score': 0.9311149421018886}, {'label': 'SIMILARITY', 'score': 4.222655172915964}, {'label': 'SIMILARITY', 'score': 3.2424217025115647}, {'label': 'SIMILARITY', 'score': 3.8234251086433306}, {'label': 'SIMILARITY', 'score': 2.693988048629551}, {'label': 'SIMILARITY', 'score': 1.9853474459150895}, {'label': 'SIMILARITY', 'score': 3.1189374361637894}, {'label': 'SIMILARITY', 'score': 3.2586036564651204}, {'label': 'SIMILARITY', 'score': 2.87678443966231}, {'label': 'SIMILARITY', 'score': 1.958518365788638}, {'label': 'SIMILARITY', 'score': 1.9550767246874374}, {'label': 'SIMILARITY', 'score': 2.692780886322404}, {'label': 'SIMILARITY', 'score': 3.275094587696003}, {'label': 'SIMILARITY', 'score': 1.2786912750031796}, {'label': 'SIMILARITY', 'score': 3.6833217771467264}, {'label': 'SIMILARITY', 'score': 1.3085240761703183}, {'label': 'SIMILARITY', 'score': 3.1117861170816816}, {'label': 'SIMILARITY', 'score': 2.464802016764592}, {'label': 'SIMILARITY', 'score': 2.844972581768081}, {'label': 'SIMILARITY', 'score': 2.4428024825316883}, {'label': 'SIMILARITY', 'score': 2.8234899560339697}, {'label': 'SIMILARITY', 'score': 1.5213090435035932}, {'label': 'SIMILARITY', 'score': 3.838739788399678}, {'label': 'SIMILARITY', 'score': 2.7417375623057065}, {'label': 'SIMILARITY', 'score': 4.1794415729816405}, {'label': 'SIMILARITY', 'score': 2.699479501318729}, {'label': 'SIMILARITY', 'score': 2.6822268680361407}, {'label': 'SIMILARITY', 'score': 2.782681603083019}, {'label': 'SIMILARITY', 'score': 2.3578295003383234}, {'label': 'SIMILARITY', 'score': 0.15254493586338416}, {'label': 'SIMILARITY', 'score': 3.013997272280212}, {'label': 'SIMILARITY', 'score': 1.7449396946161744}, {'label': 'SIMILARITY', 'score': 1.9896408030544654}, {'label': 'SIMILARITY', 'score': 2.753356573520177}, {'label': 'SIMILARITY', 'score': 2.781241072231405}, {'label': 'SIMILARITY', 'score': 1.6825804542482166}, {'label': 'SIMILARITY', 'score': 3.007459931068033}, {'label': 'SIMILARITY', 'score': 0.8978623432291614}, {'label': 'SIMILARITY', 'score': 3.1208784059423196}, {'label': 'SIMILARITY', 'score': 2.7204463250108555}, {'label': 'SIMILARITY', 'score': 3.3623010687845905}, {'label': 'SIMILARITY', 'score': 2.780273786760196}, {'label': 'SIMILARITY', 'score': 2.585373586040401}, {'label': 'SIMILARITY', 'score': 3.9634122625250794}, {'label': 'SIMILARITY', 'score': 4.018441643275457}, {'label': 'SIMILARITY', 'score': 3.237247205634415}, {'label': 'SIMILARITY', 'score': 3.042518221999745}, {'label': 'SIMILARITY', 'score': 1.3984883200719875}, {'label': 'SIMILARITY', 'score': 3.7658189923777257}, {'label': 'SIMILARITY', 'score': 2.9225479532218883}, {'label': 'SIMILARITY', 'score': 2.9890588783015346}, {'label': 'SIMILARITY', 'score': 2.9545964896664385}, {'label': 'SIMILARITY', 'score': 2.887071781488231}, {'label': 'SIMILARITY', 'score': 3.2046233835054703}, {'label': 'SIMILARITY', 'score': 1.5277225848663267}, {'label': 'SIMILARITY', 'score': 3.267028243184639}, {'label': 'SIMILARITY', 'score': 1.5106674632310002}, {'label': 'SIMILARITY', 'score': 2.142969725411569}, {'label': 'SIMILARITY', 'score': 3.148951068656051}, {'label': 'SIMILARITY', 'score': 4.181333928392094}, {'label': 'SIMILARITY', 'score': 3.2372176987323504}, {'label': 'SIMILARITY', 'score': 2.3893739862130494}, {'label': 'SIMILARITY', 'score': 3.010484649249653}, {'label': 'SIMILARITY', 'score': 3.649153581216433}, {'label': 'SIMILARITY', 'score': 2.661284567656402}, {'label': 'SIMILARITY', 'score': 1.8292658812815163}, {'label': 'SIMILARITY', 'score': 2.4446599977191203}, {'label': 'SIMILARITY', 'score': 2.870877482223721}, {'label': 'SIMILARITY', 'score': 1.4583629065237533}, {'label': 'SIMILARITY', 'score': 2.876839946654002}, {'label': 'SIMILARITY', 'score': 1.6146907888253434}, {'label': 'SIMILARITY', 'score': 1.53975500727484}, {'label': 'SIMILARITY', 'score': 1.8020869526718142}, {'label': 'SIMILARITY', 'score': 3.101973803779282}, {'label': 'SIMILARITY', 'score': 2.9526214523485392}, {'label': 'SIMILARITY', 'score': 4.120937365648803}, {'label': 'SIMILARITY', 'score': 1.635156349089511}, {'label': 'SIMILARITY', 'score': 1.7761661823043313}, {'label': 'SIMILARITY', 'score': 2.945407882871541}, {'label': 'SIMILARITY', 'score': 2.7357611827549024}, {'label': 'SIMILARITY', 'score': 2.6330905593251885}, {'label': 'SIMILARITY', 'score': 2.3991665002366998}, {'label': 'SIMILARITY', 'score': 1.703402645576806}, {'label': 'SIMILARITY', 'score': 1.946498832291517}, {'label': 'SIMILARITY', 'score': 2.5191120066890744}, {'label': 'SIMILARITY', 'score': 1.9498270535236533}, {'label': 'SIMILARITY', 'score': 2.4194205173687067}, {'label': 'SIMILARITY', 'score': 0.2959056968357674}, {'label': 'SIMILARITY', 'score': 3.0049191424059996}, {'label': 'SIMILARITY', 'score': 3.1405297779639483}, {'label': 'SIMILARITY', 'score': 4.229067080536784}, {'label': 'SIMILARITY', 'score': 1.9184418044323541}, {'label': 'SIMILARITY', 'score': 3.1472703489439002}, {'label': 'SIMILARITY', 'score': 1.6783766606717907}, {'label': 'SIMILARITY', 'score': 3.5479598396601673}, {'label': 'SIMILARITY', 'score': 2.741700891545943}, {'label': 'SIMILARITY', 'score': 3.063878569998974}, {'label': 'SIMILARITY', 'score': 3.1917002181943905}, {'label': 'SIMILARITY', 'score': 1.302850038129019}, {'label': 'SIMILARITY', 'score': 2.572313282376279}, {'label': 'SIMILARITY', 'score': 2.16521311559745}, {'label': 'SIMILARITY', 'score': 2.719105109480672}, {'label': 'SIMILARITY', 'score': 3.015189808879737}, {'label': 'SIMILARITY', 'score': 3.2638900873145005}, {'label': 'SIMILARITY', 'score': 2.941797932643801}, {'label': 'SIMILARITY', 'score': 2.400333258872417}, {'label': 'SIMILARITY', 'score': 2.6634624793125456}, {'label': 'SIMILARITY', 'score': 1.9752304055519863}, {'label': 'SIMILARITY', 'score': 2.983701371186723}, {'label': 'SIMILARITY', 'score': 2.874112732062151}, {'label': 'SIMILARITY', 'score': 3.003021679864635}, {'label': 'SIMILARITY', 'score': 3.005079510174601}, {'label': 'SIMILARITY', 'score': 1.4344893348893344}, {'label': 'SIMILARITY', 'score': 1.3904619125321092}, {'label': 'SIMILARITY', 'score': 2.703706657359532}, {'label': 'SIMILARITY', 'score': 4.203696639386409}, {'label': 'SIMILARITY', 'score': 3.0089712540947633}, {'label': 'SIMILARITY', 'score': 1.7173408921834272}, {'label': 'SIMILARITY', 'score': 3.301684078709034}, {'label': 'SIMILARITY', 'score': 2.8638276717059052}, {'label': 'SIMILARITY', 'score': 3.099359660367943}, {'label': 'SIMILARITY', 'score': 3.155920511026692}, {'label': 'SIMILARITY', 'score': 3.2995266946045336}, {'label': 'SIMILARITY', 'score': 3.2703547082601974}, {'label': 'SIMILARITY', 'score': 1.8630676826475279}, {'label': 'SIMILARITY', 'score': 1.735121183063961}, {'label': 'SIMILARITY', 'score': 1.1042625288504382}, {'label': 'SIMILARITY', 'score': 3.1886222346202446}, {'label': 'SIMILARITY', 'score': 1.416903636100036}, {'label': 'SIMILARITY', 'score': 1.390203496385461}, {'label': 'SIMILARITY', 'score': 1.4642284417469005}, {'label': 'SIMILARITY', 'score': 2.5906303992664017}, {'label': 'SIMILARITY', 'score': 2.242487168963967}, {'label': 'SIMILARITY', 'score': 2.238394317104315}, {'label': 'SIMILARITY', 'score': 2.2820638462735303}, {'label': 'SIMILARITY', 'score': 1.9673904138672553}, {'label': 'SIMILARITY', 'score': 3.040617947541788}, {'label': 'SIMILARITY', 'score': 4.142371584692511}, {'label': 'SIMILARITY', 'score': 1.7836863722592164}, {'label': 'SIMILARITY', 'score': 2.7179347615117675}, {'label': 'SIMILARITY', 'score': 2.5659810839422486}, {'label': 'SIMILARITY', 'score': 2.9796266850315014}, {'label': 'SIMILARITY', 'score': 2.1809276813511844}, {'label': 'SIMILARITY', 'score': 1.842482383754187}, {'label': 'SIMILARITY', 'score': 3.607282952984014}, {'label': 'SIMILARITY', 'score': 4.043967493118697}, {'label': 'SIMILARITY', 'score': 3.814424245891948}, {'label': 'SIMILARITY', 'score': 2.2473918398685795}, {'label': 'SIMILARITY', 'score': 2.537037631899722}, {'label': 'SIMILARITY', 'score': 4.225106885272357}, {'label': 'SIMILARITY', 'score': 1.402967261395525}, {'label': 'SIMILARITY', 'score': 2.8597259734098603}, {'label': 'SIMILARITY', 'score': 4.214001546261018}, {'label': 'SIMILARITY', 'score': 4.223094794231234}, {'label': 'SIMILARITY', 'score': 2.8305328160180974}, {'label': 'SIMILARITY', 'score': 3.910012697629907}, {'label': 'SIMILARITY', 'score': 2.5348635594174462}, {'label': 'SIMILARITY', 'score': 2.698154589356843}, {'label': 'SIMILARITY', 'score': 1.2974472136073012}, {'label': 'SIMILARITY', 'score': 2.7359394241941977}, {'label': 'SIMILARITY', 'score': 4.223836295285371}, {'label': 'SIMILARITY', 'score': 1.4201899002051974}, {'label': 'SIMILARITY', 'score': 2.5394364175531363}, {'label': 'SIMILARITY', 'score': 2.648660294808132}, {'label': 'SIMILARITY', 'score': 1.9675709763831828}, {'label': 'SIMILARITY', 'score': 4.20033223703803}, {'label': 'SIMILARITY', 'score': 2.3916943130852344}, {'label': 'SIMILARITY', 'score': 4.196578734299453}, {'label': 'SIMILARITY', 'score': 3.4220305258709}, {'label': 'SIMILARITY', 'score': 2.968707039474865}, {'label': 'SIMILARITY', 'score': 1.8084617568529233}, {'label': 'SIMILARITY', 'score': 3.1063659543284436}, {'label': 'SIMILARITY', 'score': 2.9107692357532837}, {'label': 'SIMILARITY', 'score': 1.6271712009390433}, {'label': 'SIMILARITY', 'score': 3.418058383677112}, {'label': 'SIMILARITY', 'score': 1.6502441875051381}, {'label': 'SIMILARITY', 'score': 3.1890815730859448}, {'label': 'SIMILARITY', 'score': 2.4433267686588827}, {'label': 'SIMILARITY', 'score': 3.0834173356832766}, {'label': 'SIMILARITY', 'score': 2.9890797804758544}, {'label': 'SIMILARITY', 'score': 2.568461704159322}, {'label': 'SIMILARITY', 'score': 1.9907446588639341}, {'label': 'SIMILARITY', 'score': 3.3140227225577448}, {'label': 'SIMILARITY', 'score': 1.3162520522476175}, {'label': 'SIMILARITY', 'score': 1.5420105777781523}, {'label': 'SIMILARITY', 'score': 2.8685362904164746}, {'label': 'SIMILARITY', 'score': 2.7247829061649065}, {'label': 'SIMILARITY', 'score': 1.6359079535671615}, {'label': 'SIMILARITY', 'score': 2.5174315652013823}, {'label': 'SIMILARITY', 'score': 2.8966345082209717}, {'label': 'SIMILARITY', 'score': 3.4156364976410667}, {'label': 'SIMILARITY', 'score': 2.9927332728057396}, {'label': 'SIMILARITY', 'score': 0.7761535014207026}, {'label': 'SIMILARITY', 'score': 2.2287387528024882}, {'label': 'SIMILARITY', 'score': 3.0763168607693916}, {'label': 'SIMILARITY', 'score': 2.4031904714755252}, {'label': 'SIMILARITY', 'score': 3.123832525087535}, {'label': 'SIMILARITY', 'score': 0.7696485243781807}, {'label': 'SIMILARITY', 'score': 4.214694988408066}, {'label': 'SIMILARITY', 'score': 4.229176601595463}, {'label': 'SIMILARITY', 'score': 2.275459531985689}, {'label': 'SIMILARITY', 'score': 3.0684109436661045}, {'label': 'SIMILARITY', 'score': 1.9726350790977258}, {'label': 'SIMILARITY', 'score': 3.1110983670979984}, {'label': 'SIMILARITY', 'score': 2.321016802074719}, {'label': 'SIMILARITY', 'score': 3.0019148985077586}, {'label': 'SIMILARITY', 'score': 4.2213165310312}, {'label': 'SIMILARITY', 'score': 2.871418000798965}, {'label': 'SIMILARITY', 'score': 4.2288986093464445}, {'label': 'SIMILARITY', 'score': 1.2803803446321846}, {'label': 'SIMILARITY', 'score': 3.037367559099622}, {'label': 'SIMILARITY', 'score': 2.9701033645925587}, {'label': 'SIMILARITY', 'score': 2.550179046219783}, {'label': 'SIMILARITY', 'score': 2.285556108879954}, {'label': 'SIMILARITY', 'score': 1.2479305196414519}, {'label': 'SIMILARITY', 'score': 2.3951762218395434}, {'label': 'SIMILARITY', 'score': 2.9607197638474583}, {'label': 'SIMILARITY', 'score': 2.167324023280203}, {'label': 'SIMILARITY', 'score': 4.225833041403794}, {'label': 'SIMILARITY', 'score': 2.9222268697786213}, {'label': 'SIMILARITY', 'score': 0.8443567660616678}, {'label': 'SIMILARITY', 'score': 2.8830088992169887}, {'label': 'SIMILARITY', 'score': 3.4631639016271976}, {'label': 'SIMILARITY', 'score': 3.4682795054679687}, {'label': 'SIMILARITY', 'score': 2.646263386733232}, {'label': 'SIMILARITY', 'score': 3.0610244540541194}, {'label': 'SIMILARITY', 'score': 1.106015704070109}, {'label': 'SIMILARITY', 'score': 3.1087682004556587}, {'label': 'SIMILARITY', 'score': 2.997909616946205}, {'label': 'SIMILARITY', 'score': 3.324359073047229}, {'label': 'SIMILARITY', 'score': 3.1936069443827524}, {'label': 'SIMILARITY', 'score': 3.154046797738795}, {'label': 'SIMILARITY', 'score': 3.1251077856370815}, {'label': 'SIMILARITY', 'score': 4.227261663048868}, {'label': 'SIMILARITY', 'score': 4.227816839007305}, {'label': 'SIMILARITY', 'score': 3.8851493175951592}, {'label': 'SIMILARITY', 'score': 3.059483190656141}, {'label': 'SIMILARITY', 'score': 3.287280033513668}, {'label': 'SIMILARITY', 'score': 2.6171760583526864}, {'label': 'SIMILARITY', 'score': 2.3120112126179735}, {'label': 'SIMILARITY', 'score': 2.9132116710890394}, {'label': 'SIMILARITY', 'score': 4.229463096380935}, {'label': 'SIMILARITY', 'score': 1.7935271474048726}, {'label': 'SIMILARITY', 'score': 2.6925650970471753}, {'label': 'SIMILARITY', 'score': 3.027673576164223}, {'label': 'SIMILARITY', 'score': 1.8458576559828146}, {'label': 'SIMILARITY', 'score': 1.5853411884529516}, {'label': 'SIMILARITY', 'score': 3.2792450770641133}, {'label': 'SIMILARITY', 'score': 3.234186059508279}, {'label': 'SIMILARITY', 'score': 2.472314514153721}, {'label': 'SIMILARITY', 'score': 2.0183383916705635}, {'label': 'SIMILARITY', 'score': 1.3679920324011994}, {'label': 'SIMILARITY', 'score': 1.7792717101203868}, {'label': 'SIMILARITY', 'score': 3.0513694731164085}, {'label': 'SIMILARITY', 'score': 2.4922729853593935}, {'label': 'SIMILARITY', 'score': 2.4614692042619897}, {'label': 'SIMILARITY', 'score': 2.1878887256077846}, {'label': 'SIMILARITY', 'score': 1.9801008437279666}, {'label': 'SIMILARITY', 'score': 2.938728867255567}, {'label': 'SIMILARITY', 'score': 3.210179440358155}, {'label': 'SIMILARITY', 'score': 4.196778739639225}, {'label': 'SIMILARITY', 'score': 3.1912366285838973}, {'label': 'SIMILARITY', 'score': 3.6953579486344883}, {'label': 'SIMILARITY', 'score': 2.2313742954955185}, {'label': 'SIMILARITY', 'score': 3.2169749322911523}, {'label': 'SIMILARITY', 'score': 1.7080155690133052}, {'label': 'SIMILARITY', 'score': 2.653902787224176}, {'label': 'SIMILARITY', 'score': 2.869105002351585}, {'label': 'SIMILARITY', 'score': 3.4940456300933116}, {'label': 'SIMILARITY', 'score': 3.047800578046651}, {'label': 'SIMILARITY', 'score': 3.358434172574894}, {'label': 'SIMILARITY', 'score': 3.017624336341294}, {'label': 'SIMILARITY', 'score': 4.21692380619942}, {'label': 'SIMILARITY', 'score': 2.5702631208537827}, {'label': 'SIMILARITY', 'score': 2.338742928639461}, {'label': 'SIMILARITY', 'score': 1.143535337541218}, {'label': 'SIMILARITY', 'score': 2.020739549761885}, {'label': 'SIMILARITY', 'score': 4.228928089814795}, {'label': 'SIMILARITY', 'score': 2.5209341910171017}, {'label': 'SIMILARITY', 'score': 2.785477791699258}, {'label': 'SIMILARITY', 'score': 1.814929237066062}, {'label': 'SIMILARITY', 'score': 2.1112682887836924}, {'label': 'SIMILARITY', 'score': 2.2009000337821134}, {'label': 'SIMILARITY', 'score': 2.4452508588804984}, {'label': 'SIMILARITY', 'score': 2.1468214269289745}, {'label': 'SIMILARITY', 'score': 2.3165075135077613}, {'label': 'SIMILARITY', 'score': 3.83232896258316}, {'label': 'SIMILARITY', 'score': 2.932659489283412}, {'label': 'SIMILARITY', 'score': 3.2170103283232705}, {'label': 'SIMILARITY', 'score': 2.973431858065478}, {'label': 'SIMILARITY', 'score': 1.8899742870754592}, {'label': 'SIMILARITY', 'score': 2.3717347496480263}, {'label': 'SIMILARITY', 'score': 1.6524786746269269}, {'label': 'SIMILARITY', 'score': 3.012150850613276}, {'label': 'SIMILARITY', 'score': 1.9415641221989584}, {'label': 'SIMILARITY', 'score': 1.1155918388387105}, {'label': 'SIMILARITY', 'score': 3.081651044990994}, {'label': 'SIMILARITY', 'score': 1.9563926604616555}, {'label': 'SIMILARITY', 'score': 3.0203531557415038}, {'label': 'SIMILARITY', 'score': 3.118918323211551}, {'label': 'SIMILARITY', 'score': 2.9893515446958787}, {'label': 'SIMILARITY', 'score': 1.795178900092425}, {'label': 'SIMILARITY', 'score': 2.5466234723786996}, {'label': 'SIMILARITY', 'score': 2.308530424110914}, {'label': 'SIMILARITY', 'score': 2.6463829210127345}, {'label': 'SIMILARITY', 'score': 3.515560038655625}, {'label': 'SIMILARITY', 'score': 2.3425893316412507}, {'label': 'SIMILARITY', 'score': 2.329172902958206}, {'label': 'SIMILARITY', 'score': 2.1196050355111216}, {'label': 'SIMILARITY', 'score': 3.1727839142677303}, {'label': 'SIMILARITY', 'score': 4.213105330907649}, {'label': 'SIMILARITY', 'score': 3.0063778884979304}, {'label': 'SIMILARITY', 'score': 2.0801798975837387}, {'label': 'SIMILARITY', 'score': 3.8965150577026066}, {'label': 'SIMILARITY', 'score': 3.0993755461353185}, {'label': 'SIMILARITY', 'score': 1.5618688246970154}, {'label': 'SIMILARITY', 'score': 2.37163397280361}, {'label': 'SIMILARITY', 'score': 0.2549516948147848}, {'label': 'SIMILARITY', 'score': 4.22487193862348}, {'label': 'SIMILARITY', 'score': 2.5365612702507345}, {'label': 'SIMILARITY', 'score': 2.828321671552348}, {'label': 'SIMILARITY', 'score': 4.209015886661863}, {'label': 'SIMILARITY', 'score': 3.9873668811012446}, {'label': 'SIMILARITY', 'score': 2.963598772757089}, {'label': 'SIMILARITY', 'score': 3.117778032239381}, {'label': 'SIMILARITY', 'score': 2.8296095623957647}, {'label': 'SIMILARITY', 'score': 2.6412886570846785}, {'label': 'SIMILARITY', 'score': 3.3628572547225124}, {'label': 'SIMILARITY', 'score': 3.0978703111900083}, {'label': 'SIMILARITY', 'score': 2.7681961302799247}, {'label': 'SIMILARITY', 'score': 1.9673593994694811}, {'label': 'SIMILARITY', 'score': 1.5967460917088798}, {'label': 'SIMILARITY', 'score': 3.1088220968798654}, {'label': 'SIMILARITY', 'score': 4.225849837199661}, {'label': 'SIMILARITY', 'score': 2.3122685288130196}, {'label': 'SIMILARITY', 'score': 1.3637657104706533}, {'label': 'SIMILARITY', 'score': 2.7660275755060115}, {'label': 'SIMILARITY', 'score': 2.090379964352483}, {'label': 'SIMILARITY', 'score': 0.16636513807583939}, {'label': 'SIMILARITY', 'score': 3.0114973998509047}, {'label': 'SIMILARITY', 'score': 2.0726515095438884}, {'label': 'SIMILARITY', 'score': 3.11417250159288}, {'label': 'SIMILARITY', 'score': 1.8799630154678708}, {'label': 'SIMILARITY', 'score': 3.1578492563807963}, {'label': 'SIMILARITY', 'score': 3.084253120058818}, {'label': 'SIMILARITY', 'score': 2.9071665502318615}, {'label': 'SIMILARITY', 'score': 2.2319761689726927}, {'label': 'SIMILARITY', 'score': 2.9952041208145364}, {'label': 'SIMILARITY', 'score': 3.0547821982225805}, {'label': 'SIMILARITY', 'score': 2.8983464419344696}, {'label': 'SIMILARITY', 'score': 3.1312284464668907}, {'label': 'SIMILARITY', 'score': 4.204300932984268}, {'label': 'SIMILARITY', 'score': 2.7063160843560636}, {'label': 'SIMILARITY', 'score': 2.791096458656399}, {'label': 'SIMILARITY', 'score': 3.0885360030079627}, {'label': 'SIMILARITY', 'score': 1.332945013412914}, {'label': 'SIMILARITY', 'score': 1.847425015373883}, {'label': 'SIMILARITY', 'score': 4.228275505530242}, {'label': 'SIMILARITY', 'score': 1.9247152124888682}, {'label': 'SIMILARITY', 'score': 0.8169431219732758}, {'label': 'SIMILARITY', 'score': 3.2267607969150864}, {'label': 'SIMILARITY', 'score': 2.7174433678837824}, {'label': 'SIMILARITY', 'score': 1.3683937239046093}, {'label': 'SIMILARITY', 'score': 3.508361142145068}, {'label': 'SIMILARITY', 'score': 2.192818030963479}, {'label': 'SIMILARITY', 'score': 3.4354865724665746}, {'label': 'SIMILARITY', 'score': 4.215147846290924}, {'label': 'SIMILARITY', 'score': 2.1255436689938687}, {'label': 'SIMILARITY', 'score': 4.216470166954534}, {'label': 'SIMILARITY', 'score': 3.235440572979473}, {'label': 'SIMILARITY', 'score': 2.1662290367224415}, {'label': 'SIMILARITY', 'score': 2.465251063118764}, {'label': 'SIMILARITY', 'score': 3.59834713247841}, {'label': 'SIMILARITY', 'score': 2.3163171314253423}, {'label': 'SIMILARITY', 'score': 3.0665032449928575}, {'label': 'SIMILARITY', 'score': 2.7989692444223815}, {'label': 'SIMILARITY', 'score': 3.3146389821063296}, {'label': 'SIMILARITY', 'score': 2.3004150449319654}, {'label': 'SIMILARITY', 'score': 4.228380733086852}, {'label': 'SIMILARITY', 'score': 3.8382514699972363}, {'label': 'SIMILARITY', 'score': 4.2255727415959505}, {'label': 'SIMILARITY', 'score': 4.228578590013102}, {'label': 'SIMILARITY', 'score': 3.233052197716509}, {'label': 'SIMILARITY', 'score': 1.8884356642066018}, {'label': 'SIMILARITY', 'score': 2.8605998218832567}, {'label': 'SIMILARITY', 'score': 2.107884865854394}, {'label': 'SIMILARITY', 'score': 1.876746985300691}, {'label': 'SIMILARITY', 'score': 3.384867132482312}, {'label': 'SIMILARITY', 'score': 1.876041046391394}, {'label': 'SIMILARITY', 'score': 3.651616789080623}, {'label': 'SIMILARITY', 'score': 2.5337456582375664}, {'label': 'SIMILARITY', 'score': 2.9162930587472484}, {'label': 'SIMILARITY', 'score': 4.146266798553918}, {'label': 'SIMILARITY', 'score': 2.8751735744603923}, {'label': 'SIMILARITY', 'score': 2.5830903664886526}, {'label': 'SIMILARITY', 'score': 3.1345850383712515}, {'label': 'SIMILARITY', 'score': 2.7288055351918525}, {'label': 'SIMILARITY', 'score': 1.270226435619703}, {'label': 'SIMILARITY', 'score': 3.0136191918709065}, {'label': 'SIMILARITY', 'score': 4.2256860898754205}, {'label': 'SIMILARITY', 'score': 2.8931609681004784}, {'label': 'SIMILARITY', 'score': 2.5114512358905206}, {'label': 'SIMILARITY', 'score': 3.368801761254934}, {'label': 'SIMILARITY', 'score': 1.4774440915366829}, {'label': 'SIMILARITY', 'score': 2.386954516424344}, {'label': 'SIMILARITY', 'score': 3.209878902359878}, {'label': 'SIMILARITY', 'score': 4.1272374735389015}, {'label': 'SIMILARITY', 'score': 1.9616727832753103}, {'label': 'SIMILARITY', 'score': 3.0039680174400964}, {'label': 'SIMILARITY', 'score': 2.982002194607692}, {'label': 'SIMILARITY', 'score': 3.177786325335908}, {'label': 'SIMILARITY', 'score': 2.1046710205349126}, {'label': 'SIMILARITY', 'score': 2.165023972249091}, {'label': 'SIMILARITY', 'score': 3.592700643579415}, {'label': 'SIMILARITY', 'score': 2.521986238429711}, {'label': 'SIMILARITY', 'score': 3.373807512890416}, {'label': 'SIMILARITY', 'score': 2.9049455934442294}, {'label': 'SIMILARITY', 'score': 2.856658354533901}, {'label': 'SIMILARITY', 'score': 1.7426468983048402}, {'label': 'SIMILARITY', 'score': 3.0424236101849016}, {'label': 'SIMILARITY', 'score': 2.9858042690889337}, {'label': 'SIMILARITY', 'score': 0.504013267764325}, {'label': 'SIMILARITY', 'score': 3.2805564793425726}, {'label': 'SIMILARITY', 'score': 2.8312084604391483}, {'label': 'SIMILARITY', 'score': 3.2123207857645797}, {'label': 'SIMILARITY', 'score': 4.22927349532147}, {'label': 'SIMILARITY', 'score': 3.051984780185712}, {'label': 'SIMILARITY', 'score': 3.101996965692566}, {'label': 'SIMILARITY', 'score': 2.018421871204093}, {'label': 'SIMILARITY', 'score': 1.8312015208211498}, {'label': 'SIMILARITY', 'score': 4.185641823576176}, {'label': 'SIMILARITY', 'score': 3.0865187789761155}, {'label': 'SIMILARITY', 'score': 1.6802744314869074}, {'label': 'SIMILARITY', 'score': 3.5221811660185547}, {'label': 'SIMILARITY', 'score': 1.957312447590583}, {'label': 'SIMILARITY', 'score': 1.758290716669835}, {'label': 'SIMILARITY', 'score': 2.96028435586003}, {'label': 'SIMILARITY', 'score': 2.093440917579721}, {'label': 'SIMILARITY', 'score': 2.7580767604480707}, {'label': 'SIMILARITY', 'score': 2.8029755179065954}, {'label': 'SIMILARITY', 'score': 1.6741620790609557}, {'label': 'SIMILARITY', 'score': 1.7252206154817131}, {'label': 'SIMILARITY', 'score': 2.2323160432881695}, {'label': 'SIMILARITY', 'score': 0.6986927759657476}, {'label': 'SIMILARITY', 'score': 3.1619540587927273}, {'label': 'SIMILARITY', 'score': 3.304690060324743}, {'label': 'SIMILARITY', 'score': 1.697535468603868}, {'label': 'SIMILARITY', 'score': 1.281282074834865}, {'label': 'SIMILARITY', 'score': 2.0924303542915452}, {'label': 'SIMILARITY', 'score': 2.9147231990287295}, {'label': 'SIMILARITY', 'score': 3.262528293242628}, {'label': 'SIMILARITY', 'score': 1.8174581388368556}, {'label': 'SIMILARITY', 'score': 4.227783183456517}, {'label': 'SIMILARITY', 'score': 2.765096712429445}, {'label': 'SIMILARITY', 'score': 1.1657263191104097}, {'label': 'SIMILARITY', 'score': 3.336360034884898}, {'label': 'SIMILARITY', 'score': 3.0768232228509373}, {'label': 'SIMILARITY', 'score': 2.6085763695642705}, {'label': 'SIMILARITY', 'score': 2.003049335501599}, {'label': 'SIMILARITY', 'score': 0.7029095483129665}, {'label': 'SIMILARITY', 'score': 1.314963093410187}, {'label': 'SIMILARITY', 'score': 1.0326080828885902}, {'label': 'SIMILARITY', 'score': 2.5121279618519794}, {'label': 'SIMILARITY', 'score': 2.9299164323048856}, {'label': 'SIMILARITY', 'score': 3.163551922751922}, {'label': 'SIMILARITY', 'score': 2.8965768077653453}, {'label': 'SIMILARITY', 'score': 3.210555230058434}, {'label': 'SIMILARITY', 'score': 2.9347734751562946}, {'label': 'SIMILARITY', 'score': 3.3623029099836916}, {'label': 'SIMILARITY', 'score': 2.007123570511957}, {'label': 'SIMILARITY', 'score': 2.6902331536180037}, {'label': 'SIMILARITY', 'score': 3.9015707707134157}, {'label': 'SIMILARITY', 'score': 2.2517055868808957}, {'label': 'SIMILARITY', 'score': 1.6530406985043842}, {'label': 'SIMILARITY', 'score': 1.5026500006063463}, {'label': 'SIMILARITY', 'score': 2.655117334808296}, {'label': 'SIMILARITY', 'score': 3.431202912884334}, {'label': 'SIMILARITY', 'score': 4.224226111619519}, {'label': 'SIMILARITY', 'score': 4.1723251818726075}, {'label': 'SIMILARITY', 'score': 2.9411997065404916}, {'label': 'SIMILARITY', 'score': 1.4937418826928135}, {'label': 'SIMILARITY', 'score': 1.3292516798245482}, {'label': 'SIMILARITY', 'score': 1.2201148445783387}, {'label': 'SIMILARITY', 'score': 1.992869325522529}, {'label': 'SIMILARITY', 'score': 1.6416990265831697}, {'label': 'SIMILARITY', 'score': 2.7466526968497633}, {'label': 'SIMILARITY', 'score': 2.447970327859124}, {'label': 'SIMILARITY', 'score': 1.0743024268548}, {'label': 'SIMILARITY', 'score': 4.225165630311883}, {'label': 'SIMILARITY', 'score': 1.8574109144845843}, {'label': 'SIMILARITY', 'score': 1.3362759307378396}, {'label': 'SIMILARITY', 'score': 2.7948728126108806}, {'label': 'SIMILARITY', 'score': 3.11791609693693}, {'label': 'SIMILARITY', 'score': 2.938298251145741}, {'label': 'SIMILARITY', 'score': 2.2089437536021306}, {'label': 'SIMILARITY', 'score': 2.9906014744115224}, {'label': 'SIMILARITY', 'score': 3.7589064467176363}, {'label': 'SIMILARITY', 'score': 3.2853637941660354}, {'label': 'SIMILARITY', 'score': 1.6474454832356982}, {'label': 'SIMILARITY', 'score': 3.3257942657455835}, {'label': 'SIMILARITY', 'score': 2.8051562877022116}, {'label': 'SIMILARITY', 'score': 2.7987830209081777}, {'label': 'SIMILARITY', 'score': 4.223371223724058}, {'label': 'SIMILARITY', 'score': 2.1665346838834836}, {'label': 'SIMILARITY', 'score': 4.221258005160855}, {'label': 'SIMILARITY', 'score': 2.9202531953772914}, {'label': 'SIMILARITY', 'score': 4.226984187287527}, {'label': 'SIMILARITY', 'score': 3.370555567645415}, {'label': 'SIMILARITY', 'score': 2.940958254583442}, {'label': 'SIMILARITY', 'score': 1.9657620320539009}, {'label': 'SIMILARITY', 'score': 3.1203806944997092}, {'label': 'SIMILARITY', 'score': 2.4817044252866087}, {'label': 'SIMILARITY', 'score': 2.4733872333054485}, {'label': 'SIMILARITY', 'score': 2.5773923040038462}, {'label': 'SIMILARITY', 'score': 3.1584085766825676}, {'label': 'SIMILARITY', 'score': 4.199198203170543}, {'label': 'SIMILARITY', 'score': 3.2313161312320755}, {'label': 'SIMILARITY', 'score': 2.851687395224263}, {'label': 'SIMILARITY', 'score': 1.91543510143894}, {'label': 'SIMILARITY', 'score': 3.8967686362646767}, {'label': 'SIMILARITY', 'score': 4.2209278143608016}, {'label': 'SIMILARITY', 'score': 2.974681981112783}, {'label': 'SIMILARITY', 'score': 1.6554277110102877}, {'label': 'SIMILARITY', 'score': 2.817148053504156}, {'label': 'SIMILARITY', 'score': 2.799027652203503}, {'label': 'SIMILARITY', 'score': 4.222646800997861}, {'label': 'SIMILARITY', 'score': 2.7530157827191672}, {'label': 'SIMILARITY', 'score': 3.3891611788952685}, {'label': 'SIMILARITY', 'score': 2.9702446351492435}, {'label': 'SIMILARITY', 'score': 3.131051521709927}, {'label': 'SIMILARITY', 'score': 3.2831943423826777}, {'label': 'SIMILARITY', 'score': 1.4143304323767074}, {'label': 'SIMILARITY', 'score': 3.047992115297071}, {'label': 'SIMILARITY', 'score': 1.5337461102893883}, {'label': 'SIMILARITY', 'score': 3.4698136989522537}, {'label': 'SIMILARITY', 'score': 3.1681427439383785}, {'label': 'SIMILARITY', 'score': 4.227366787034085}, {'label': 'SIMILARITY', 'score': 3.319521657588833}, {'label': 'SIMILARITY', 'score': 1.8865076668658614}, {'label': 'SIMILARITY', 'score': 2.827326772307038}, {'label': 'SIMILARITY', 'score': 3.091775323579}, {'label': 'SIMILARITY', 'score': 3.1594777234283793}, {'label': 'SIMILARITY', 'score': 3.064416991575085}, {'label': 'SIMILARITY', 'score': 1.1810980721197446}, {'label': 'SIMILARITY', 'score': 1.5517487753558867}, {'label': 'SIMILARITY', 'score': 1.8709704987206877}, {'label': 'SIMILARITY', 'score': 2.009221186979087}, {'label': 'SIMILARITY', 'score': 1.879760655782385}, {'label': 'SIMILARITY', 'score': 1.4216297880866184}, {'label': 'SIMILARITY', 'score': 3.1963138857940763}, {'label': 'SIMILARITY', 'score': 4.224905499148487}, {'label': 'SIMILARITY', 'score': 2.9730094550312827}, {'label': 'SIMILARITY', 'score': 3.0513860611488113}, {'label': 'SIMILARITY', 'score': 2.6755964091396662}, {'label': 'SIMILARITY', 'score': 2.501545951479285}, {'label': 'SIMILARITY', 'score': 1.6785758600970857}, {'label': 'SIMILARITY', 'score': 2.9899946375239836}, {'label': 'SIMILARITY', 'score': 1.7960174707891192}, {'label': 'SIMILARITY', 'score': 2.552483132124955}, {'label': 'SIMILARITY', 'score': 3.2470584937320797}, {'label': 'SIMILARITY', 'score': 2.788003666261623}, {'label': 'SIMILARITY', 'score': 2.9847288974513435}, {'label': 'SIMILARITY', 'score': 3.221959442867943}, {'label': 'SIMILARITY', 'score': 2.7497179375651495}, {'label': 'SIMILARITY', 'score': 2.1251652015502476}, {'label': 'SIMILARITY', 'score': 2.537898732346371}, {'label': 'SIMILARITY', 'score': 2.451260523646507}, {'label': 'SIMILARITY', 'score': 3.1361312905043466}, {'label': 'SIMILARITY', 'score': 4.2270514472724345}, {'label': 'SIMILARITY', 'score': 2.783904289490688}, {'label': 'SIMILARITY', 'score': 2.4103839453618643}, {'label': 'SIMILARITY', 'score': 3.0885760421990276}, {'label': 'SIMILARITY', 'score': 2.7931173491421935}, {'label': 'SIMILARITY', 'score': 2.7556749690977327}, {'label': 'SIMILARITY', 'score': 2.555068862187991}, {'label': 'SIMILARITY', 'score': 2.7269982496406735}, {'label': 'SIMILARITY', 'score': 3.155598118258739}, {'label': 'SIMILARITY', 'score': 2.789934850187707}, {'label': 'SIMILARITY', 'score': 2.7391870490070174}, {'label': 'SIMILARITY', 'score': 1.9464404974427176}, {'label': 'SIMILARITY', 'score': 2.6198147262481055}, {'label': 'SIMILARITY', 'score': 3.8388669620353033}, {'label': 'SIMILARITY', 'score': 2.0039281943168046}, {'label': 'SIMILARITY', 'score': 3.051939138847183}, {'label': 'SIMILARITY', 'score': 2.8867345301599183}, {'label': 'SIMILARITY', 'score': 1.6947026194955437}, {'label': 'SIMILARITY', 'score': 4.22828392333901}, {'label': 'SIMILARITY', 'score': 4.051873700072448}, {'label': 'SIMILARITY', 'score': 2.615022722251908}, {'label': 'SIMILARITY', 'score': 2.610131810930863}, {'label': 'SIMILARITY', 'score': 3.1245386846940146}, {'label': 'SIMILARITY', 'score': 3.3694039608812405}, {'label': 'SIMILARITY', 'score': 3.2203507989647155}, {'label': 'SIMILARITY', 'score': 1.0203347709001205}, {'label': 'SIMILARITY', 'score': 3.2814356193064564}, {'label': 'SIMILARITY', 'score': 3.1175841835994875}, {'label': 'SIMILARITY', 'score': 2.5821943839573063}, {'label': 'SIMILARITY', 'score': 3.2592994356612737}, {'label': 'SIMILARITY', 'score': 3.097257553532507}, {'label': 'SIMILARITY', 'score': 2.9651471926294075}, {'label': 'SIMILARITY', 'score': 3.133768022005759}, {'label': 'SIMILARITY', 'score': 3.4527473135956006}, {'label': 'SIMILARITY', 'score': 4.220388869522037}, {'label': 'SIMILARITY', 'score': 3.123470758288352}, {'label': 'SIMILARITY', 'score': 2.955147495105575}, {'label': 'SIMILARITY', 'score': 2.4887331591657813}, {'label': 'SIMILARITY', 'score': 4.229083929172131}, {'label': 'SIMILARITY', 'score': 1.6165743912916741}, {'label': 'SIMILARITY', 'score': 3.0055448410552557}, {'label': 'SIMILARITY', 'score': 3.239566012666129}, {'label': 'SIMILARITY', 'score': 3.386099506804807}, {'label': 'SIMILARITY', 'score': 2.905237508382747}, {'label': 'SIMILARITY', 'score': 1.4251582076716607}, {'label': 'SIMILARITY', 'score': 2.9778949484665747}, {'label': 'SIMILARITY', 'score': 2.2202683320207677}, {'label': 'SIMILARITY', 'score': 3.6984828100915514}, {'label': 'SIMILARITY', 'score': 3.3210726495244716}, {'label': 'SIMILARITY', 'score': 2.4576788520171418}, {'label': 'SIMILARITY', 'score': 2.9326147922239443}, {'label': 'SIMILARITY', 'score': 1.9219192492009223}, {'label': 'SIMILARITY', 'score': 3.8416571158121053}, {'label': 'SIMILARITY', 'score': 1.5740185473072184}, {'label': 'SIMILARITY', 'score': 1.9831126801158796}, {'label': 'SIMILARITY', 'score': 1.8862647092144855}, {'label': 'SIMILARITY', 'score': 4.188861306871045}, {'label': 'SIMILARITY', 'score': 4.227190184869217}, {'label': 'SIMILARITY', 'score': 2.2512641956901893}, {'label': 'SIMILARITY', 'score': 2.192689348418153}, {'label': 'SIMILARITY', 'score': 3.237609550441211}, {'label': 'SIMILARITY', 'score': 1.7878838783951838}, {'label': 'SIMILARITY', 'score': 1.454804664607477}, {'label': 'SIMILARITY', 'score': 2.7447843127892324}, {'label': 'SIMILARITY', 'score': 3.1528540426932232}, {'label': 'SIMILARITY', 'score': 3.8331766773186704}, {'label': 'SIMILARITY', 'score': 2.615074325414789}, {'label': 'SIMILARITY', 'score': 4.220781561133232}, {'label': 'SIMILARITY', 'score': 3.6735673744264177}, {'label': 'SIMILARITY', 'score': 1.9789175842869253}, {'label': 'SIMILARITY', 'score': 2.812372837093739}, {'label': 'SIMILARITY', 'score': 2.4856716845599833}, {'label': 'SIMILARITY', 'score': 2.573727636426762}, {'label': 'SIMILARITY', 'score': 2.4222854841496675}, {'label': 'SIMILARITY', 'score': 3.5825657287780297}, {'label': 'SIMILARITY', 'score': 2.9359733108820647}, {'label': 'SIMILARITY', 'score': 4.032577080870264}, {'label': 'SIMILARITY', 'score': 1.3551786387910625}, {'label': 'SIMILARITY', 'score': 2.7871461771693262}, {'label': 'SIMILARITY', 'score': 3.240093484150846}, {'label': 'SIMILARITY', 'score': 4.136529230479042}, {'label': 'SIMILARITY', 'score': 3.0900628178378966}, {'label': 'SIMILARITY', 'score': 2.069190509360031}, {'label': 'SIMILARITY', 'score': 2.841282284441624}, {'label': 'SIMILARITY', 'score': 2.814593093500041}, {'label': 'SIMILARITY', 'score': 4.22802300326461}, {'label': 'SIMILARITY', 'score': 2.0421291714384426}, {'label': 'SIMILARITY', 'score': 3.3536219096805397}, {'label': 'SIMILARITY', 'score': 3.2996291637925075}, {'label': 'SIMILARITY', 'score': 1.5004674132073434}, {'label': 'SIMILARITY', 'score': 1.5111773389782026}, {'label': 'SIMILARITY', 'score': 2.521711662774929}, {'label': 'SIMILARITY', 'score': 3.0564487973044177}, {'label': 'SIMILARITY', 'score': 4.2281534550364706}, {'label': 'SIMILARITY', 'score': 2.74439882156908}, {'label': 'SIMILARITY', 'score': 4.225064926581685}, {'label': 'SIMILARITY', 'score': 2.678050348781929}, {'label': 'SIMILARITY', 'score': 3.2073563911089793}, {'label': 'SIMILARITY', 'score': 3.3079197242312617}, {'label': 'SIMILARITY', 'score': 1.5595881227702542}, {'label': 'SIMILARITY', 'score': 2.795136367447385}, {'label': 'SIMILARITY', 'score': 2.712905677236881}, {'label': 'SIMILARITY', 'score': 1.4446230134548612}, {'label': 'SIMILARITY', 'score': 2.7738307960376654}, {'label': 'SIMILARITY', 'score': 2.9327228131918726}, {'label': 'SIMILARITY', 'score': 2.927872221729528}, {'label': 'SIMILARITY', 'score': 3.3199304348456833}, {'label': 'SIMILARITY', 'score': 1.8481913676593917}, {'label': 'SIMILARITY', 'score': 1.0233217339427254}, {'label': 'SIMILARITY', 'score': 2.845468600962301}, {'label': 'SIMILARITY', 'score': 3.066912559696785}, {'label': 'SIMILARITY', 'score': 2.9808410330891397}, {'label': 'SIMILARITY', 'score': 2.8557191880809416}, {'label': 'SIMILARITY', 'score': 2.3592832690287455}, {'label': 'SIMILARITY', 'score': 3.116470220822215}, {'label': 'SIMILARITY', 'score': 3.0910642011416707}, {'label': 'SIMILARITY', 'score': 2.7784055363697258}, {'label': 'SIMILARITY', 'score': 0.7925220736640286}, {'label': 'SIMILARITY', 'score': 2.2099596497707577}, {'label': 'SIMILARITY', 'score': 2.5077087966630107}, {'label': 'SIMILARITY', 'score': 2.9456327169814567}, {'label': 'SIMILARITY', 'score': 3.103881961588625}, {'label': 'SIMILARITY', 'score': 2.617083945434252}, {'label': 'SIMILARITY', 'score': 2.734749591904275}, {'label': 'SIMILARITY', 'score': 2.2552181196624077}, {'label': 'SIMILARITY', 'score': 2.0053821082553145}, {'label': 'SIMILARITY', 'score': 1.6806508596789276}, {'label': 'SIMILARITY', 'score': 1.923165942986546}, {'label': 'SIMILARITY', 'score': 0.8678666368481547}, {'label': 'SIMILARITY', 'score': 2.905357445780855}, {'label': 'SIMILARITY', 'score': 3.217802214559931}, {'label': 'SIMILARITY', 'score': 1.8884132116685588}, {'label': 'SIMILARITY', 'score': 2.750514841021356}, {'label': 'SIMILARITY', 'score': 2.848001481281147}, {'label': 'SIMILARITY', 'score': 1.8596053786898723}, {'label': 'SIMILARITY', 'score': 1.3602368077693165}, {'label': 'SIMILARITY', 'score': 2.0935367093443107}, {'label': 'SIMILARITY', 'score': 1.8347962476781605}, {'label': 'SIMILARITY', 'score': 3.7373490482199263}, {'label': 'SIMILARITY', 'score': 1.78956640528508}, {'label': 'SIMILARITY', 'score': 3.409580684175606}, {'label': 'SIMILARITY', 'score': 1.633440135196017}, {'label': 'SIMILARITY', 'score': 1.8970556223411723}, {'label': 'SIMILARITY', 'score': 3.2386940307101466}, {'label': 'SIMILARITY', 'score': 3.0303772402119087}, {'label': 'SIMILARITY', 'score': 2.8021784300688184}, {'label': 'SIMILARITY', 'score': 3.5982253103784116}, {'label': 'SIMILARITY', 'score': 2.4267101519330296}, {'label': 'SIMILARITY', 'score': 2.592350550816409}, {'label': 'SIMILARITY', 'score': 2.0103187466029655}, {'label': 'SIMILARITY', 'score': 3.0776823099638233}, {'label': 'SIMILARITY', 'score': 1.6488073764728854}, {'label': 'SIMILARITY', 'score': 0.13343882476801083}, {'label': 'SIMILARITY', 'score': 3.550090151490886}, {'label': 'SIMILARITY', 'score': 2.938015009169587}, {'label': 'SIMILARITY', 'score': 0.3417628953317474}, {'label': 'SIMILARITY', 'score': 3.0442253742294425}, {'label': 'SIMILARITY', 'score': 1.8632296487493372}, {'label': 'SIMILARITY', 'score': 1.5399927986906268}, {'label': 'SIMILARITY', 'score': 3.0484414630806107}, {'label': 'SIMILARITY', 'score': 2.019662026442775}, {'label': 'SIMILARITY', 'score': 3.26291096685721}, {'label': 'SIMILARITY', 'score': 1.4783154283778386}, {'label': 'SIMILARITY', 'score': 2.3042720669038097}, {'label': 'SIMILARITY', 'score': 2.7092012933260294}, {'label': 'SIMILARITY', 'score': 1.8408419711799286}, {'label': 'SIMILARITY', 'score': 2.2462484084891825}, {'label': 'SIMILARITY', 'score': 3.103119458348488}, {'label': 'SIMILARITY', 'score': 1.434264944451407}, {'label': 'SIMILARITY', 'score': 1.6578234770401423}, {'label': 'SIMILARITY', 'score': 3.205390351137702}, {'label': 'SIMILARITY', 'score': 1.9412029533831532}, {'label': 'SIMILARITY', 'score': 2.9300043730248917}, {'label': 'SIMILARITY', 'score': 1.4843039469466695}, {'label': 'SIMILARITY', 'score': 0.5160253935872487}, {'label': 'SIMILARITY', 'score': 3.096874213784272}, {'label': 'SIMILARITY', 'score': 3.5277509426230718}, {'label': 'SIMILARITY', 'score': 3.0858567050896704}, {'label': 'SIMILARITY', 'score': 0.6379446981790331}, {'label': 'SIMILARITY', 'score': 1.7902489290434058}, {'label': 'SIMILARITY', 'score': 1.2113153071863008}, {'label': 'SIMILARITY', 'score': 4.227930434613974}, {'label': 'SIMILARITY', 'score': 2.864304067744304}, {'label': 'SIMILARITY', 'score': 3.140841742569009}, {'label': 'SIMILARITY', 'score': 3.8703074385236307}, {'label': 'SIMILARITY', 'score': 1.99227490826603}, {'label': 'SIMILARITY', 'score': 1.6809931514954324}, {'label': 'SIMILARITY', 'score': 2.8703407469332007}, {'label': 'SIMILARITY', 'score': 2.96133879567211}, {'label': 'SIMILARITY', 'score': 4.224918084627374}, {'label': 'SIMILARITY', 'score': 1.4214851714482544}, {'label': 'SIMILARITY', 'score': 2.328818399955627}, {'label': 'SIMILARITY', 'score': 2.666950049308541}, {'label': 'SIMILARITY', 'score': 1.7554415545469173}, {'label': 'SIMILARITY', 'score': 2.0243244296231806}, {'label': 'SIMILARITY', 'score': 2.921156030805957}, {'label': 'SIMILARITY', 'score': 2.105409827886432}, {'label': 'SIMILARITY', 'score': 1.8047724149532816}, {'label': 'SIMILARITY', 'score': 2.5231849019622903}, {'label': 'SIMILARITY', 'score': 2.4082562299205827}, {'label': 'SIMILARITY', 'score': 4.127577156910778}, {'label': 'SIMILARITY', 'score': 2.9422486921696405}, {'label': 'SIMILARITY', 'score': 1.3394031773364805}, {'label': 'SIMILARITY', 'score': 2.9752662851340688}, {'label': 'SIMILARITY', 'score': 2.989761930683661}, {'label': 'SIMILARITY', 'score': 3.0458182677319288}, {'label': 'SIMILARITY', 'score': 4.227623334612204}, {'label': 'SIMILARITY', 'score': 3.3986200549400625}, {'label': 'SIMILARITY', 'score': 3.1478922000110945}, {'label': 'SIMILARITY', 'score': 2.9188616830206193}, {'label': 'SIMILARITY', 'score': 1.926172299634595}, {'label': 'SIMILARITY', 'score': 2.667875899156841}, {'label': 'SIMILARITY', 'score': 3.037891783140517}, {'label': 'SIMILARITY', 'score': 2.8612017886453556}, {'label': 'SIMILARITY', 'score': 3.128706634548026}, {'label': 'SIMILARITY', 'score': 3.029188490347478}, {'label': 'SIMILARITY', 'score': 1.6478030127655028}, {'label': 'SIMILARITY', 'score': 1.181904591164107}, {'label': 'SIMILARITY', 'score': 2.5917103679798683}, {'label': 'SIMILARITY', 'score': 2.9292502903368645}, {'label': 'SIMILARITY', 'score': 2.9702073893452057}, {'label': 'SIMILARITY', 'score': 2.3445346604658486}, {'label': 'SIMILARITY', 'score': 0.0316818031920107}, {'label': 'SIMILARITY', 'score': 1.8057421404884415}, {'label': 'SIMILARITY', 'score': 2.0445529189744605}, {'label': 'SIMILARITY', 'score': 3.086090668256187}, {'label': 'SIMILARITY', 'score': 3.144536605433389}, {'label': 'SIMILARITY', 'score': 2.4741339739464547}, {'label': 'SIMILARITY', 'score': 1.294805006169131}, {'label': 'SIMILARITY', 'score': 2.2665988873637213}, {'label': 'SIMILARITY', 'score': 2.1747431454887676}, {'label': 'SIMILARITY', 'score': 3.6755664341253294}, {'label': 'SIMILARITY', 'score': 3.3192545321985083}, {'label': 'SIMILARITY', 'score': 2.2234580140250317}, {'label': 'SIMILARITY', 'score': 2.904132026272363}, {'label': 'SIMILARITY', 'score': 2.9728794182310683}, {'label': 'SIMILARITY', 'score': 2.9616955964462557}, {'label': 'SIMILARITY', 'score': 2.358399471387357}, {'label': 'SIMILARITY', 'score': 2.026729104938132}, {'label': 'SIMILARITY', 'score': 3.1244455873174513}, {'label': 'SIMILARITY', 'score': 1.7983836037180685}, {'label': 'SIMILARITY', 'score': 1.9011590813750212}, {'label': 'SIMILARITY', 'score': 2.6239088387158387}, {'label': 'SIMILARITY', 'score': 3.0135991558884156}, {'label': 'SIMILARITY', 'score': 2.3482624726324217}, {'label': 'SIMILARITY', 'score': 2.238352560087081}, {'label': 'SIMILARITY', 'score': 2.867475856940139}, {'label': 'SIMILARITY', 'score': 2.4031371789261926}, {'label': 'SIMILARITY', 'score': 3.499846305626015}, {'label': 'SIMILARITY', 'score': 2.107428950532489}, {'label': 'SIMILARITY', 'score': 3.2064398430815526}, {'label': 'SIMILARITY', 'score': 2.8354928159320534}, {'label': 'SIMILARITY', 'score': 2.5519417879576416}, {'label': 'SIMILARITY', 'score': 3.0644141938975316}, {'label': 'SIMILARITY', 'score': 2.2400020054901364}, {'label': 'SIMILARITY', 'score': 2.6400195581187993}, {'label': 'SIMILARITY', 'score': 3.0299338505231552}, {'label': 'SIMILARITY', 'score': 3.1648607344136406}, {'label': 'SIMILARITY', 'score': 1.2229361743354061}, {'label': 'SIMILARITY', 'score': 3.1704107590992407}, {'label': 'SIMILARITY', 'score': 2.951614410658074}, {'label': 'SIMILARITY', 'score': 3.352139935718038}, {'label': 'SIMILARITY', 'score': 1.226877975464907}, {'label': 'SIMILARITY', 'score': 4.221412687896774}, {'label': 'SIMILARITY', 'score': 1.7911842932918305}, {'label': 'SIMILARITY', 'score': 2.8182808441270333}, {'label': 'SIMILARITY', 'score': 4.226223614540295}, {'label': 'SIMILARITY', 'score': 2.876966323902264}, {'label': 'SIMILARITY', 'score': 3.269651769045096}, {'label': 'SIMILARITY', 'score': 2.4795974940468177}, {'label': 'SIMILARITY', 'score': 2.777824570426215}, {'label': 'SIMILARITY', 'score': 1.9713490437426475}, {'label': 'SIMILARITY', 'score': 3.064103695993174}, {'label': 'SIMILARITY', 'score': 2.9625600398163323}, {'label': 'SIMILARITY', 'score': 2.046584605899881}, {'label': 'SIMILARITY', 'score': 3.073990319309617}, {'label': 'SIMILARITY', 'score': 4.226698381359096}, {'label': 'SIMILARITY', 'score': 3.350213876950072}, {'label': 'SIMILARITY', 'score': 2.7876239344158287}, {'label': 'SIMILARITY', 'score': 3.779318210090974}, {'label': 'SIMILARITY', 'score': 2.9923768139896434}, {'label': 'SIMILARITY', 'score': 2.1735825409805685}, {'label': 'SIMILARITY', 'score': 4.227392018387717}, {'label': 'SIMILARITY', 'score': 3.189541105924135}, {'label': 'SIMILARITY', 'score': 2.9619581731806277}, {'label': 'SIMILARITY', 'score': 3.1531892782344992}, {'label': 'SIMILARITY', 'score': 2.6686973192371877}, {'label': 'SIMILARITY', 'score': 2.1973203458007307}, {'label': 'SIMILARITY', 'score': 0.8625304901818189}, {'label': 'SIMILARITY', 'score': 4.215097980622575}, {'label': 'SIMILARITY', 'score': 4.060187598134467}, {'label': 'SIMILARITY', 'score': 2.1010594809092633}, {'label': 'SIMILARITY', 'score': 2.60552849781949}, {'label': 'SIMILARITY', 'score': 2.911726559317776}, {'label': 'SIMILARITY', 'score': 1.9234359982908213}, {'label': 'SIMILARITY', 'score': 2.3127033275236997}, {'label': 'SIMILARITY', 'score': 1.6987287065040573}, {'label': 'SIMILARITY', 'score': 2.978755301091307}, {'label': 'SIMILARITY', 'score': 1.7800887487023}, {'label': 'SIMILARITY', 'score': 2.123717430940097}, {'label': 'SIMILARITY', 'score': 2.602834859433103}, {'label': 'SIMILARITY', 'score': 3.1612813315659296}, {'label': 'SIMILARITY', 'score': 2.719318776185432}, {'label': 'SIMILARITY', 'score': 3.0999056853306928}, {'label': 'SIMILARITY', 'score': 2.812845768118447}, {'label': 'SIMILARITY', 'score': 1.8287763958594616}, {'label': 'SIMILARITY', 'score': 2.599880730711132}, {'label': 'SIMILARITY', 'score': 2.0963373591873786}, {'label': 'SIMILARITY', 'score': 3.837315923809184}, {'label': 'SIMILARITY', 'score': 3.5813231996554387}, {'label': 'SIMILARITY', 'score': 4.1827947415190465}, {'label': 'SIMILARITY', 'score': 3.0039666933246054}, {'label': 'SIMILARITY', 'score': 3.2069785200684513}, {'label': 'SIMILARITY', 'score': 4.22266354490213}, {'label': 'SIMILARITY', 'score': 2.7977753020223037}, {'label': 'SIMILARITY', 'score': 2.2507247780137387}, {'label': 'SIMILARITY', 'score': 3.9805914979505985}, {'label': 'SIMILARITY', 'score': 2.90942841636924}, {'label': 'SIMILARITY', 'score': 2.0393114635748306}, {'label': 'SIMILARITY', 'score': 3.0389491816340417}, {'label': 'SIMILARITY', 'score': 2.7290572919469414}, {'label': 'SIMILARITY', 'score': 4.167941444499178}, {'label': 'SIMILARITY', 'score': 2.557031781389589}, {'label': 'SIMILARITY', 'score': 2.17318421636502}, {'label': 'SIMILARITY', 'score': 0.9934040061687817}, {'label': 'SIMILARITY', 'score': 2.885648390614758}, {'label': 'SIMILARITY', 'score': 2.0192782762221917}, {'label': 'SIMILARITY', 'score': 2.975145013477121}, {'label': 'SIMILARITY', 'score': 4.2053335739898525}, {'label': 'SIMILARITY', 'score': 1.8441060505660978}, {'label': 'SIMILARITY', 'score': 1.9996067410523675}, {'label': 'SIMILARITY', 'score': 1.675173869749569}, {'label': 'SIMILARITY', 'score': 1.6996819978254547}, {'label': 'SIMILARITY', 'score': 2.897027671923171}, {'label': 'SIMILARITY', 'score': 4.163722522592244}, {'label': 'SIMILARITY', 'score': 4.05382684051749}, {'label': 'SIMILARITY', 'score': 2.488922659307119}, {'label': 'SIMILARITY', 'score': 3.4573943001697858}, {'label': 'SIMILARITY', 'score': 2.3303513916601384}, {'label': 'SIMILARITY', 'score': 2.6029964128632623}, {'label': 'SIMILARITY', 'score': 2.729114281647501}, {'label': 'SIMILARITY', 'score': 2.293139440245977}, {'label': 'SIMILARITY', 'score': 2.7661849347364322}, {'label': 'SIMILARITY', 'score': 2.8247463962720447}, {'label': 'SIMILARITY', 'score': 1.5170502553255005}, {'label': 'SIMILARITY', 'score': 2.1945998292469784}, {'label': 'SIMILARITY', 'score': 3.393832753030138}, {'label': 'SIMILARITY', 'score': 2.999861631078586}, {'label': 'SIMILARITY', 'score': 2.794473205030836}, {'label': 'SIMILARITY', 'score': 2.7532475516780686}, {'label': 'SIMILARITY', 'score': 0.960778706397081}, {'label': 'SIMILARITY', 'score': 2.5863589743791118}, {'label': 'SIMILARITY', 'score': 2.968456964803852}, {'label': 'SIMILARITY', 'score': 1.6514872603930506}, {'label': 'SIMILARITY', 'score': 1.4611180780137891}, {'label': 'SIMILARITY', 'score': 2.5430186969213944}, {'label': 'SIMILARITY', 'score': 2.907020937319277}, {'label': 'SIMILARITY', 'score': 2.8316689472697236}, {'label': 'SIMILARITY', 'score': 2.878780878372552}, {'label': 'SIMILARITY', 'score': 2.3935315713554197}, {'label': 'SIMILARITY', 'score': 3.1380881404574628}, {'label': 'SIMILARITY', 'score': 2.6928852827042427}, {'label': 'SIMILARITY', 'score': 1.9376394117376454}, {'label': 'SIMILARITY', 'score': 2.9249559518191566}, {'label': 'SIMILARITY', 'score': 2.231203454768787}, {'label': 'SIMILARITY', 'score': 3.0884287623838467}, {'label': 'SIMILARITY', 'score': 3.299387768908125}, {'label': 'SIMILARITY', 'score': 3.2076131634451754}, {'label': 'SIMILARITY', 'score': 2.8859924588164354}, {'label': 'SIMILARITY', 'score': 2.1708241512187922}, {'label': 'SIMILARITY', 'score': 1.789202917232214}, {'label': 'SIMILARITY', 'score': 2.9269639266669762}, {'label': 'SIMILARITY', 'score': 0.9962036319248612}, {'label': 'SIMILARITY', 'score': 2.96321072916933}, {'label': 'SIMILARITY', 'score': 3.070238990393299}, {'label': 'SIMILARITY', 'score': 1.6068761946503334}, {'label': 'SIMILARITY', 'score': 1.6210063933522243}, {'label': 'SIMILARITY', 'score': 3.0220033856530146}, {'label': 'SIMILARITY', 'score': 1.1435665603222345}, {'label': 'SIMILARITY', 'score': 2.8444480960268166}, {'label': 'SIMILARITY', 'score': 2.759175291806565}, {'label': 'SIMILARITY', 'score': 4.227892568019177}, {'label': 'SIMILARITY', 'score': 2.8810807796743942}, {'label': 'SIMILARITY', 'score': 2.2358435627188142}, {'label': 'SIMILARITY', 'score': 2.9606115324507765}, {'label': 'SIMILARITY', 'score': 1.275598764231552}, {'label': 'SIMILARITY', 'score': 3.4540024510881584}, {'label': 'SIMILARITY', 'score': 2.4499834528366335}, {'label': 'SIMILARITY', 'score': 3.2163589047681787}, {'label': 'SIMILARITY', 'score': 3.4523025558837137}, {'label': 'SIMILARITY', 'score': 4.2239033499516445}, {'label': 'SIMILARITY', 'score': 1.809298987109514}, {'label': 'SIMILARITY', 'score': 2.848532235563948}, {'label': 'SIMILARITY', 'score': 3.119147700710317}, {'label': 'SIMILARITY', 'score': 3.0850139958565737}, {'label': 'SIMILARITY', 'score': 2.944442492708951}, {'label': 'SIMILARITY', 'score': 1.7885388717529371}, {'label': 'SIMILARITY', 'score': 2.7185649576894377}, {'label': 'SIMILARITY', 'score': 4.226345436472109}, {'label': 'SIMILARITY', 'score': 4.224569943110671}, {'label': 'SIMILARITY', 'score': 0.484728008360793}]\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe(prepare(fine_runed_corpus_semantic), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions:\n",
    "    prediction['score'] = logit(prediction['score'])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(mean_embbeding_corpus_semantic_test):\n",
    "    mean_embbeding_corpus_semantic_test[i] = (\n",
    "        compute_mean_embedding(frase1, model_100MB.wv, 100),\n",
    "        compute_mean_embedding(frase2, model_100MB.wv, 100)\n",
    "    )\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model amb embeddings entrenables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "vectors_1 =[pair[0] for pair in mean_embbeding_corpus_semantic]\n",
    "vectors_2 =[pair[1] for pair in mean_embbeding_corpus_semantic]\n",
    "\n",
    "# Tokenització i creació del vocabulari\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Padding de les seqüències\n",
    "max_length = 10\n",
    "data_1 = pad_sequences(sequences_1, maxlen=max_length)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=max_length)\n",
    "\n",
    "# Paràmetres del model\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 100\n",
    "hidden_size = 64\n",
    "input_length = max_length\n",
    "\n",
    "# Creació d'embeddings aleatoris\n",
    "random_embeddings = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Creació d'embeddings de Word2Vec\n",
    "texts_combined = [text.split() for text in texts_1 + texts_2]\n",
    "word2vec_model = Word2Vec(texts_combined, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "word2vec_embeddings = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        word2vec_embeddings[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Entrenament del model amb embeddings aleatoris\n",
    "model_random = build_and_compile_model(random_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_random.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Entrenament del model amb embeddings de Word2Vec\n",
    "model_word2vec = build_and_compile_model(word2vec_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_word2vec.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Avalua els resultats dels models\n",
    "loss_random = model_random.evaluate([data_1, data_2], labels)\n",
    "loss_word2vec = model_word2vec.evaluate([data_1, data_2], labels)\n",
    "\n",
    "print(f'Random Embeddings - Loss: {loss_random}')\n",
    "print(f'Word2Vec Embeddings - Loss: {loss_word2vec}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(\n",
    "        input_length: int = 10, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16,\n",
    ") -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1, )\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "    _pooled_1, _pooled_2 = pooling(_embedded_1, mask=_input_mask_1), pooling(_embedded_2, mask=_input_mask_2)\n",
    "    _concatenated = concatenate((_pooled_1, _pooled_2, ))\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=_output, )\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Exemple de dades\n",
    "texts_1 = [\"aquest és un text d'exemple\", \"un altre text\"]\n",
    "texts_2 = [\"aquest és un altre text\", \"un text diferent\"]\n",
    "labels = [0.8, 0.4]  # Similitud entre les frases\n",
    "\n",
    "# Tokenització i creació del vocabulari\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Padding de les seqüències\n",
    "max_length = 10\n",
    "data_1 = pad_sequences(sequences_1, maxlen=max_length)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=max_length)\n",
    "\n",
    "# Paràmetres del model\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 100\n",
    "hidden_size = 64\n",
    "input_length = max_length\n",
    "\n",
    "# Creació d'embeddings de Word2Vec\n",
    "texts_combined = [text.split() for text in texts_1 + texts_2]\n",
    "word2vec_model = Word2Vec(texts_combined, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "word2vec_embeddings = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        word2vec_embeddings[i] = word2vec_model.wv[word]\n",
    "\n",
    "def build_and_compile_model(embedding_matrix, input_length: int = 100, hidden_size: int = 64) -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "\n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        vocab_size, embedding_dim, weights=[embedding_matrix], input_length=input_length, mask_zero=True, trainable=True)\n",
    "\n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "\n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "\n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Entrenament del model amb embeddings de Word2Vec\n",
    "model_word2vec = build_and_compile_model(word2vec_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_word2vec.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Avalua els resultats dels models\n",
    "loss_word2vec = model_word2vec.evaluate([data_1, data_2], labels)\n",
    "\n",
    "print(f'Word2Vec Embeddings - Loss: {loss_word2vec}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Defineix funcions per calcular els embeddings de frases\n",
    "def compute_mean_embedding(text, model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in text if word in model]\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        mean_vector = np.zeros(embedding_dim)\n",
    "    return mean_vector\n",
    "\n",
    "def compute_weighted_mean_embedding(text, model, word2tfidf, embedding_dim):\n",
    "    \"\"\"\n",
    "    Utilitza una ponderació per a cada vector de paraula, com ara la freqüència \n",
    "    inversa del document (TF-IDF), per calcular una mitjana ponderada dels vectors.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] * word2tfidf[word] for word in text if word in model and word in word2tfidf]\n",
    "    if vectors:\n",
    "        weighted_mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        weighted_mean_vector = np.zeros(embedding_dim)\n",
    "    return weighted_mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de corpus\n",
    "corpus = [\"Aquest és un text de exemple\", \"Aquí hi ha un altre text\"]\n",
    "\n",
    "# Prepara els vectors TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.7557\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1112\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4268\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5486\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4800\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3008\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0543\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2087\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2880\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3351\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.10263795]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'ús amb embeddings pre-entrenats (Word2Vec)\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carrega el model pre-entrenat (substitueix 'path_to_model' amb la ruta real)\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format('path_to_model', binary=True)\n",
    "\n",
    "# Defineix la longitud de les seqüències i la dimensió dels embeddings\n",
    "input_length = 10\n",
    "embedding_dim = model_100MB.vector_size\n",
    "\n",
    "# Prepara les dades d'entrenament\n",
    "X_train1 = np.array([compute_mean_embedding(text.split(), model_100MB.wv, embedding_dim) for text in corpus])\n",
    "X_train2 = np.array([compute_weighted_mean_embedding(text.split(), model_100MB.wv, word2tfidf, embedding_dim) for text in corpus])\n",
    "y_train = np.random.rand(len(corpus), 1)\n",
    "\n",
    "# Defineix el model de Keras (ajusta les dimensions segons la longitud de les seqüències i la dimensió dels embeddings)\n",
    "def build_and_compile_model(input_dim, hidden_size=64):\n",
    "    input_1 = tf.keras.Input(shape=(input_dim,))\n",
    "    input_2 = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')(concatenate)\n",
    "    output = tf.keras.layers.Dense(1)(hidden)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construir i compilar el model\n",
    "m = build_and_compile_model(input_dim=embedding_dim)\n",
    "\n",
    "# Entrenar el model\n",
    "m.fit([X_train1, X_train2], y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Exemple de predicció\n",
    "input_data1 = compute_mean_embedding(\"Aquest és un text de exemple\".split(), model_100MB.wv, embedding_dim).reshape(1, -1)\n",
    "input_data2 = compute_weighted_mean_embedding(\"Aquí hi ha un altre text\".split(), model_100MB.wv, word2tfidf, embedding_dim).reshape(1, -1)\n",
    "\n",
    "# Predir la similitud\n",
    "y_pred = m.predict([input_data1, input_data2])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24829134],\n",
       "       [0.49559584]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
