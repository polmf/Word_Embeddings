{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 4 (De nou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from typing import Tuple, List, Optional\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codi per entrenar models word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/catalan_general_crawling contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/catalan_general_crawling\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files per 100MB: 57333\n",
      "files per 500MB: 213432\n",
      "files per 1GB: 410702\n"
     ]
    }
   ],
   "source": [
    "# Getting the first 100 MB of the dataset\n",
    "size = 0\n",
    "for i in range(len(dataset)):\n",
    "    size += len(dataset[i]['text'])\n",
    "    if size > 100000000:  # 100 MB\n",
    "        print('files per 100MB:', i)\n",
    "        break\n",
    "\n",
    "dataset_100MB = dataset.select(list(range(i)))\n",
    "\n",
    "\n",
    "# Getting the first 500 MB of the dataset\n",
    "size = 0\n",
    "for j in range(len(dataset)):\n",
    "    size += len(dataset[j]['text'])  # corrected from dataset[i]['text']\n",
    "    if size > 500000000:  # 500 MB\n",
    "        print('files per 500MB:', j)\n",
    "        break\n",
    "\n",
    "dataset_500MB = dataset.select(list(range(j)))\n",
    "\n",
    "# Getting the first 1 GB of the dataset\n",
    "size = 0\n",
    "for g in range(len(dataset)):\n",
    "    size += len(dataset[g]['text'])  # corrected from dataset[i]['text']\n",
    "    if size > 1000000000:  # 1 GB\n",
    "        print('files per 1GB:', g)\n",
    "        break\n",
    "\n",
    "dataset_1GB = dataset.select(list(range(g)))\n",
    "\n",
    "# Getting the whole dataset\n",
    "dataset_complet = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['100MB'] = dataset_100MB\n",
    "datasets['500MB'] = dataset_500MB\n",
    "datasets['1GB'] = dataset_1GB  \n",
    "datasets['complet'] = dataset_complet\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_catala = [ \n",
    "    \"a\", \"abans\", \"ací\", \"així\", \"alguns\", \"alguna\", \"algunes\", \"algú\", \"alhora\", \n",
    "    \"als\", \"allò\", \"aquell\", \"aquelles\", \"aquell\", \"aquells\", \"baix\", \n",
    "    \"cada\", \"com\", \"com a\", \"eixa\", \"eixes\", \"eixí\", \n",
    "    \"eixos\", \"el\", \"ella\", \"elles\", \"ell\", \"ells\", \"en\", \"endavant\", \"enfront\", \n",
    "    \"ens\", \"entre\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \n",
    "    \"i\", \"igual\", \"iguals\", \"ja\", \"jo\", \"l'\", \"la\", \"les\", \"li\", \"els\", \"tu\", \"nosaltres\", \"vosaltres\",\n",
    "    \"de\", \"del\", \"dels\", \"d'un\", \"d'una\", \"d'uns\", \"d'unes\", \"des\", \"des de\",\n",
    "    ]\n",
    "\n",
    "# Funció per netejar i tokenitzar el text\n",
    "def preprocess_text(text):\n",
    "    # Neteja el text: treu caràcters no desitjats, converteix a minúscules, etc.\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Substitueix caràcters no alfanumèrics per espais\n",
    "    text = text.lower()  # Converteix a minúscules\n",
    "    words = text.split()  # Tokenitza\n",
    "    # elimina stopwords\n",
    "    words = [word for word in words if word not in stopwords_catala]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega el fitxer de text i processa'l\n",
    "def create_corpus(dataset):\n",
    "    corpus = []\n",
    "    for line in dataset:\n",
    "        words = preprocess_text(line['text'])\n",
    "        if words:  # Assegura't que la línia no està buida\n",
    "            corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus_dict = {}\n",
    "for mida in [\"100MB\", \"500MB\", \"1GB\", \"complet\"]:\n",
    "    dataset = datasets[mida]\n",
    "    corpus_dict[f\"corpus_{mida}\"] = create_corpus(dataset=dataset)\n",
    "    print(f\"Corpus {mida} creat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_word2vec(corpus, mida_vector=100, finestra=5, sg=1, min_count=10, workers=4, epochs=25):\n",
    "    \n",
    "    model = word2vec.Word2Vec(corpus, vector_size=mida_vector, window=finestra, min_count=min_count, workers=workers, epochs=epochs, sg=sg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entrenar models per a cada mida de dataset\n",
    "for mida in [\"500MB\", \"1GB\", \"complet\"]: # \"100MB\"\n",
    "    corpus = corpus_dict[f\"corpus_{mida}\"]\n",
    "    model = entrenar_word2vec(corpus)\n",
    "    models[mida] = model\n",
    "    model.save(f\"model_{mida}.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['100MB', '500MB', '1GB', 'complet']:\n",
    "    # load model\n",
    "    models[model] = gensim.models.Word2Vec.load(f\"model_{model}.model\")\n",
    "    \n",
    "models['100MB'] = gensim.models.Word2Vec.load(\"model_100MB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caseta', 0.715048611164093),\n",
       " ('família', 0.6787399053573608),\n",
       " ('llar', 0.6702232360839844),\n",
       " ('pairal', 0.6680922508239746),\n",
       " ('acollidora', 0.6579598188400269),\n",
       " ('cabana', 0.6460942625999451),\n",
       " ('bonica', 0.6448329091072083),\n",
       " ('jardí', 0.640557587146759),\n",
       " ('habitació', 0.6359531283378601),\n",
       " ('granja', 0.6318382024765015)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['500MB'].wv.most_similar(\"casa\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisites\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/sts-ca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/sts-ca\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "text_semantic = load_dataset(\"projecte-aina/sts-ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atorga per primer cop les mencions Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència Universitària'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_semantic['train']['sentence1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialització"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pairs = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"train\"].to_list()]\n",
    "input_pairs_val = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"validation\"].to_list()]\n",
    "input_pairs_test = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"test\"].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_pairs = input_pairs + input_pairs_val + input_pairs_test\n",
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(sentence_1) for sentence_1, _, _ in all_input_pairs]\n",
    "sentences_2_preproc = [simple_preprocess(sentence_2) for _, sentence_2, _ in all_input_pairs]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "diccionario = Dictionary(sentences_pairs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Atorga per primer cop les mencions Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència Universitària',\n",
       " 'Creen la menció M. Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència universitària',\n",
       " 3.5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de los pesos TF-IDF para las oraciones pre-procesadas\n",
    "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
    "modelo_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    bow = dictionary.doc2bow(sentence_preproc)\n",
    "    tf_idf = tf_idf_model[bow]\n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in wv_model:\n",
    "            vectors.append(wv_model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        dictionary: Dictionary = None,\n",
    "        tf_idf_model: TfidfModel = None,\n",
    "        wv_model: Word2Vec = None,\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess_text(sentence_1)\n",
    "        sentence_2_preproc = preprocess_text(sentence_2)\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, wv_model=wv_model,)\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, wv_model=wv_model,)\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "        else:\n",
    "            # Cálculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model_better(embedding_size: int = 300, learning_rate: float = 1e-3) -> tf.keras.Model:\n",
    "    # Capa de entrada para los pares de vectores\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
    "\n",
    "    # Hidden layer\n",
    "    first_projection = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "    )\n",
    "    projected_1 =  first_projection(input_1)\n",
    "    projected_2 = first_projection(input_2)\n",
    "    \n",
    "    # Compute the cosine distance using a Lambda layer\n",
    "    def normalized_product(x):\n",
    "        x1, x2 = x\n",
    "        x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "        x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "        return x1_normalized * x2_normalized\n",
    "\n",
    "    output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
    "    output = tf.keras.layers.Dropout(0.1)(output)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        16,\n",
    "        activation=\"relu\",\n",
    "    )(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "    )(output)\n",
    "    \n",
    "    output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
    "    \n",
    "    # Define output\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def model_2(\n",
    "    input_length: int = 37,\n",
    "    dictionary_size: int = 1000,\n",
    "    embedding_size: int = 100,\n",
    "    learning_rate: float = 0.01,\n",
    "    pretrained_weights: Optional[np.ndarray] = None,\n",
    "    trainable: bool = False,\n",
    "    use_cosine: bool = False,\n",
    "    l2_regularizer: float = 1e-4,\n",
    ") -> tf.keras.Model:\n",
    "    # Inputs\n",
    "    input_1 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "    input_2 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "\n",
    "    # Embedding Layer\n",
    "    if pretrained_weights is None:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, embeddings_initializer='uniform'\n",
    "        )\n",
    "    else:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            pretrained_weights.shape[0], pretrained_weights.shape[1],  # Ajustar la forma de los embeddings pre-entrenados\n",
    "            input_length=input_length,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(pretrained_weights),\n",
    "            trainable=trainable,\n",
    "        )\n",
    "\n",
    "    # Embed the inputs\n",
    "    embedded_1 = embedding_layer(input_1)\n",
    "    embedded_2 = embedding_layer(input_2)\n",
    "    # Pass through the embedding layer\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_mlp = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_regularizer)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    # Apply attention to each embedding\n",
    "    attention_weights_1 = attention_mlp(embedded_1)  \n",
    "    attention_weights_2 = attention_mlp(embedded_2) \n",
    "    # Mask the attention weights\n",
    "    attention_weights_1 = tf.exp(attention_weights_1) * tf.cast(_input_mask_1[:, :, None], tf.float32)\n",
    "    attention_weights_2 = tf.exp(attention_weights_2) * tf.cast(_input_mask_2[:, :, None], tf.float32)\n",
    "    # Normalize attention weights\n",
    "    attention_weights_1 = attention_weights_1 / tf.reduce_sum(attention_weights_1, axis=1, keepdims=True)\n",
    "    attention_weights_2 = attention_weights_2 / tf.reduce_sum(attention_weights_2, axis=1, keepdims=True)\n",
    "    # Compute context vectors\n",
    "    projected_1 = tf.reduce_sum(embedded_1 * attention_weights_1, axis=1) \n",
    "    projected_2 = tf.reduce_sum(embedded_2 * attention_weights_2, axis=1) \n",
    "    \n",
    "    if use_cosine:\n",
    "        # Compute the cosine distance using a Lambda layer\n",
    "        def cosine_distance(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return 2.5 * (1.0 + tf.reduce_sum(x1_normalized * x2_normalized, axis=1))\n",
    "        output = tf.keras.layers.Lambda(cosine_distance)([projected_1, projected_2])\n",
    "    else:\n",
    "        # Compute the cosine distance using a Lambda layer\n",
    "        def normalized_product(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return x1_normalized * x2_normalized\n",
    "    \n",
    "        output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
    "        output = tf.keras.layers.Dropout(0.1)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_regularizer)\n",
    "        )(output)\n",
    "        output = tf.keras.layers.Dropout(0.2)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "        )(output)\n",
    "        \n",
    "        output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
    "        \n",
    "    # Model Definition\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=output)\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferents models de Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8483 - val_loss: 0.8717\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7588 - val_loss: 1.0024\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4848 - val_loss: 0.9483\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.2249 - val_loss: 0.9719\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1660 - val_loss: 0.9587\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1107 - val_loss: 0.9752\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0795 - val_loss: 0.9577\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0812 - val_loss: 0.9516\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0644 - val_loss: 0.9702\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0611 - val_loss: 0.9635\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0584 - val_loss: 0.9752\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0492 - val_loss: 0.9580\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0473 - val_loss: 0.9851\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0412 - val_loss: 0.9832\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0452 - val_loss: 0.9663\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0428 - val_loss: 0.9526\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0372 - val_loss: 0.9929\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0375 - val_loss: 0.9601\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0373 - val_loss: 0.9854\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.9847\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0401 - val_loss: 0.9730\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0353 - val_loss: 0.9588\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0345 - val_loss: 0.9898\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0373 - val_loss: 0.9721\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0336 - val_loss: 0.9726\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0298 - val_loss: 0.9807\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.9650\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.9768\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.9401\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0278 - val_loss: 0.9800\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.9607\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0308 - val_loss: 0.9530\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0283 - val_loss: 0.9648\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.9517\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.9568\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0278 - val_loss: 0.9629\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0288 - val_loss: 0.9546\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0262 - val_loss: 0.9547\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0287 - val_loss: 0.9584\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0314 - val_loss: 0.9611\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.9461\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.9651\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0209 - val_loss: 0.9402\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0218 - val_loss: 0.9398\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0212 - val_loss: 0.9728\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.9451\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.9518\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 0.9557\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0218 - val_loss: 0.9543\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.9403\n"
     ]
    }
   ],
   "source": [
    "# Calcula el número de muestras de entrenamiento y validación\n",
    "num_train_samples = len(input_pairs)\n",
    "num_val_samples = len(input_pairs_val)\n",
    "\n",
    "# Combina las muestras de entrenamiento y validación para procesarlas juntas\n",
    "all_input_pairs = input_pairs + input_pairs_val + input_pairs_test\n",
    "\n",
    "# Preprocesamiento de todas las oraciones y construcción del vocabulario\n",
    "preprocessed_sentences = [preprocess_text(pair[0]) + preprocess_text(pair[1]) for pair in all_input_pairs]\n",
    "vocab = set([word for sentence in preprocessed_sentences for word in sentence])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Codificación one-hot de cada oración preprocesada\n",
    "encoded_sentences = [one_hot(' '.join(sentence), vocab_size) for sentence in preprocessed_sentences]\n",
    "\n",
    "# Obtener la longitud máxima de las secuencias codificadas\n",
    "max_seq_length = max(len(seq) for seq in encoded_sentences)\n",
    "\n",
    "# Rellenar las secuencias codificadas para que todas tengan la misma longitud\n",
    "padded_encoded_sentences = pad_sequences(encoded_sentences, maxlen=max_seq_length)\n",
    "\n",
    "# Dividir los pares de entrada nuevamente\n",
    "train_encoded_pairs = [\n",
    "    (padded_encoded_sentences[i], padded_encoded_sentences[i + num_train_samples], label)\n",
    "    for i, (_, _, label) in enumerate(input_pairs)\n",
    "    if i + num_train_samples < len(padded_encoded_sentences)\n",
    "]\n",
    "\n",
    "val_encoded_pairs = [\n",
    "    (padded_encoded_sentences[i], padded_encoded_sentences[i + num_val_samples], label)\n",
    "    for i, (_, _, label) in enumerate(input_pairs_val)\n",
    "    if i + num_val_samples < len(padded_encoded_sentences)\n",
    "]\n",
    "\n",
    "# Entrenar el modelo con los datos codificados en one-hot\n",
    "model = model_2(dictionary_size=vocab_size, input_length=max_seq_length, use_cosine=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    x=[np.array([pair[0] for pair in train_encoded_pairs]), np.array([pair[1] for pair in train_encoded_pairs])],\n",
    "    y=np.array([pair[2] for pair in train_encoded_pairs]),\n",
    "    validation_data=(\n",
    "        [np.array([pair[0] for pair in val_encoded_pairs]), np.array([pair[1] for pair in val_encoded_pairs])],\n",
    "        np.array([pair[2] for pair in val_encoded_pairs]),\n",
    "    ),\n",
    "    epochs=50,  # Ajustar según sea necesario\n",
    "    batch_size=32,  # Ajustar según sea necesario\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Codificación one-hot de cada oración en el conjunto de prueba\n",
    "test_encoded_sentences = [one_hot(sentence, vocab_size) for sentence in input_1_sequences_test + input_2_sequences_test]\n",
    "\n",
    "# Obtener la longitud máxima de las secuencias codificadas\n",
    "max_seq_length = max(len(seq) for seq in test_encoded_sentences)\n",
    "\n",
    "# Rellenar las secuencias codificadas para que todas tengan la misma longitud\n",
    "padded_test_data = pad_sequences(test_encoded_sentences, maxlen=max_seq_length)\n",
    "\n",
    "# Obtener las predicciones del modelo en el conjunto de prueba\n",
    "predictions = model.predict([padded_test_data[:,0], padded_test_data[:,1]])\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(labels_test, predictions)\n",
    "\n",
    "print(\"Coeficiente de determinación (R^2) en el conjunto de prueba:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprcessament del text_semantic\n",
    "corpus_semantic = []\n",
    "semantic_score = []\n",
    "for line in text_semantic['train']:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic.append((frase1, frase2))\n",
    "    semantic_score.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_semantic_test = []\n",
    "semantic_score_test = []\n",
    "for line in text_semantic['test']:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic_test.append((frase1, frase2))\n",
    "    semantic_score_test.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari = {}\n",
    "\n",
    "for frases in corpus_semantic:\n",
    "    for frase in frases:\n",
    "        for paraula in frase:\n",
    "            if paraula in vocabulari:\n",
    "                vocabulari[paraula] += 1\n",
    "            else:\n",
    "                vocabulari[paraula] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari_reduit = {palabra for palabra, frecuencia in vocabulari.items() if frecuencia >= 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función para calcular la representación one-hot de un texto\n",
    "def compute_one_hot_encoding(text, vocabulari):\n",
    "    \"\"\"\n",
    "    Genera la representación one-hot de un texto utilizando un vocabulario predefinido.\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(len(vocabulari))\n",
    "    vocabulario_list = list(vocabulari)\n",
    "    for palabra in text:\n",
    "        if palabra in vocabulari:\n",
    "            index = vocabulario_list.index(palabra)\n",
    "            one_hot_vector[index] = 1\n",
    "        \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "one_hot_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic):\n",
    "    one_hot_corpus_semantic[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "model_one_hot = model_2(\n",
    "    input_length=736,\n",
    "    dictionary_size=len(vocabulari_reduit),\n",
    "    embedding_size=100,\n",
    "    learning_rate=0.01,\n",
    "    use_cosine=True,\n",
    "    l2_regularizer=1e-4,\n",
    ")\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic])\n",
    "\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "model_one_hot.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "one_hot_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic_test):\n",
    "    one_hot_corpus_semantic_test[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = model_one_hot.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realitzem el model de regressio amb els embeddings dels models Word2Vec + Mean i + Mean Ponderada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec + Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model 100MB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.34132889471710465\n",
      "Correlación de Pearson (baseline-validation): 0.2878746083296036\n",
      "Correlación de Pearson (baseline-test): 0.40315940546719736\n",
      "\n",
      "65/65 [==============================] - 0s 647us/step\n",
      "Correlación de Pearson (train): 0.9713779307322604\n",
      "16/16 [==============================] - 0s 790us/step\n",
      "Correlación de Pearson (validation): 0.2543408649259307\n",
      "16/16 [==============================] - 0s 668us/step\n",
      "Correlación de Pearson (test): 0.3128291269021576\n",
      "\n",
      "\n",
      "Entrenant model 500MB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.452860546989999\n",
      "Correlación de Pearson (baseline-validation): 0.4618448540799148\n",
      "Correlación de Pearson (baseline-test): 0.5399993029883255\n",
      "\n",
      "65/65 [==============================] - 0s 654us/step\n",
      "Correlación de Pearson (train): 0.9585509443790302\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson (validation): 0.2879460976375824\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Correlación de Pearson (test): 0.3898876578189631\n",
      "\n",
      "\n",
      "Entrenant model 1GB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.46200113639497536\n",
      "Correlación de Pearson (baseline-validation): 0.4586275065105852\n",
      "Correlación de Pearson (baseline-test): 0.5487582028405382\n",
      "\n",
      "65/65 [==============================] - 0s 708us/step\n",
      "Correlación de Pearson (train): 0.962510744540326\n",
      "16/16 [==============================] - 0s 836us/step\n",
      "Correlación de Pearson (validation): 0.2781500111687194\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Correlación de Pearson (test): 0.379328217138237\n",
      "\n",
      "\n",
      "Entrenant model complet\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.4545686670182321\n",
      "Correlación de Pearson (baseline-validation): 0.47057408002119083\n",
      "Correlación de Pearson (baseline-test): 0.5268634171857982\n",
      "\n",
      "65/65 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson (train): 0.9621992604967757\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson (validation): 0.260865453940287\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Correlación de Pearson (test): 0.3941646318609221\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def mapp(word2vec, batch_size: int = 64, num_epochs: int = 64):\n",
    "    \n",
    "    # Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "    mapped = map_pairs(input_pairs, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    \n",
    "    # Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "    mapped_train = map_pairs(input_pairs,  dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    mapped_val = map_pairs(input_pairs_val, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    mapped_test = map_pairs(input_pairs_test, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "\n",
    "    # Obtener las listas de train y test\n",
    "    x_train, y_train = pair_list_to_x_y(mapped_train)\n",
    "    x_val, y_val = pair_list_to_x_y(mapped_val)\n",
    "\n",
    "    # Preparar los conjuntos de datos de entrenamiento y validación\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    \n",
    "    model = build_and_compile_model_better(embedding_size=word2vec.vector_size)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=0)\n",
    "    \n",
    "    from scipy.stats import pearsonr\n",
    "    x_test, y_test = pair_list_to_x_y(mapped_test)\n",
    "    \n",
    "    # Baseline\n",
    "    def compute_pearson_baseline(x_, y_):\n",
    "        y_pred_baseline = []\n",
    "        for v1, v2 in zip(*x_):\n",
    "            d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "            y_pred_baseline.append(d)\n",
    "        # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "        correlation, _ = pearsonr(y_pred_baseline, y_.flatten())\n",
    "        return correlation\n",
    "    \n",
    "    # Imprimir el coeficiente de correlación de Pearson\n",
    "    print(f\"Correlación de Pearson (baseline-train): {compute_pearson_baseline(x_train, y_train)}\")\n",
    "    print(f\"Correlación de Pearson (baseline-validation): {compute_pearson_baseline(x_val, y_val)}\")\n",
    "    print(f\"Correlación de Pearson (baseline-test): {compute_pearson_baseline(x_test, y_test)}\\n\")\n",
    "    \n",
    "    def compute_pearson(x_, y_):\n",
    "        # Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "        y_pred = model.predict(x_)\n",
    "        # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "        correlation, _ = pearsonr(y_pred.flatten(), y_.flatten())\n",
    "        return correlation\n",
    "\n",
    "    # Imprimir el coeficiente de correlación de Pearson\n",
    "    print(f\"Correlación de Pearson (train): {compute_pearson(x_train, y_train)}\")\n",
    "    print(f\"Correlación de Pearson (validation): {compute_pearson(x_val, y_val)}\")\n",
    "    print(f\"Correlación de Pearson (test): {compute_pearson(x_test, y_test)}\\n\\n\")\n",
    "\n",
    "for model in ['100MB', '500MB', '1GB', 'complet']:\n",
    "    print(f\"Entrenant model {model}\\n\")\n",
    "    mapp(models[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec + Mean Ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model 100MB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.3648063022380385\n",
      "Correlación de Pearson (baseline-validation): 0.27936252945628576\n",
      "Correlación de Pearson (baseline-test): 0.3922879849557813\n",
      "\n",
      "65/65 [==============================] - 0s 688us/step\n",
      "Correlación de Pearson (train): 0.97947417844422\n",
      "16/16 [==============================] - 0s 730us/step\n",
      "Correlación de Pearson (validation): 0.2206613558163073\n",
      "16/16 [==============================] - 0s 738us/step\n",
      "Correlación de Pearson (test): 0.36146354618584164\n",
      "\n",
      "\n",
      "Entrenant model 500MB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.46535328131056514\n",
      "Correlación de Pearson (baseline-validation): 0.4927845302072769\n",
      "Correlación de Pearson (baseline-test): 0.5245793205439706\n",
      "\n",
      "65/65 [==============================] - 0s 695us/step\n",
      "Correlación de Pearson (train): 0.9708647046487451\n",
      "16/16 [==============================] - 0s 646us/step\n",
      "Correlación de Pearson (validation): 0.2878802680116447\n",
      "16/16 [==============================] - 0s 800us/step\n",
      "Correlación de Pearson (test): 0.36742282696603207\n",
      "\n",
      "\n",
      "Entrenant model 1GB\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.46755773415503343\n",
      "Correlación de Pearson (baseline-validation): 0.4936641345624569\n",
      "Correlación de Pearson (baseline-test): 0.5349217743711233\n",
      "\n",
      "65/65 [==============================] - 0s 682us/step\n",
      "Correlación de Pearson (train): 0.9681921259961669\n",
      "16/16 [==============================] - 0s 754us/step\n",
      "Correlación de Pearson (validation): 0.3027051033339575\n",
      "16/16 [==============================] - 0s 802us/step\n",
      "Correlación de Pearson (test): 0.40395339710116057\n",
      "\n",
      "\n",
      "Entrenant model complet\n",
      "\n",
      "Correlación de Pearson (baseline-train): 0.4647217994640573\n",
      "Correlación de Pearson (baseline-validation): 0.5084799291222295\n",
      "Correlación de Pearson (baseline-test): 0.5169887263876756\n",
      "\n",
      "65/65 [==============================] - 0s 688us/step\n",
      "Correlación de Pearson (train): 0.965679153420463\n",
      "16/16 [==============================] - 0s 715us/step\n",
      "Correlación de Pearson (validation): 0.2900266178479769\n",
      "16/16 [==============================] - 0s 793us/step\n",
      "Correlación de Pearson (test): 0.3742118760515454\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def mapp(word2vec, batch_size: int = 64, num_epochs: int = 64):\n",
    "    \n",
    "    # Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "    mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    \n",
    "    # Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "    mapped_train = map_pairs(input_pairs,  tf_idf_model=modelo_tfidf, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "    mapped_test = map_pairs(input_pairs_test, tf_idf_model=modelo_tfidf, dictionary=diccionario, wv_model=word2vec.wv)\n",
    "\n",
    "    # Obtener las listas de train y test\n",
    "    x_train, y_train = pair_list_to_x_y(mapped_train)\n",
    "    x_val, y_val = pair_list_to_x_y(mapped_val)\n",
    "\n",
    "    # Preparar los conjuntos de datos de entrenamiento y validación\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    \n",
    "    model = build_and_compile_model_better(embedding_size=word2vec.vector_size)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=0)\n",
    "    \n",
    "    from scipy.stats import pearsonr\n",
    "    x_test, y_test = pair_list_to_x_y(mapped_test)\n",
    "    \n",
    "    # Baseline\n",
    "    def compute_pearson_baseline(x_, y_):\n",
    "        y_pred_baseline = []\n",
    "        for v1, v2 in zip(*x_):\n",
    "            d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "            y_pred_baseline.append(d)\n",
    "        # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "        correlation, _ = pearsonr(y_pred_baseline, y_.flatten())\n",
    "        return correlation\n",
    "    \n",
    "    # Imprimir el coeficiente de correlación de Pearson\n",
    "    print(f\"Correlación de Pearson (baseline-train): {compute_pearson_baseline(x_train, y_train)}\")\n",
    "    print(f\"Correlación de Pearson (baseline-validation): {compute_pearson_baseline(x_val, y_val)}\")\n",
    "    print(f\"Correlación de Pearson (baseline-test): {compute_pearson_baseline(x_test, y_test)}\\n\")\n",
    "    \n",
    "    def compute_pearson(x_, y_):\n",
    "        # Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "        y_pred = model.predict(x_)\n",
    "        # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "        correlation, _ = pearsonr(y_pred.flatten(), y_.flatten())\n",
    "        return correlation\n",
    "\n",
    "    # Imprimir el coeficiente de correlación de Pearson\n",
    "    print(f\"Correlación de Pearson (train): {compute_pearson(x_train, y_train)}\")\n",
    "    print(f\"Correlación de Pearson (validation): {compute_pearson(x_val, y_val)}\")\n",
    "    print(f\"Correlación de Pearson (test): {compute_pearson(x_test, y_test)}\\n\\n\")\n",
    "\n",
    "for model in ['100MB', '500MB', '1GB', 'complet']:\n",
    "    print(f\"Entrenant model {model}\\n\")\n",
    "    mapp(models[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec + Mean amb model de Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE del model de regressió: 0.6825629832324949\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Exemple de preprocessament de text\n",
    "def obtenir_vectores_mean(text, model_word2vec):\n",
    "    tokens = text.split()\n",
    "    vectors = [model_word2vec.wv[word] for word in tokens if word in model_word2vec.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model_word2vec.vector_size)\n",
    "\n",
    "# Exemple d'entrenament del model de regressió\n",
    "def entrenar_model_regressio(texts, labels, model_word2vec):\n",
    "    X = np.array([np.concatenate((obtenir_vectores_mean(text1, model_word2vec), obtenir_vectores_mean(text2, model_word2vec))) \n",
    "                  for text1, text2 in texts])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediccions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, prediccions)\n",
    "    return model, mse\n",
    "\n",
    "# Entrenar el model de regressió utilitzant un dels models Word2Vec\n",
    "texts =  [(sentence1, sentence2) for sentence1, sentence2, _ in input_pairs] # textos del dataset\n",
    "labels = [label for _, _, label in input_pairs]  # similituds reals\n",
    "model_word2vec = Word2Vec.load(\"model_100MB\") # utilitzar el model Word2Vec entrenat\n",
    "\n",
    "model_regressio, mse = entrenar_model_regressio(texts, labels, model_word2vec)\n",
    "print(f\"MSE del model de regressió: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un valor d'MSE de 0.6825629832324949 indica que l'error quadràtic mitjà entre les prediccions del model i els valors reals és relativament alt. Això suggereix que el model de regressió no està fent prediccions molt precises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud predita: 2.6517136096954346\n"
     ]
    }
   ],
   "source": [
    "# Funció per predir la similitud entre dos textos\n",
    "def predir_similitud(text1, text2, model_regressio, model_word2vec):\n",
    "    vector1 = obtenir_vectores_mean(text1, model_word2vec)\n",
    "    vector2 = obtenir_vectores_mean(text2, model_word2vec)\n",
    "    vector_concat = np.concatenate((vector1, vector2)).reshape(1, -1)\n",
    "    prediccio = model_regressio.predict(vector_concat)\n",
    "    return prediccio[0]\n",
    "\n",
    "# Exemple de predicció\n",
    "text1 = \"Aquest és un text\"\n",
    "text2 = \"Aquest és un altre text\"\n",
    "similitud = predir_similitud(text1, text2, model_regressio, model_word2vec)\n",
    "print(f\"Similitud predita: {similitud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_md = spacy.load('ca_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_mean_embedding(text1, text2, model):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    text1 = ' '.join(text1)\n",
    "    text2 = ' '.join(text2)\n",
    "    frase1 = model(text1).vector\n",
    "    frase2 = model(text2).vector\n",
    "    \n",
    "    return frase1, frase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "spacy_mean_embbeding_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m spacy download ca_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_trf = spacy.load('ca_core_news_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_trf_CLS(frase1, frase2, model):# Procesar la frase\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    doc1 = model(frase1)\n",
    "    doc2 = model(frase2)\n",
    "\n",
    "    # Obtener el embedding CLS\n",
    "    embedding_CLS1 = doc1._.trf_data.last_hidden_layer_state\n",
    "    embedding_array1 = embedding_CLS1.data\n",
    "    embedding_CLS2 = doc2._.trf_data.last_hidden_layer_state\n",
    "    embedding_array2 = embedding_CLS2.data\n",
    "    \n",
    "    return embedding_array1, embedding_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_trf_CLS_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_CLS_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_CLS_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_trf_CLS_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_CLS_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_CLS_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo\n",
    "nlp_trf = spacy.load('ca_core_news_trf')\n",
    "\n",
    "# Frase de ejemplo\n",
    "frase1 = 'atorga per primer cop les mencions encarna sanahuja a la inclusió de la perspectiva de gènere en docència universitària'\n",
    "doc = nlp_trf(frase1)\n",
    "\n",
    "# Obtener las representaciones de la última capa oculta\n",
    "embeddings = doc._.trf_data.last_hidden_layer_state\n",
    "\n",
    "# Convertir a numpy array\n",
    "embeddings_np = np.array(embeddings.data)\n",
    "\n",
    "# Calcular la media de los embeddings a lo largo del eje de las palabras\n",
    "mean_embedding = np.mean(embeddings_np, axis=1)\n",
    "\n",
    "print(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_trf_mean(frase1, frase2, model):\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    doc1 = model(frase1)\n",
    "    doc2 = model(frase2)\n",
    "\n",
    "    # Obtener el embedding CLS\n",
    "    embedding_CLS1 = doc1._.trf_data.last_hidden_layer_state\n",
    "    embedding_array1 = embedding_CLS1.data\n",
    "    embedding_CLS2 = doc2._.trf_data.last_hidden_layer_state\n",
    "    embedding_array2 = embedding_CLS2.data\n",
    "    \n",
    "    return np.array(embedding_array1.data), np.array(embedding_array2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_trf_mean_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_mean_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_mean_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_trf_mean_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_mean_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_mean(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_mean_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_mean_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_mean_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from scipy.special import logit\n",
    "\n",
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoBERTa_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(RoBERTa_corpus_semantic):\n",
    "    frase1 = get_embeddings(frase1)\n",
    "    frase2 = get_embeddings(frase2)\n",
    "    RoBERTa_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in RoBERTa_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in RoBERTa_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Convert sentence pairs to embeddings using the transformer model\n",
    "def get_embeddings(sentence_pairs):\n",
    "    prepared_pairs = prepare(sentence_pairs)\n",
    "    embeddings = []\n",
    "    for pair in prepared_pairs:\n",
    "        inputs = tokenizer(pair, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings\n",
    "\n",
    "fine_tuned_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_tuned_corpus_semantic):\n",
    "    fine_tuned_corpus_semantic[i] = (' '.join(frase1), ' '.join(frase2))\n",
    "\n",
    "embeddings = get_embeddings(fine_tuned_corpus_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_runed_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_runed_corpus_semantic):\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    fine_runed_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe(prepare(fine_runed_corpus_semantic), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions:\n",
    "    prediction['score'] = logit(prediction['score'])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(text, model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in text if word in model]\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        mean_vector = np.zeros(embedding_dim)\n",
    "    return mean_vector\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(mean_embbeding_corpus_semantic_test):\n",
    "    mean_embbeding_corpus_semantic_test[i] = (\n",
    "        compute_mean_embedding(frase1, model_100MB.wv, 100),\n",
    "        compute_mean_embedding(frase2, model_100MB.wv, 100)\n",
    "    )\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model amb embeddings entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)       [(None, 37)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)       [(None, 37)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 37, 100)              100000    ['input_27[0][0]',            \n",
      "                                                                     'input_28[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.not_equal_2 (TFOpL  (None, 37)                   0         ['input_27[0][0]']            \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.not_equal_3 (TFOpL  (None, 37)                   0         ['input_28[0][0]']            \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 37, 1)                1633      ['embedding_1[0][0]',         \n",
      "                                                                     'embedding_1[1][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  (None, 37, 1)                0         ['tf.math.not_equal_2[0][0]'] \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3  (None, 37, 1)                0         ['tf.math.not_equal_3[0][0]'] \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.math.exp_2 (TFOpLambda)  (None, 37, 1)                0         ['sequential_1[0][0]']        \n",
      "                                                                                                  \n",
      " tf.cast_2 (TFOpLambda)      (None, 37, 1)                0         ['tf.__operators__.getitem_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " tf.math.exp_3 (TFOpLambda)  (None, 37, 1)                0         ['sequential_1[1][0]']        \n",
      "                                                                                                  \n",
      " tf.cast_3 (TFOpLambda)      (None, 37, 1)                0         ['tf.__operators__.getitem_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLa  (None, 37, 1)                0         ['tf.math.exp_2[0][0]',       \n",
      " mbda)                                                               'tf.cast_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLa  (None, 37, 1)                0         ['tf.math.exp_3[0][0]',       \n",
      " mbda)                                                               'tf.cast_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_4 (TFOp  (None, 1, 1)                 0         ['tf.math.multiply_4[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_5 (TFOp  (None, 1, 1)                 0         ['tf.math.multiply_5[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.truediv_2 (TFOpLam  (None, 37, 1)                0         ['tf.math.multiply_4[0][0]',  \n",
      " bda)                                                                'tf.math.reduce_sum_4[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.truediv_3 (TFOpLam  (None, 37, 1)                0         ['tf.math.multiply_5[0][0]',  \n",
      " bda)                                                                'tf.math.reduce_sum_5[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLa  (None, 37, 100)              0         ['embedding_1[0][0]',         \n",
      " mbda)                                                               'tf.math.truediv_2[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLa  (None, 37, 100)              0         ['embedding_1[1][0]',         \n",
      " mbda)                                                               'tf.math.truediv_3[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_6 (TFOp  (None, 100)                  0         ['tf.math.multiply_6[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_7 (TFOp  (None, 100)                  0         ['tf.math.multiply_7[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " lambda_25 (Lambda)          (None,)                      0         ['tf.math.reduce_sum_6[0][0]',\n",
      "                                                                     'tf.math.reduce_sum_7[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 101633 (397.00 KB)\n",
      "Trainable params: 101633 (397.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def model_2_wv_rnd(\n",
    "    input_length: int = 37,\n",
    "    dictionary_size: int = 1000,\n",
    "    embedding_size: int = 100,\n",
    "    learning_rate: float = 0.01,\n",
    "    pretrained_weights: Optional[np.ndarray] = None,\n",
    "    trainable: bool = False,\n",
    "    use_cosine: bool = False,\n",
    "    l2_regularizer: float = 1e-4,\n",
    ") -> tf.keras.Model:\n",
    "    # Inputs\n",
    "    input_1 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "    input_2 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "\n",
    "    # Embedding Layer\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, embeddings_initializer='uniform'\n",
    "        )\n",
    "    else:\n",
    "        # Obtener la matriz de embeddings del modelo Word2Vec\n",
    "        word_vectors = pretrained_weights.wv\n",
    "        embedding_matrix = np.zeros((len(word_vectors), embedding_size))\n",
    "        for i in range(len(word_vectors)):\n",
    "            embedding_matrix[i] = word_vectors[word_vectors.index_to_key[i]]\n",
    "            \n",
    "        dictionary_size = embedding_matrix.shape[0]\n",
    "        embedding_size = embedding_matrix.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size,\n",
    "            embedding_size,\n",
    "            input_length=input_length,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=initializer,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "\n",
    "    # Embed the inputs\n",
    "    embedded_1 = embedding(input_1)\n",
    "    embedded_2 = embedding(input_2)\n",
    "    # Pass through the embedding layer\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_mlp = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_regularizer)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    # Apply attention to each embedding\n",
    "    attention_weights_1 = attention_mlp(embedded_1)  \n",
    "    attention_weights_2 = attention_mlp(embedded_2) \n",
    "    # Mask the attention weights\n",
    "    attention_weights_1 = tf.exp(attention_weights_1) * tf.cast(_input_mask_1[:, :, None], tf.float32)\n",
    "    attention_weights_2 = tf.exp(attention_weights_2) * tf.cast(_input_mask_2[:, :, None], tf.float32)\n",
    "    # Normalize attention weights\n",
    "    attention_weights_1 = attention_weights_1 / tf.reduce_sum(attention_weights_1, axis=1, keepdims=True)\n",
    "    attention_weights_2 = attention_weights_2 / tf.reduce_sum(attention_weights_2, axis=1, keepdims=True)\n",
    "    # Compute context vectors\n",
    "    projected_1 = tf.reduce_sum(embedded_1 * attention_weights_1, axis=1) \n",
    "    projected_2 = tf.reduce_sum(embedded_2 * attention_weights_2, axis=1) \n",
    "    \n",
    "    if use_cosine:\n",
    "        # Compute the cosine distance using a Lambda layer\n",
    "        def cosine_distance(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return 2.5 * (1.0 + tf.reduce_sum(x1_normalized * x2_normalized, axis=1))\n",
    "        output = tf.keras.layers.Lambda(cosine_distance)([projected_1, projected_2])\n",
    "    else:\n",
    "         # Compute the cosine distance using a Lambda layer\n",
    "        def normalized_product(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return x1_normalized * x2_normalized\n",
    "    \n",
    "        output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
    "        output = tf.keras.layers.Dropout(0.1)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_regularizer)\n",
    "        )(output)\n",
    "        output = tf.keras.layers.Dropout(0.2)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "        )(output)\n",
    "        \n",
    "        output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
    "        \n",
    "    # Model Definition\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=output)\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "baseline_model = model_2(use_cosine = True)\n",
    "baseline_model.compile()\n",
    "baseline_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenitzar i preparar les seqüències d'entrada per als models\n",
    "def tokenitzar(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def construir_diccionari(texts):\n",
    "    word_to_index = defaultdict(lambda: len(word_to_index))\n",
    "    for text in texts:\n",
    "        tokens = tokenitzar(text)\n",
    "        for token in tokens:\n",
    "            word_to_index[token]\n",
    "    return dict(word_to_index)\n",
    "\n",
    "def text_to_sequence(text, word_to_index):\n",
    "    tokens = tokenitzar(text)\n",
    "    return [word_to_index[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAIN #####\n",
    "\n",
    "# Suposem que tens els teus texts i labels\n",
    "texts = [(sentence1, sentence2) for sentence1, sentence2, _ in input_pairs]\n",
    "labels = [label for _, _, label in input_pairs]\n",
    "\n",
    "# Obtenim tots els textos del dataset\n",
    "all_texts = [text for pair in input_pairs for text in pair[:2]]\n",
    "word_to_index = construir_diccionari(all_texts)\n",
    "\n",
    "# Convertir tots els textos a seqüències d'índexs\n",
    "input_sequences = [(text_to_sequence(pair[0], word_to_index), text_to_sequence(pair[1], word_to_index)) for pair in input_pairs]\n",
    "\n",
    "# max length of the sequences\n",
    "max_len = max(max(len(seq[0]), len(seq[1])) for seq in input_sequences) \n",
    "\n",
    "# Padding de les seqüències d'índexs\n",
    "input_1_sequences = pad_sequences([seq[0] for seq in input_sequences], maxlen=max_len, padding='post')\n",
    "input_2_sequences = pad_sequences([seq[1] for seq in input_sequences], maxlen=max_len, padding='post')\n",
    "\n",
    "# Convertir les seqüències i labels a arrays numpy\n",
    "input_1_sequences = np.array(input_1_sequences)\n",
    "input_2_sequences = np.array(input_2_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Ajustar la mida del diccionari basat en el nombre de paraules úniques\n",
    "dictionary_size = len(word_to_index)\n",
    "\n",
    "#### TEST ####\n",
    "texts_test = [(sentence1, sentence2) for sentence1, sentence2, _ in input_pairs_test]\n",
    "labels_test = [label for _, _, label in input_pairs_test]\n",
    "\n",
    "# Obtenim tots els textos del dataset\n",
    "all_texts_test = [text for pair in input_pairs_test for text in pair[:2]]\n",
    "word_to_index_test = construir_diccionari(all_texts_test)\n",
    "\n",
    "# Convertir tots els textos a seqüències d'índexs\n",
    "input_sequences_test = [(text_to_sequence(pair[0], word_to_index_test), text_to_sequence(pair[1], word_to_index_test)) for pair in input_pairs_test]\n",
    "\n",
    "# max length of the sequences\n",
    "max_len_test = max(max(len(seq[0]), len(seq[1])) for seq in input_sequences_test) \n",
    "\n",
    "# Padding de les seqüències d'índexs\n",
    "input_1_sequences_test = pad_sequences([seq[0] for seq in input_sequences_test], maxlen=max_len, padding='post')\n",
    "input_2_sequences_test = pad_sequences([seq[1] for seq in input_sequences_test], maxlen=max_len, padding='post')\n",
    "\n",
    "# Convertir les seqüències i labels a arrays numpy\n",
    "input_1_sequences_test = np.array(input_1_sequences_test)\n",
    "input_2_sequences_test= np.array(input_2_sequences_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "##### VALIDATION #####\n",
    "# Suposem que tens els teus texts i labels\n",
    "texts_val = [(sentence1, sentence2) for sentence1, sentence2, _ in input_pairs_val]\n",
    "labels_val = [label for _, _, label in input_pairs_val]\n",
    "\n",
    "# Obtenim tots els textos del dataset\n",
    "all_texts_val = [text for pair in input_pairs_val for text in pair[:2]]\n",
    "word_to_index_val = construir_diccionari(all_texts_val)\n",
    "\n",
    "# Convertir tots els textos a seqüències d'índexs\n",
    "input_sequences_val = [(text_to_sequence(pair[0], word_to_index_val), text_to_sequence(pair[1], word_to_index_val)) for pair in input_pairs_val]\n",
    "\n",
    "# max length of the sequences\n",
    "max_len_val = max(max(len(seq[0]), len(seq[1])) for seq in input_sequences_val) \n",
    "\n",
    "# Padding de les seqüències d'índexs\n",
    "input_1_sequences_val = pad_sequences([seq[0] for seq in input_sequences_val], maxlen=max_len, padding='post')\n",
    "input_2_sequences_val = pad_sequences([seq[1] for seq in input_sequences_val], maxlen=max_len, padding='post')\n",
    "\n",
    "\n",
    "# Convertir les seqüències i labels a arrays numpy\n",
    "input_1_sequences_val = np.array(input_1_sequences_val)\n",
    "input_2_sequences_val= np.array(input_2_sequences_val)\n",
    "labels_val = np.array(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Crear Dataset de TensorFlow per a les dades d'entrenament\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((input_1_sequences, input_2_sequences), labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(labels)).batch(batch_size)\n",
    "\n",
    "# Crear Dataset de TensorFlow per a les dades de validació\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((input_1_sequences_val, input_2_sequences_val), labels_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Crear Dataset de TensorFlow per a les dades de test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((input_1_sequences_test, input_2_sequences_test), labels_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding_models = {}\n",
    "for model in models:\n",
    "    word2vec_embedding_models[f\"{model}\"] = model_2_wv_rnd(pretrained_weights=models[model], trainable=True, use_cosine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenament embeddings pel model 100MB\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "MSE del model amb Word2Vec Embeddings en train: 0.01931370235979557\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "\n",
      "\n",
      "\n",
      "MSE del model amb Word2Vec Embeddings en test: 2.2849441930538372\n",
      "R^2 del model amb Word2Vec Embeddings en test: -2.0263778878504257\n",
      "Entrenament embeddings pel model 500MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m word2vec_embedding_models[word2vec_model]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenament embeddings pel model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword2vec_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mentrenament_wv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_1_sequences_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_2_sequences_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 6\u001b[0m, in \u001b[0;36mentrenament_wv\u001b[1;34m(model, input_1_test, input_2_test, labels_test, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentrenament_wv\u001b[39m(model: gensim\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mword2vec, input_1_test, input_2_test, labels_test, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_1_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_2_sequences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_1_sequences_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_2_sequences_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Evaluate en el train\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     mse_word2vec \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(train_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def entrenament_wv(model: gensim.models.word2vec, input_1_test, input_2_test, labels_test, batch_size: int = 64):\n",
    "    \n",
    "    model.fit([input_1_sequences, input_2_sequences], labels, epochs=50, batch_size = batch_size, \n",
    "              validation_data=([input_1_sequences_val, input_2_sequences_val], labels_val), verbose=0)\n",
    "\n",
    "    # Evaluate en el train\n",
    "    mse_word2vec = model.evaluate(train_dataset)\n",
    "    print(f\"MSE del model amb Word2Vec Embeddings en train: {mse_word2vec}\")\n",
    "\n",
    "    # Prediccions del test\n",
    "    prediccions_word2vec = model.predict([input_1_test, input_2_test])\n",
    "\n",
    "    # MSE\n",
    "    mse_word2vec = mean_squared_error(labels_test, prediccions_word2vec)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(f\"MSE del model amb Word2Vec Embeddings en test: {mse_word2vec}\")\n",
    "    print(f\"R^2 del model amb Word2Vec Embeddings en test: {r2_score(labels_test, prediccions_word2vec)}\\n\")\n",
    "    \n",
    "for word2vec_model in word2vec_embedding_models:\n",
    "    model = word2vec_embedding_models[word2vec_model]\n",
    "    print(f\"Entrenament embeddings pel model {word2vec_model}\")\n",
    "    datasets = entrenament_wv(model, input_1_sequences_test, input_2_sequences_test, labels_test, batch_size=64)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.7145 - val_loss: 4.2570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.4605 - val_loss: 3.2239\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.6264 - val_loss: 3.3220\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.4782 - val_loss: 3.4054\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.5328 - val_loss: 2.9246\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.5845 - val_loss: 2.7458\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.5066 - val_loss: 3.4012\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.4396 - val_loss: 3.0944\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2986 - val_loss: 2.5471\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2878 - val_loss: 3.5059\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1167 - val_loss: 2.9260\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1328 - val_loss: 3.0651\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1865 - val_loss: 2.8659\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1988 - val_loss: 3.0466\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1058 - val_loss: 2.7014\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2063 - val_loss: 2.6210\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1856 - val_loss: 2.5780\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2435 - val_loss: 3.0978\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1614 - val_loss: 2.5000\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1870 - val_loss: 2.4384\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1827 - val_loss: 2.4733\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1864 - val_loss: 2.6388\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2054 - val_loss: 2.4767\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.2333 - val_loss: 2.5972\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1571 - val_loss: 3.5374\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0925 - val_loss: 2.9507\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0742 - val_loss: 3.1573\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0292 - val_loss: 3.0232\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 13ms/step - loss: 1.0193 - val_loss: 3.0390\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 13ms/step - loss: 1.0557 - val_loss: 2.6756\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 14ms/step - loss: 1.1513 - val_loss: 2.6987\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0999 - val_loss: 2.9608\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0804 - val_loss: 2.4783\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1066 - val_loss: 2.5246\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0091 - val_loss: 2.4359\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0794 - val_loss: 2.2972\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1378 - val_loss: 2.3854\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0270 - val_loss: 2.4797\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0121 - val_loss: 2.3794\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0345 - val_loss: 2.7494\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0207 - val_loss: 2.8044\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0490 - val_loss: 2.9314\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1030 - val_loss: 2.6221\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0984 - val_loss: 2.4437\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 0.9883 - val_loss: 2.6711\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 0.9845 - val_loss: 2.7776\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1040 - val_loss: 2.5949\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0880 - val_loss: 2.6594\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0747 - val_loss: 2.6290\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.0370 - val_loss: 2.3253\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6473\n",
      "MSE del model amb Word2Vec Embeddings en train: 0.6472876071929932\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "\n",
      "\n",
      "\n",
      "MSE del model amb Word2Vec Embeddings en test: 1.0034994895027056\n",
      "R^2 del model amb Word2Vec Embeddings en test: -0.32912159287411624\n"
     ]
    }
   ],
   "source": [
    "model_random = model_2(dictionary_size=dictionary_size, use_cosine=True)\n",
    "\n",
    "model_random.fit([input_1_sequences, input_2_sequences], labels, epochs=50, batch_size = batch_size, \n",
    "              validation_data=([input_1_sequences_val, input_2_sequences_val], labels_val), verbose=1)\n",
    "\n",
    "# Evaluate en el train\n",
    "mse_word2vec = model.evaluate(train_dataset)\n",
    "print(f\"MSE del model amb Word2Vec Embeddings en train: {mse_word2vec}\")\n",
    "\n",
    "# Prediccions del test\n",
    "prediccions_word2vec = model.predict([input_1_sequences_test, input_1_sequences_test])\n",
    "\n",
    "# MSE\n",
    "mse_word2vec = mean_squared_error(labels_test, prediccions_word2vec)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(f\"MSE del model amb Word2Vec Embeddings en test: {mse_word2vec}\")\n",
    "print(f\"R^2 del model amb Word2Vec Embeddings en test: {r2_score(labels_test, prediccions_word2vec)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
