{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 4 (De nou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codi per entrenar models word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/catalan_general_crawling contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/catalan_general_crawling\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57333\n",
      "213432\n",
      "410702\n"
     ]
    }
   ],
   "source": [
    "# Getting the first 100 MB of the dataset\n",
    "size = 0\n",
    "for i in range(len(dataset)):\n",
    "    size += len(dataset[i]['text'])\n",
    "    if size > 100000000:  # 100 MB\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "dataset_100MB = dataset.select(list(range(i)))\n",
    "\n",
    "# Getting the first 500 MB of the dataset\n",
    "size = 0\n",
    "for j in range(len(dataset)):\n",
    "    size += len(dataset[j]['text'])  # corrected from dataset[i]['text']\n",
    "    if size > 500000000:  # 500 MB\n",
    "        print(j)\n",
    "        break\n",
    "\n",
    "dataset_500MB = dataset.select(list(range(j)))\n",
    "\n",
    "# Getting the first 1 GB of the dataset\n",
    "size = 0\n",
    "for g in range(len(dataset)):\n",
    "    size += len(dataset[g]['text'])  # corrected from dataset[i]['text']\n",
    "    if size > 1000000000:  # 1 GB\n",
    "        print(g)\n",
    "        break\n",
    "\n",
    "dataset_1GB = dataset.select(list(range(g)))\n",
    "\n",
    "# Getting the whole dataset\n",
    "dataset_full = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_100MB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Carrega el fitxer de text i processa'l\u001b[39;00m\n\u001b[0;32m     14\u001b[0m corpus_100MB \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset_100MB\u001b[49m:\n\u001b[0;32m     16\u001b[0m     words \u001b[38;5;241m=\u001b[39m preprocess_text(line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m words:  \u001b[38;5;66;03m# Assegura't que la línia no està buida\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_100MB' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "stopwords_catala = [ \n",
    "    \"a\", \"abans\", \"ací\", \"així\", \"alguns\", \"alguna\", \"algunes\", \"algú\", \"alhora\", \n",
    "    \"als\", \"allò\", \"aquell\", \"aquelles\", \"aquell\", \"aquells\", \"baix\", \n",
    "    \"cada\", \"com\", \"com a\", \"eixa\", \"eixes\", \"eixí\", \n",
    "    \"eixos\", \"el\", \"ella\", \"elles\", \"ell\", \"ells\", \"en\", \"endavant\", \"enfront\", \n",
    "    \"ens\", \"entre\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \n",
    "    \"i\", \"igual\", \"iguals\", \"ja\", \"jo\", \"l'\", \"la\", \"les\", \"li\", \"els\", \"tu\", \"nosaltres\", \"vosaltres\",\n",
    "    \"de\", \"del\", \"dels\", \"d'un\", \"d'una\", \"d'uns\", \"d'unes\", \"des\", \"des de\",\n",
    "    ]\n",
    "\n",
    "# Funció per netejar i tokenitzar el text\n",
    "def preprocess_text(text):\n",
    "    # Neteja el text: treu caràcters no desitjats, converteix a minúscules, etc.\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Substitueix caràcters no alfanumèrics per espais\n",
    "    text = text.lower()  # Converteix a minúscules\n",
    "    words = text.split()  # Tokenitza\n",
    "    # elimina stopwords\n",
    "    words = [word for word in words if word not in stopwords_catala]\n",
    "    return words\n",
    "\n",
    "# Carrega el fitxer de text i processa'l\n",
    "corpus_100MB = []\n",
    "for line in dataset_100MB:\n",
    "    words = preprocess_text(line['text'])\n",
    "    if words:  # Assegura't que la línia no està buida\n",
    "        corpus_100MB.append(words)\n",
    "\n",
    "# Comprova algunes línies per assegurar-te que tot està bé\n",
    "print(corpus_100MB[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "import os\n",
    "\n",
    "def entrenar_word2vec(directori_corpus, mida_vector=100, finestra=5, sg=1):\n",
    "    sentences = PathLineSentences(directori_corpus)\n",
    "    model = Word2Vec(sentences, vector_size=mida_vector, window=finestra, sg=sg)\n",
    "    return model\n",
    "\n",
    "# Entrenar models per a cada mida de dataset\n",
    "models = {}\n",
    "for mida in [\"100MB\", \"500MB\", \"1GB\", \"complet\"]:\n",
    "    directori_corpus = f\"dataset_{mida}\"\n",
    "    model = entrenar_word2vec(directori_corpus)\n",
    "    models[mida] = model\n",
    "    model.save(f\"word2vec_skipgram_{mida}.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_and_compile_model(input_length: int = 764, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16) -> tf.keras.Model:\n",
    "    \n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "    \n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True)\n",
    "    \n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "    \n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Exemple d'ús del model\n",
    "baseline_model = build_and_compile_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisites\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/sts-ca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/sts-ca\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "text_semantic = load_dataset(\"projecte-aina/sts-ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atorga per primer cop les mencions Encarna Sanahuja a la inclusió de la perspectiva de gènere en docència Universitària'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_semantic['train']['sentence1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pairs = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"train\"].to_list()]\n",
    "input_pairs_val = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"validation\"].to_list()]\n",
    "input_pairs_test = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in text_semantic[\"test\"].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprcessament del text_semantic\n",
    "corpus_semantic = []\n",
    "semantic_score = []\n",
    "for line in text_semantic['train']:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic.append((frase1, frase2))\n",
    "    semantic_score.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_semantic_test = []\n",
    "semantic_score_test = []\n",
    "for line in text_semantic['test']:\n",
    "    frase1 = preprocess_text(line['sentence1'])\n",
    "    frase2 = preprocess_text(line['sentence2'])\n",
    "\n",
    "    corpus_semantic_test.append((frase1, frase2))\n",
    "    semantic_score_test.append(line['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari = {}\n",
    "\n",
    "for frases in corpus_semantic:\n",
    "    for frase in frases:\n",
    "        for paraula in frase:\n",
    "            if paraula in vocabulari:\n",
    "                vocabulari[paraula] += 1\n",
    "            else:\n",
    "                vocabulari[paraula] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulari_reduit = {palabra for palabra, frecuencia in vocabulari.items() if frecuencia >= 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función para calcular la representación one-hot de un texto\n",
    "def compute_one_hot_encoding(text, vocabulari):\n",
    "    \"\"\"\n",
    "    Genera la representación one-hot de un texto utilizando un vocabulario predefinido.\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(len(vocabulari))\n",
    "    vocabulario_list = list(vocabulari)\n",
    "    for palabra in text:\n",
    "        if palabra in vocabulari:\n",
    "            index = vocabulario_list.index(palabra)\n",
    "            one_hot_vector[index] = 1\n",
    "        \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "one_hot_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic):\n",
    "    one_hot_corpus_semantic[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "vectors_1 = np.clip(vectors_1, 0, 1)\n",
    "vectors_2 = np.clip(vectors_2, 0, 1)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "one_hot_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(one_hot_corpus_semantic_test):\n",
    "    one_hot_corpus_semantic_test[i] = (compute_one_hot_encoding(frase1, vocabulari_reduit), \n",
    "                                    compute_one_hot_encoding(frase2, vocabulari_reduit))\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in one_hot_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in one_hot_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realitzem el model de regressio amb els embeddings dels models Word2Vec + Mean i + Mean Ponderada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_pairs = input_pairs + input_pairs_val + input_pairs_test\n",
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(sentence_1) for sentence_1, _, _ in all_input_pairs]\n",
    "sentences_2_preproc = [simple_preprocess(sentence_2) for _, sentence_2, _ in all_input_pairs]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "diccionario = Dictionary(sentences_pairs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de los pesos TF-IDF para las oraciones pre-procesadas\n",
    "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
    "modelo_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    bow = dictionary.doc2bow(sentence_preproc)\n",
    "    tf_idf = tf_idf_model[bow]\n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in wv_model:\n",
    "            vectors.append(wv_model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        dictionary: Dictionary = None,\n",
    "        tf_idf_model: TfidfModel = None,\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess_text(sentence_1)\n",
    "        sentence_2_preproc = preprocess_text(sentence_2)\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "        else:\n",
    "            # Cálculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Mean Ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "mapped_train = map_pairs(input_pairs,  tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_test = map_pairs(input_pairs_test, tf_idf_model=modelo_tfidf, dictionary=diccionario, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Mean amb model de Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE del model de regressió: 0.6825629832324949\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Exemple de preprocessament de text\n",
    "def obtenir_vectores_mean(text, model_word2vec):\n",
    "    tokens = text.split()\n",
    "    vectors = [model_word2vec.wv[word] for word in tokens if word in model_word2vec.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model_word2vec.vector_size)\n",
    "\n",
    "# Exemple d'entrenament del model de regressió\n",
    "def entrenar_model_regressio(texts, labels, model_word2vec):\n",
    "    X = np.array([np.concatenate((obtenir_vectores_mean(text1, model_word2vec), obtenir_vectores_mean(text2, model_word2vec))) \n",
    "                  for text1, text2 in texts])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediccions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, prediccions)\n",
    "    return model, mse\n",
    "\n",
    "# Entrenar el model de regressió utilitzant un dels models Word2Vec\n",
    "texts =  [(sentence1, sentence2) for sentence1, sentence2, _ in input_pairs] # textos del dataset\n",
    "labels = [label for _, _, label in input_pairs]  # similituds reals\n",
    "model_word2vec = Word2Vec.load(\"model_100MB\") # utilitzar el model Word2Vec entrenat\n",
    "\n",
    "model_regressio, mse = entrenar_model_regressio(texts, labels, model_word2vec)\n",
    "print(f\"MSE del model de regressió: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un valor d'MSE de 0.6825629832324949 indica que l'error quadràtic mitjà entre les prediccions del model i els valors reals és relativament alt. Això suggereix que el model de regressió no està fent prediccions molt precises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud predita: 2.6517136096954346\n"
     ]
    }
   ],
   "source": [
    "# Funció per predir la similitud entre dos textos\n",
    "def predir_similitud(text1, text2, model_regressio, model_word2vec):\n",
    "    vector1 = obtenir_vectores_mean(text1, model_word2vec)\n",
    "    vector2 = obtenir_vectores_mean(text2, model_word2vec)\n",
    "    vector_concat = np.concatenate((vector1, vector2)).reshape(1, -1)\n",
    "    prediccio = model_regressio.predict(vector_concat)\n",
    "    return prediccio[0]\n",
    "\n",
    "# Exemple de predicció\n",
    "text1 = \"Aquest és un text\"\n",
    "text2 = \"Aquest és un altre text\"\n",
    "similitud = predir_similitud(text1, text2, model_regressio, model_word2vec)\n",
    "print(f\"Similitud predita: {similitud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_md = spacy.load('ca_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_mean_embedding(text1, text2, model):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    text1 = ' '.join(text1)\n",
    "    text2 = ' '.join(text2)\n",
    "    frase1 = model(text1).vector\n",
    "    frase2 = model(text2).vector\n",
    "    \n",
    "    return frase1, frase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar les frases de corpus_semantic a vectors\n",
    "spacy_mean_embbeding_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_mean_embbeding_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_mean_embedding(frase1, frase2, nlp_md)\n",
    "    spacy_mean_embbeding_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in spacy_mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m spacy download ca_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_trf = spacy.load('ca_core_news_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_trf_CLS(frase1, frase2, model):# Procesar la frase\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    doc1 = model(frase1)\n",
    "    doc2 = model(frase2)\n",
    "\n",
    "    # Obtener el embedding CLS\n",
    "    embedding_CLS1 = doc1._.trf_data.last_hidden_layer_state\n",
    "    embedding_array1 = embedding_CLS1.data\n",
    "    embedding_CLS2 = doc2._.trf_data.last_hidden_layer_state\n",
    "    embedding_array2 = embedding_CLS2.data\n",
    "    \n",
    "    return embedding_array1, embedding_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_trf_CLS_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_CLS_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_CLS_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_trf_CLS_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_CLS_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_CLS_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo\n",
    "nlp_trf = spacy.load('ca_core_news_trf')\n",
    "\n",
    "# Frase de ejemplo\n",
    "frase1 = 'atorga per primer cop les mencions encarna sanahuja a la inclusió de la perspectiva de gènere en docència universitària'\n",
    "doc = nlp_trf(frase1)\n",
    "\n",
    "# Obtener las representaciones de la última capa oculta\n",
    "embeddings = doc._.trf_data.last_hidden_layer_state\n",
    "\n",
    "# Convertir a numpy array\n",
    "embeddings_np = np.array(embeddings.data)\n",
    "\n",
    "# Calcular la media de los embeddings a lo largo del eje de las palabras\n",
    "mean_embedding = np.mean(embeddings_np, axis=1)\n",
    "\n",
    "print(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_trf_mean(frase1, frase2, model):\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    doc1 = model(frase1)\n",
    "    doc2 = model(frase2)\n",
    "\n",
    "    # Obtener el embedding CLS\n",
    "    embedding_CLS1 = doc1._.trf_data.last_hidden_layer_state\n",
    "    embedding_array1 = embedding_CLS1.data\n",
    "    embedding_CLS2 = doc2._.trf_data.last_hidden_layer_state\n",
    "    embedding_array2 = embedding_CLS2.data\n",
    "    \n",
    "    return np.array(embedding_array1.data), np.array(embedding_array2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_trf_mean_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_mean_corpus_semantic):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_CLS(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_mean_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_CLS_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "spacy_trf_mean_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(spacy_trf_mean_corpus_semantic_test):\n",
    "    \n",
    "    frase1, frase2 = spacy_trf_mean(frase1, frase2, nlp_trf)\n",
    "    spacy_trf_mean_corpus_semantic_test[i] = (frase1, frase2)\n",
    "\n",
    "vectors_1 = np.array([pair[0][0] for pair in spacy_trf_mean_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[0][1] for pair in spacy_trf_mean_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from scipy.special import logit\n",
    "\n",
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoBERTa_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(RoBERTa_corpus_semantic):\n",
    "    frase1 = get_embeddings(frase1)\n",
    "    frase2 = get_embeddings(frase2)\n",
    "    RoBERTa_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenament de la xarxa amb els vectors de les frases ####\n",
    "\n",
    "# Convertir les llistes de tuples a arrays de NumPy\n",
    "vectors_1 = np.array([pair[0][0] for pair in RoBERTa_corpus_semantic])\n",
    "vectors_2 = np.array([pair[0][1] for pair in RoBERTa_corpus_semantic])\n",
    "\n",
    "# Comprovar si hi ha valors fora de rang i substituir-los\n",
    "#vectors_1 = np.clip(vectors_1, 0, 999)\n",
    "#vectors_2 = np.clip(vectors_2, 0, 999)\n",
    "\n",
    "# Convertir les llistes de similituds a un array de NumPy\n",
    "semantic_score = np.array(semantic_score)\n",
    "\n",
    "print(vectors_1.shape)\n",
    "print(vectors_2.shape)\n",
    "print(semantic_score.shape)\n",
    "\n",
    "baseline_model.fit(\n",
    "    [vectors_1, vectors_2], \n",
    "    semantic_score, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Convert sentence pairs to embeddings using the transformer model\n",
    "def get_embeddings(sentence_pairs):\n",
    "    prepared_pairs = prepare(sentence_pairs)\n",
    "    embeddings = []\n",
    "    for pair in prepared_pairs:\n",
    "        inputs = tokenizer(pair, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings\n",
    "\n",
    "fine_tuned_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_tuned_corpus_semantic):\n",
    "    fine_tuned_corpus_semantic[i] = (' '.join(frase1), ' '.join(frase2))\n",
    "\n",
    "embeddings = get_embeddings(fine_tuned_corpus_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_runed_corpus_semantic = corpus_semantic.copy()\n",
    "for i, (frase1, frase2) in enumerate(fine_runed_corpus_semantic):\n",
    "    frase1 = ' '.join(frase1)\n",
    "    frase2 = ' '.join(frase2)\n",
    "    fine_runed_corpus_semantic[i] = (frase1, frase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe(prepare(fine_runed_corpus_semantic), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions:\n",
    "    prediction['score'] = logit(prediction['score'])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(text, model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in text if word in model]\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        mean_vector = np.zeros(embedding_dim)\n",
    "    return mean_vector\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Pasar las frases de corpus_semantic a vectores\n",
    "mean_embbeding_corpus_semantic_test = corpus_semantic_test.copy()\n",
    "for i, (frase1, frase2) in enumerate(mean_embbeding_corpus_semantic_test):\n",
    "    mean_embbeding_corpus_semantic_test[i] = (\n",
    "        compute_mean_embedding(frase1, model_100MB.wv, 100),\n",
    "        compute_mean_embedding(frase2, model_100MB.wv, 100)\n",
    "    )\n",
    "\n",
    "vectors_1 = np.array([pair[0] for pair in mean_embbeding_corpus_semantic_test])\n",
    "vectors_2 = np.array([pair[1] for pair in mean_embbeding_corpus_semantic_test])\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "predicciones = baseline_model.predict([vectors_1, vectors_2])\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(semantic_score_test, predicciones)\n",
    "\n",
    "# Calcular el coeficiente de determinación (R^2)\n",
    "r2 = r2_score(semantic_score_test, predicciones)\n",
    "\n",
    "# Mostrar las predicciones, el MSE y el R^2\n",
    "#print(\"Predicciones:\", predicciones)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R^2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model amb embeddings entrenables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "vectors_1 =[pair[0] for pair in mean_embbeding_corpus_semantic]\n",
    "vectors_2 =[pair[1] for pair in mean_embbeding_corpus_semantic]\n",
    "\n",
    "# Tokenització i creació del vocabulari\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Padding de les seqüències\n",
    "max_length = 10\n",
    "data_1 = pad_sequences(sequences_1, maxlen=max_length)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=max_length)\n",
    "\n",
    "# Paràmetres del model\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 100\n",
    "hidden_size = 64\n",
    "input_length = max_length\n",
    "\n",
    "# Creació d'embeddings aleatoris\n",
    "random_embeddings = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Creació d'embeddings de Word2Vec\n",
    "texts_combined = [text.split() for text in texts_1 + texts_2]\n",
    "word2vec_model = Word2Vec(texts_combined, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "word2vec_embeddings = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        word2vec_embeddings[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Entrenament del model amb embeddings aleatoris\n",
    "model_random = build_and_compile_model(random_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_random.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Entrenament del model amb embeddings de Word2Vec\n",
    "model_word2vec = build_and_compile_model(word2vec_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_word2vec.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Avalua els resultats dels models\n",
    "loss_random = model_random.evaluate([data_1, data_2], labels)\n",
    "loss_word2vec = model_word2vec.evaluate([data_1, data_2], labels)\n",
    "\n",
    "print(f'Random Embeddings - Loss: {loss_random}')\n",
    "print(f'Word2Vec Embeddings - Loss: {loss_word2vec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Exemple de dades\n",
    "texts_1 = [\"aquest és un text d'exemple\", \"un altre text\"]\n",
    "texts_2 = [\"aquest és un altre text\", \"un text diferent\"]\n",
    "labels = [0.8, 0.4]  # Similitud entre les frases\n",
    "\n",
    "# Tokenització i creació del vocabulari\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Padding de les seqüències\n",
    "max_length = 10\n",
    "data_1 = pad_sequences(sequences_1, maxlen=max_length)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=max_length)\n",
    "\n",
    "# Paràmetres del model\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 100\n",
    "hidden_size = 64\n",
    "input_length = max_length\n",
    "\n",
    "# Creació d'embeddings de Word2Vec\n",
    "texts_combined = [text.split() for text in texts_1 + texts_2]\n",
    "word2vec_model = Word2Vec(texts_combined, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "word2vec_embeddings = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        word2vec_embeddings[i] = word2vec_model.wv[word]\n",
    "\n",
    "def build_and_compile_model(embedding_matrix, input_length: int = 100, hidden_size: int = 64) -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32), tf.keras.Input((input_length, ), dtype=tf.int32)\n",
    "\n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        vocab_size, embedding_dim, weights=[embedding_matrix], input_length=input_length, mask_zero=True, trainable=True)\n",
    "\n",
    "    lstm = tf.keras.layers.LSTM(hidden_size)\n",
    "\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "\n",
    "    # Pass through the layers\n",
    "    _embedded_1, _embedded_2 = embedding(input_1), embedding(input_2)\n",
    "    _lstm_1, _lstm_2 = lstm(_embedded_1), lstm(_embedded_2)\n",
    "\n",
    "    _concatenated = concatenate([_lstm_1, _lstm_2])\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=_output)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "# Entrenament del model amb embeddings de Word2Vec\n",
    "model_word2vec = build_and_compile_model(word2vec_embeddings, input_length=max_length, hidden_size=hidden_size)\n",
    "model_word2vec.fit([data_1, data_2], labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Avalua els resultats dels models\n",
    "loss_word2vec = model_word2vec.evaluate([data_1, data_2], labels)\n",
    "\n",
    "print(f'Word2Vec Embeddings - Loss: {loss_word2vec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Defineix funcions per calcular els embeddings de frases\n",
    "def compute_mean_embedding(text, model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Agafa els vectors d'embeddings de cada paraula en una frase o document i \n",
    "    calcula la mitjana dels vectors per obtenir una única representació vectorial \n",
    "    per a la frase o document.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in text if word in model]\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        mean_vector = np.zeros(embedding_dim)\n",
    "    return mean_vector\n",
    "\n",
    "def compute_weighted_mean_embedding(text, model, word2tfidf, embedding_dim):\n",
    "    \"\"\"\n",
    "    Utilitza una ponderació per a cada vector de paraula, com ara la freqüència \n",
    "    inversa del document (TF-IDF), per calcular una mitjana ponderada dels vectors.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] * word2tfidf[word] for word in text if word in model and word in word2tfidf]\n",
    "    if vectors:\n",
    "        weighted_mean_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        weighted_mean_vector = np.zeros(embedding_dim)\n",
    "    return weighted_mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de corpus\n",
    "corpus = [\"Aquest és un text de exemple\", \"Aquí hi ha un altre text\"]\n",
    "\n",
    "# Prepara els vectors TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.7557\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1112\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4268\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5486\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4800\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3008\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0543\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2087\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2880\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3351\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.10263795]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'ús amb embeddings pre-entrenats (Word2Vec)\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carrega el model pre-entrenat (substitueix 'path_to_model' amb la ruta real)\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format('path_to_model', binary=True)\n",
    "\n",
    "# Defineix la longitud de les seqüències i la dimensió dels embeddings\n",
    "input_length = 10\n",
    "embedding_dim = model_100MB.vector_size\n",
    "\n",
    "# Prepara les dades d'entrenament\n",
    "X_train1 = np.array([compute_mean_embedding(text.split(), model_100MB.wv, embedding_dim) for text in corpus])\n",
    "X_train2 = np.array([compute_weighted_mean_embedding(text.split(), model_100MB.wv, word2tfidf, embedding_dim) for text in corpus])\n",
    "y_train = np.random.rand(len(corpus), 1)\n",
    "\n",
    "# Defineix el model de Keras (ajusta les dimensions segons la longitud de les seqüències i la dimensió dels embeddings)\n",
    "def build_and_compile_model(input_dim, hidden_size=64):\n",
    "    input_1 = tf.keras.Input(shape=(input_dim,))\n",
    "    input_2 = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')(concatenate)\n",
    "    output = tf.keras.layers.Dense(1)(hidden)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construir i compilar el model\n",
    "m = build_and_compile_model(input_dim=embedding_dim)\n",
    "\n",
    "# Entrenar el model\n",
    "m.fit([X_train1, X_train2], y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Exemple de predicció\n",
    "input_data1 = compute_mean_embedding(\"Aquest és un text de exemple\".split(), model_100MB.wv, embedding_dim).reshape(1, -1)\n",
    "input_data2 = compute_weighted_mean_embedding(\"Aquí hi ha un altre text\".split(), model_100MB.wv, word2tfidf, embedding_dim).reshape(1, -1)\n",
    "\n",
    "# Predir la similitud\n",
    "y_pred = m.predict([input_data1, input_data2])\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
